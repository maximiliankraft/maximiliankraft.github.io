{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"1. Jahrgang Python Skriptum von Prof. Radlbauer 2. Jahrgang Grundlagen zum HTTP ASP-Anwendung in Azure deployen 3. Jahrgang Webentwicklung mit Remix und Prisma - Grundlagen 4. Jahrgang Theoriefragen zu Docker Webauthn mit Spring Webentwicklung mit Remix und Prisma - Erweitert \u00dcbungsaufgaben zu Promises 5. Jahrgang Docker Basics Spring-Anwendung in Azure deployen Container Registries erkl\u00e4rt Devcontainer CI/CD mit Kaniko Incident response mit Prometheus Backups mit rclone erstellen Angaben f\u00fcr \u00dcbungen 1. Jahrgang HBGM 2. Jahrgang HBGM 3. Jahrgang HBGM 3. Jahrgang HIF 4. Jahrgang HBGM 5. Jahrgang HBGM Allgemeines Protokollvorlage Live Code XP Einige Links funktionieren noch nicht. Solltest du auf eine Aufgabe nicht zugreifen k\u00f6nnen, schicke mir einfach eine E-Mail","title":"Startseite"},{"location":"#1-jahrgang","text":"Python Skriptum von Prof. Radlbauer","title":"1. Jahrgang"},{"location":"#2-jahrgang","text":"Grundlagen zum HTTP ASP-Anwendung in Azure deployen","title":"2. Jahrgang"},{"location":"#3-jahrgang","text":"Webentwicklung mit Remix und Prisma - Grundlagen","title":"3. Jahrgang"},{"location":"#4-jahrgang","text":"Theoriefragen zu Docker Webauthn mit Spring Webentwicklung mit Remix und Prisma - Erweitert \u00dcbungsaufgaben zu Promises","title":"4. Jahrgang"},{"location":"#5-jahrgang","text":"Docker Basics Spring-Anwendung in Azure deployen Container Registries erkl\u00e4rt Devcontainer CI/CD mit Kaniko Incident response mit Prometheus Backups mit rclone erstellen","title":"5. Jahrgang"},{"location":"#angaben-fur-ubungen","text":"1. Jahrgang HBGM 2. Jahrgang HBGM 3. Jahrgang HBGM 3. Jahrgang HIF 4. Jahrgang HBGM 5. Jahrgang HBGM","title":"Angaben f\u00fcr \u00dcbungen"},{"location":"#allgemeines","text":"Protokollvorlage Live Code XP Einige Links funktionieren noch nicht. Solltest du auf eine Aufgabe nicht zugreifen k\u00f6nnen, schicke mir einfach eine E-Mail","title":"Allgemeines"},{"location":"LivecodeXP/","text":"Livecode XP","title":"LivecodeXP"},{"location":"Assignments/1XHBGM/Index/","text":"Aufgabensammlung f\u00fcr den 1. Jahrgang HBGM Nicht alle hier aufgelisteten Aufgaben m\u00fcssen auch erledigt werden. Die zu erf\u00fcllenden Aufgaben und deren Abgabedaten sind auf Moodle einsehbar.","title":"\u00dcbungen 1XHBGM"},{"location":"Assignments/2XHBGM/FirstProjectASP/","text":"Erstes Projekt in ASP Checkliste Folgende Punkte sollen erarbeitet werden und im Protokoll die Vorgehensweise erkl\u00e4rt werden: - Projekt angelegt - Projekt gestartet (IDE) - Projekt gestartet (Konsole) - Seite hinzugef\u00fcgt - Bestehende Seite Text ge\u00e4ndert","title":"Azure Anleitung ASP"},{"location":"Assignments/2XHBGM/FirstProjectASP/#erstes-projekt-in-asp","text":"","title":"Erstes Projekt in ASP"},{"location":"Assignments/2XHBGM/FirstProjectASP/#checkliste","text":"Folgende Punkte sollen erarbeitet werden und im Protokoll die Vorgehensweise erkl\u00e4rt werden: - Projekt angelegt - Projekt gestartet (IDE) - Projekt gestartet (Konsole) - Seite hinzugef\u00fcgt - Bestehende Seite Text ge\u00e4ndert","title":"Checkliste"},{"location":"Assignments/2XHBGM/Index/","text":"Aufgabensammlung f\u00fcr den 2. Jahrgang HBGM Nicht alle hier aufgelisteten Aufgaben m\u00fcssen auch erledigt werden. Die zu erf\u00fcllenden Aufgaben und deren Abgabedaten sind auf Moodle einsehbar. ASP Einrichten Einfache ToDo Listen Webseite Daten als CSV speichern Authentication Datenbank Eigenes Projekt Deployment Nicht genutzte Aufgabenstellungen Cookies Dateiupload Kalender ASP Komponenten","title":"\u00dcbungen 2XHBGM"},{"location":"Assignments/2XHBGM/Index/#nicht-genutzte-aufgabenstellungen","text":"Cookies Dateiupload Kalender ASP Komponenten","title":"Nicht genutzte Aufgabenstellungen"},{"location":"Assignments/2XHBGM/authentication/","text":"Authentication Aktuell k\u00f6nnen die ToDos von jedem eingesehen und bearbeitet werden. F\u00fcr eine moderne Webanwendungen ist das noch nicht ganz ideal. Zum Gl\u00fcck kann man bei ASP.NET Core ganz einfach eine Authentication hinzuf\u00fcgen. Erstelle daf\u00fcr ein neues Projekt und achte darauf das beim Punkt Auth Individual Authentication ausgew\u00e4hlt ist. Startet man nun das neu angelegte Projekt ist der selbe Beispielcode drinnen wie sonst auch. Nur diesmal sind einige Dateien extra dabei die sich k\u00fcmmern um: - Das Speichern von neuen Benutzern in einer SqLite Datenbank - Verschl\u00fcsselung der Passw\u00f6rter - Versenden von Best\u00e4tigungsmails - etc. F\u00fcr das versenden von Best\u00e4tigungsmails muss man einen Server angeben an den die C#-Anwendung SMTP-Pakete schicken kann. Da wir so einen Server nicht haben ist es auch ok wenn wir die notwendigkeit einer Best\u00e4tigungsmail kurzerhand ausschalten. Gehe dazu in die Hauptdatei Program.cs und setze den Wert RequireConfirmedAccount auf false . builder.Services.AddDefaultIdentity<IdentityUser>(options => options.SignIn.RequireConfirmedAccount = false) .AddEntityFrameworkStores<ApplicationDbContext>(); Nun sollte es m\u00f6glich sein dass du dich auf deiner eigenen Anwendung registrieren und danach einloggen kannst. Dokumentiere diese Funktion in deinem Protokoll. ToDo Integration Kopiere alle Dateien die du brauchst um ToDos in einer CSV zu speichern aus deinem alten Projekt in das neue. Es soll nun m\u00f6glich sein dass die ToDo-Seite nur aufgerufen werden kann wenn man eingeloggt ist. Ist man nicht eingeloggt und m\u00f6chte die Seite aufrufen soll man auf die Login-Seite weitergeleitet werden. Schau dir dazu den Code in Shared/LoginDisplay.razor an. Mit dem <AuthroizeView> -Element kann man den Login-Status pr\u00fcfen. Es soll m\u00f6glich sein dass mehrere User ToDo-Eintr\u00e4ge anlegen. Jeder User soll nur seine eigenen Eintr\u00e4ge sehen. Erweitere daf\u00fcr die Klasse ToDoItem.cs um eine Klassenvariable in der der Besitzer des Items gespeichert wird. Beim Laden der Eintr\u00e4ge aus der CSV soll eine gefilterte Liste zur\u00fcck gegeben werden in der nicht alle Eintr\u00e4ge vorhanden sind sondern nur die des aktuellen angemeldeten Benutzers. Ob du die Eintr\u00e4ge alle in eine Datei speicherst und beim Aufruf filterst oder du je Benutzer eine eigene Datei anlegst - oder ganz anders - kannst du selbst planen. Beides ist in Ordnung.","title":"Azure Anleitung ASP"},{"location":"Assignments/2XHBGM/authentication/#authentication","text":"Aktuell k\u00f6nnen die ToDos von jedem eingesehen und bearbeitet werden. F\u00fcr eine moderne Webanwendungen ist das noch nicht ganz ideal. Zum Gl\u00fcck kann man bei ASP.NET Core ganz einfach eine Authentication hinzuf\u00fcgen. Erstelle daf\u00fcr ein neues Projekt und achte darauf das beim Punkt Auth Individual Authentication ausgew\u00e4hlt ist. Startet man nun das neu angelegte Projekt ist der selbe Beispielcode drinnen wie sonst auch. Nur diesmal sind einige Dateien extra dabei die sich k\u00fcmmern um: - Das Speichern von neuen Benutzern in einer SqLite Datenbank - Verschl\u00fcsselung der Passw\u00f6rter - Versenden von Best\u00e4tigungsmails - etc. F\u00fcr das versenden von Best\u00e4tigungsmails muss man einen Server angeben an den die C#-Anwendung SMTP-Pakete schicken kann. Da wir so einen Server nicht haben ist es auch ok wenn wir die notwendigkeit einer Best\u00e4tigungsmail kurzerhand ausschalten. Gehe dazu in die Hauptdatei Program.cs und setze den Wert RequireConfirmedAccount auf false . builder.Services.AddDefaultIdentity<IdentityUser>(options => options.SignIn.RequireConfirmedAccount = false) .AddEntityFrameworkStores<ApplicationDbContext>(); Nun sollte es m\u00f6glich sein dass du dich auf deiner eigenen Anwendung registrieren und danach einloggen kannst. Dokumentiere diese Funktion in deinem Protokoll.","title":"Authentication"},{"location":"Assignments/2XHBGM/authentication/#todo-integration","text":"Kopiere alle Dateien die du brauchst um ToDos in einer CSV zu speichern aus deinem alten Projekt in das neue. Es soll nun m\u00f6glich sein dass die ToDo-Seite nur aufgerufen werden kann wenn man eingeloggt ist. Ist man nicht eingeloggt und m\u00f6chte die Seite aufrufen soll man auf die Login-Seite weitergeleitet werden. Schau dir dazu den Code in Shared/LoginDisplay.razor an. Mit dem <AuthroizeView> -Element kann man den Login-Status pr\u00fcfen. Es soll m\u00f6glich sein dass mehrere User ToDo-Eintr\u00e4ge anlegen. Jeder User soll nur seine eigenen Eintr\u00e4ge sehen. Erweitere daf\u00fcr die Klasse ToDoItem.cs um eine Klassenvariable in der der Besitzer des Items gespeichert wird. Beim Laden der Eintr\u00e4ge aus der CSV soll eine gefilterte Liste zur\u00fcck gegeben werden in der nicht alle Eintr\u00e4ge vorhanden sind sondern nur die des aktuellen angemeldeten Benutzers. Ob du die Eintr\u00e4ge alle in eine Datei speicherst und beim Aufruf filterst oder du je Benutzer eine eigene Datei anlegst - oder ganz anders - kannst du selbst planen. Beides ist in Ordnung.","title":"ToDo Integration"},{"location":"Assignments/2XHBGM/calendar/","text":"MedikamentenerinnerungsKalender","title":"Calendar"},{"location":"Assignments/2XHBGM/calendar/#medikamentenerinnerungskalender","text":"","title":"MedikamentenerinnerungsKalender"},{"location":"Assignments/2XHBGM/components/","text":"","title":"Components"},{"location":"Assignments/2XHBGM/cookies/","text":"","title":"Cookies"},{"location":"Assignments/2XHBGM/database/","text":"Datenbankanbindung an ASP Link zu Beispielprojekt: github.com Link zu Tutorials: youtube.com Einf\u00fchrung Datenbanken Um verschiedene Arten von Daten zu speichern k\u00f6nnte man mehrere CSV-Dateien verwenden. Jedoch ist es in diesem Ansatz schwer effizient mit den Daten umzugehen. Man kann eigentlich nur alle Daten auf einmal anzeigen. M\u00f6chte man die Daten filtern muss man die komplette Datei in den RAM laden (teilweise laden g\u00e4nge auch, w\u00e4re aber noch umst\u00e4ndlicher). Dann jedes Element in einer for(each)-Schleife durchgehen und pr\u00fcfen ob die Bedingung erf\u00fcllt ist. Datenbanken nehmen einem diese Arbeit zu einem gro\u00dfen Teil ab. Man kann Abfragen schreiben in denen man spezifiziert welche Art von Daten man haben m\u00f6chte und die Datenbank erledigt das Filtern von selbst. Kurz erkl\u00e4rt wie diese Optimierungen funktionieren: Stell dir vor wir haben folgende Klasse: class Person{ int Id; string FirstName; string LastName; } Aus einer CSV-Datei wurden Instanzen dieser Klasse gelesen und in der Liste List<Person> people; gespeichert. M\u00f6chte man nun eine Liste haben in der - gefiltert - alle Personen stehen die den Vornamen Peter haben kann man so vorgehen: List<Person> filtered = new List<Person>(); foreach(var person: people){ if(person.FirstName == \"Peter\"){ filtered.Add(person); } } Bei ein paar tausend Datens\u00e4tzen dauert das nicht all zu lange auf modernen Prozessoren. Jedoch st\u00f6\u00dft man mit diesem einfachen Ansatz an seine Grenzen wenn man mehrere Millionen oder noch mehr Datens\u00e4tze verarbeitet. Die Suche - muss jedes mal neu ausgef\u00fchrt werden - ist nicht parallel Datenbanken bauen sich intern Strukturen auf um beim Einf\u00fcgen der Daten sp\u00e4ter schneller zu wissen wo diese liegen. \u00c4hnlich wie ein Inhaltsverzeichnis in einem Buch. Solche Strukturen nennt man B-Trees bzw. balanced Trees. Deren genaue Arbeitsweise ist f\u00fcr die Erf\u00fcllung dieser Aufgabe nicht relevant, falls es dich aber interessiert kannst du hier mehr dar\u00fcber lesen: Introduction of B-Tree . Vorbereitung Um mit Datenbanken in ASP.NET Core arbeiten zu k\u00f6nnen ben\u00f6tigt man zus\u00e4tzliche Bibliotheken die in der Standard-Installation nicht dabei sind. Diese kann man \u00fcber den NuGet-Paketmanager installieren. Dazu muss man in Rider in der Sidebar zun\u00e4chst in die NuGet-Sektion gehen. In dem Suchfeld dann bitte nach folgenden Paketen suchen: - Microsoft.EntityFrameworkCore - Microsoft.EntityFrameworkCore.Sqlite Funktioniert in Visual Studio \u00e4hnlich, siehe folgende Anleitung Damit wird zun\u00e4chst das Entity Framework installiert. Achte auf eine f\u00fcr dich kompatible Version. Wenn du das .NET Core SDK 8 nimmst muss die EF (Entity Framework) Major Version ebenfalls 8 sein. In meinem Fall in dem Screenshot habe ich f\u00fcr alles Version 6 genommen, die rennt stabiler in Rider. EF ist eine Bibliothek welche ein einheitliches Interface bietet zum Zugriff auf verschiedene Datenbanken. Das Paket Microsoft.EntityFrameworkCore.Sqlite beinhaltet Treiber damit EF weis wie es mit SQLite-Datenbanken umgehen kann. Es gibt auch Treiber f\u00fcr MySQL, MSSQL uvm. Integration Lege eine neue Klasse an die von DbContext erbt. Darin kannst du den Pfad angeben an dem die Datenbank-Datei gespeichert wird. Bei anderen Datenbanken muss man dort die URL zum Server angeben, Username, Passwort etc. Zudem muss man durch \u00fcberschreiben der Methode OnConfiguring festlegen was passiert wenn der Datenbank-Kontext konfiguriert wird. Das noch leere \u00fcbergebene options -Objekt muss man konfigurieren. Durch Aufruf der UseSQlite -Methode mit einem Connection-String gibt man dem SQLite-Treiber alle Informationen die er ben\u00f6tigt um eine Datenbank anzulegen. Die DbSet<T> -Variablen muss man f\u00fcr jede Klasse anlegen die in der Datenbank gespeichert werden soll. In deinem Fall dann die Klasse ToDoItem . Hier habe ich den Beispielcode mit den WeatherForecasts erweitert dass diese in der Datenbank gespeichert werden k\u00f6nnen. public class DataContext : DbContext { private static string _dbPath = \"data.db\"; // The following configures EF to create a Sqlite database file protected override void OnConfiguring(DbContextOptionsBuilder options) { options.UseSqlite($\"Data Source={_dbPath}\"); } // A DbSet is a collection that can store and retrieve data // from the Database public DbSet<WeatherForecast> WeatherForecasts { get; set; } } Im WeatherForecastService kann man dann folgenderma\u00dfen Daten aus der Datenbank lesen und schreiben: // ctor (kurzform f\u00fcr Konstruktor) public WeatherForecastService() { // Neuen Datenbankkontext anlegen db = new DataContext(); // Sicherstellen dass die Datei erstellt wurde db.Database.EnsureCreated(); } public Task<WeatherForecast[]> GetForecastAsync(DateTime startDate) { // generiere zuf\u00e4llige daten var weatherData = Enumerable.Range(1, 5).Select(index => new WeatherForecast { Date = startDate.AddDays(index), TemperatureC = Random.Shared.Next(-20, 55), Summary = Summaries[Random.Shared.Next(Summaries.Length)] }); // f\u00fcge mehrere Elemente auf einmal in die Datenbank ein db.WeatherForecasts.AddRange(weatherData); // Schreibe alle zwischengespeicherten Daten // in die Datenbank db.SaveChanges(); // hole alle Daten aus der Datenbank und konvertiere // das Ergebnis in ein Array return Task.FromResult(db.WeatherForecasts.ToArray()); } Aufgabenstellung Installieren der NuGet-Pakete (wenn nicht bereits installiert) Datenbank-Kontext erstellen Daten in Datenbank schreiben Daten aus Datenbank lesen Daten nach Benutzer filtern Beispiel wie man im WeatherForecast filtern kann nach positiven Temperaturen: db.WeatherForecasts.Select(forecast => forecast.TemperatureC > 0);","title":"Azure Anleitung ASP"},{"location":"Assignments/2XHBGM/database/#datenbankanbindung-an-asp","text":"Link zu Beispielprojekt: github.com Link zu Tutorials: youtube.com","title":"Datenbankanbindung an ASP"},{"location":"Assignments/2XHBGM/database/#einfuhrung-datenbanken","text":"Um verschiedene Arten von Daten zu speichern k\u00f6nnte man mehrere CSV-Dateien verwenden. Jedoch ist es in diesem Ansatz schwer effizient mit den Daten umzugehen. Man kann eigentlich nur alle Daten auf einmal anzeigen. M\u00f6chte man die Daten filtern muss man die komplette Datei in den RAM laden (teilweise laden g\u00e4nge auch, w\u00e4re aber noch umst\u00e4ndlicher). Dann jedes Element in einer for(each)-Schleife durchgehen und pr\u00fcfen ob die Bedingung erf\u00fcllt ist. Datenbanken nehmen einem diese Arbeit zu einem gro\u00dfen Teil ab. Man kann Abfragen schreiben in denen man spezifiziert welche Art von Daten man haben m\u00f6chte und die Datenbank erledigt das Filtern von selbst. Kurz erkl\u00e4rt wie diese Optimierungen funktionieren: Stell dir vor wir haben folgende Klasse: class Person{ int Id; string FirstName; string LastName; } Aus einer CSV-Datei wurden Instanzen dieser Klasse gelesen und in der Liste List<Person> people; gespeichert. M\u00f6chte man nun eine Liste haben in der - gefiltert - alle Personen stehen die den Vornamen Peter haben kann man so vorgehen: List<Person> filtered = new List<Person>(); foreach(var person: people){ if(person.FirstName == \"Peter\"){ filtered.Add(person); } } Bei ein paar tausend Datens\u00e4tzen dauert das nicht all zu lange auf modernen Prozessoren. Jedoch st\u00f6\u00dft man mit diesem einfachen Ansatz an seine Grenzen wenn man mehrere Millionen oder noch mehr Datens\u00e4tze verarbeitet. Die Suche - muss jedes mal neu ausgef\u00fchrt werden - ist nicht parallel Datenbanken bauen sich intern Strukturen auf um beim Einf\u00fcgen der Daten sp\u00e4ter schneller zu wissen wo diese liegen. \u00c4hnlich wie ein Inhaltsverzeichnis in einem Buch. Solche Strukturen nennt man B-Trees bzw. balanced Trees. Deren genaue Arbeitsweise ist f\u00fcr die Erf\u00fcllung dieser Aufgabe nicht relevant, falls es dich aber interessiert kannst du hier mehr dar\u00fcber lesen: Introduction of B-Tree .","title":"Einf\u00fchrung Datenbanken"},{"location":"Assignments/2XHBGM/database/#vorbereitung","text":"Um mit Datenbanken in ASP.NET Core arbeiten zu k\u00f6nnen ben\u00f6tigt man zus\u00e4tzliche Bibliotheken die in der Standard-Installation nicht dabei sind. Diese kann man \u00fcber den NuGet-Paketmanager installieren. Dazu muss man in Rider in der Sidebar zun\u00e4chst in die NuGet-Sektion gehen. In dem Suchfeld dann bitte nach folgenden Paketen suchen: - Microsoft.EntityFrameworkCore - Microsoft.EntityFrameworkCore.Sqlite Funktioniert in Visual Studio \u00e4hnlich, siehe folgende Anleitung Damit wird zun\u00e4chst das Entity Framework installiert. Achte auf eine f\u00fcr dich kompatible Version. Wenn du das .NET Core SDK 8 nimmst muss die EF (Entity Framework) Major Version ebenfalls 8 sein. In meinem Fall in dem Screenshot habe ich f\u00fcr alles Version 6 genommen, die rennt stabiler in Rider. EF ist eine Bibliothek welche ein einheitliches Interface bietet zum Zugriff auf verschiedene Datenbanken. Das Paket Microsoft.EntityFrameworkCore.Sqlite beinhaltet Treiber damit EF weis wie es mit SQLite-Datenbanken umgehen kann. Es gibt auch Treiber f\u00fcr MySQL, MSSQL uvm.","title":"Vorbereitung"},{"location":"Assignments/2XHBGM/database/#integration","text":"Lege eine neue Klasse an die von DbContext erbt. Darin kannst du den Pfad angeben an dem die Datenbank-Datei gespeichert wird. Bei anderen Datenbanken muss man dort die URL zum Server angeben, Username, Passwort etc. Zudem muss man durch \u00fcberschreiben der Methode OnConfiguring festlegen was passiert wenn der Datenbank-Kontext konfiguriert wird. Das noch leere \u00fcbergebene options -Objekt muss man konfigurieren. Durch Aufruf der UseSQlite -Methode mit einem Connection-String gibt man dem SQLite-Treiber alle Informationen die er ben\u00f6tigt um eine Datenbank anzulegen. Die DbSet<T> -Variablen muss man f\u00fcr jede Klasse anlegen die in der Datenbank gespeichert werden soll. In deinem Fall dann die Klasse ToDoItem . Hier habe ich den Beispielcode mit den WeatherForecasts erweitert dass diese in der Datenbank gespeichert werden k\u00f6nnen. public class DataContext : DbContext { private static string _dbPath = \"data.db\"; // The following configures EF to create a Sqlite database file protected override void OnConfiguring(DbContextOptionsBuilder options) { options.UseSqlite($\"Data Source={_dbPath}\"); } // A DbSet is a collection that can store and retrieve data // from the Database public DbSet<WeatherForecast> WeatherForecasts { get; set; } } Im WeatherForecastService kann man dann folgenderma\u00dfen Daten aus der Datenbank lesen und schreiben: // ctor (kurzform f\u00fcr Konstruktor) public WeatherForecastService() { // Neuen Datenbankkontext anlegen db = new DataContext(); // Sicherstellen dass die Datei erstellt wurde db.Database.EnsureCreated(); } public Task<WeatherForecast[]> GetForecastAsync(DateTime startDate) { // generiere zuf\u00e4llige daten var weatherData = Enumerable.Range(1, 5).Select(index => new WeatherForecast { Date = startDate.AddDays(index), TemperatureC = Random.Shared.Next(-20, 55), Summary = Summaries[Random.Shared.Next(Summaries.Length)] }); // f\u00fcge mehrere Elemente auf einmal in die Datenbank ein db.WeatherForecasts.AddRange(weatherData); // Schreibe alle zwischengespeicherten Daten // in die Datenbank db.SaveChanges(); // hole alle Daten aus der Datenbank und konvertiere // das Ergebnis in ein Array return Task.FromResult(db.WeatherForecasts.ToArray()); }","title":"Integration"},{"location":"Assignments/2XHBGM/database/#aufgabenstellung","text":"Installieren der NuGet-Pakete (wenn nicht bereits installiert) Datenbank-Kontext erstellen Daten in Datenbank schreiben Daten aus Datenbank lesen Daten nach Benutzer filtern Beispiel wie man im WeatherForecast filtern kann nach positiven Temperaturen: db.WeatherForecasts.Select(forecast => forecast.TemperatureC > 0);","title":"Aufgabenstellung"},{"location":"Assignments/2XHBGM/deployment/","text":"","title":"Deployment"},{"location":"Assignments/2XHBGM/fileupload/","text":"","title":"Fileupload"},{"location":"Assignments/2XHBGM/persistenttodo/","text":"ToDo Liste mit CSV persisitieren Unter \"persistieren\" versteht man das dauerhafte Speichern von Daten. Aktuell ist es so dass wenn man die Seite mit der ToDo Liste neu l\u00e4dt alle Daten verloren gegangen sind. Nicht sehr Sinnvoll wenn man eigentlich sp\u00e4ter daran erinnert werden m\u00f6chte. Werden die Daten l\u00e4nger gespeichert kommen wir dem Funktionsumfang einer normalen Webseite immer n\u00e4her. Um das zu \u00e4ndern kann man eine eigene Klasse schreiben welche einen Service startet der sich um das lesen und schreiben der Daten auf die Festplatte (in Dateien) k\u00fcmmert. Im Beispielcode ist bereits ein Service enthalten. Der WeatherForecastService . Dieser generiert am Server zuf\u00e4llige Daten und sendet diese dann zum Client. Auch die bisherige ToDo-App generierte (wenn auch nur tempor\u00e4r) eine Liste am Server. Schritt 1: Service erstellen Lege im Ordner /Data eine neue Klasse an mit dem Namen CSVService . Diese Klasse wird Daten laden, speichern, \u00e4ndern und l\u00f6schen k\u00f6nnen. Das UML Diagramm daf\u00fcr k\u00f6nnte z.B so aussehen: Lesen einer CSV-Datei kannst du von einer bestehenden Abgabe ableiten hier . Zum schreiben von CSV-Dateien gibt es auf StackOverflow sch\u00f6ne Eintr\u00e4ge . Um Daten aus der CSV Datei zu \u00e4ndern oder zu l\u00f6schen kannst du die komplette CSV-Datei in eine Liste lesen die \u00c4nderung bei der entsprechenden Id anwenden und die neue Liste wieder in die CSV-Datei speichern. Schritt 2: Service zug\u00e4nglich machen Um den fertig programmierten Service verwenden zu k\u00f6nnen musst du in der Datei Program.cs ein Singelton hinzuf\u00fcgen mit builder.Services.AddSingleton<servicename>(); . In deinem Fall lautet der Servicename wahrscheinlich CSVService. Wenn du die Klasse anders genannt hast musst du das entsprechend anpassen. Schritt 3: Service in Razorpages aufrufen In der ToDo.razor um den Singleton verwenden zu k\u00f6nnen musst du den Service injecten. Das geht mit @inject servicename variablenname . Im Code-Behind @code{...} . Kannst du dann mit z.B variablenname.LoadItems() auf den Service zugreifen. Die von LoadItems() zur\u00fcckgegebene Liste kan dann verwendet werden um sie in einer foreach-Schleife wie in der vorherigen abgabe anzuzeigen.","title":"Azure Anleitung ASP"},{"location":"Assignments/2XHBGM/persistenttodo/#todo-liste-mit-csv-persisitieren","text":"Unter \"persistieren\" versteht man das dauerhafte Speichern von Daten. Aktuell ist es so dass wenn man die Seite mit der ToDo Liste neu l\u00e4dt alle Daten verloren gegangen sind. Nicht sehr Sinnvoll wenn man eigentlich sp\u00e4ter daran erinnert werden m\u00f6chte. Werden die Daten l\u00e4nger gespeichert kommen wir dem Funktionsumfang einer normalen Webseite immer n\u00e4her. Um das zu \u00e4ndern kann man eine eigene Klasse schreiben welche einen Service startet der sich um das lesen und schreiben der Daten auf die Festplatte (in Dateien) k\u00fcmmert. Im Beispielcode ist bereits ein Service enthalten. Der WeatherForecastService . Dieser generiert am Server zuf\u00e4llige Daten und sendet diese dann zum Client. Auch die bisherige ToDo-App generierte (wenn auch nur tempor\u00e4r) eine Liste am Server.","title":"ToDo Liste mit CSV persisitieren"},{"location":"Assignments/2XHBGM/persistenttodo/#schritt-1-service-erstellen","text":"Lege im Ordner /Data eine neue Klasse an mit dem Namen CSVService . Diese Klasse wird Daten laden, speichern, \u00e4ndern und l\u00f6schen k\u00f6nnen. Das UML Diagramm daf\u00fcr k\u00f6nnte z.B so aussehen: Lesen einer CSV-Datei kannst du von einer bestehenden Abgabe ableiten hier . Zum schreiben von CSV-Dateien gibt es auf StackOverflow sch\u00f6ne Eintr\u00e4ge . Um Daten aus der CSV Datei zu \u00e4ndern oder zu l\u00f6schen kannst du die komplette CSV-Datei in eine Liste lesen die \u00c4nderung bei der entsprechenden Id anwenden und die neue Liste wieder in die CSV-Datei speichern.","title":"Schritt 1: Service erstellen"},{"location":"Assignments/2XHBGM/persistenttodo/#schritt-2-service-zuganglich-machen","text":"Um den fertig programmierten Service verwenden zu k\u00f6nnen musst du in der Datei Program.cs ein Singelton hinzuf\u00fcgen mit builder.Services.AddSingleton<servicename>(); . In deinem Fall lautet der Servicename wahrscheinlich CSVService. Wenn du die Klasse anders genannt hast musst du das entsprechend anpassen.","title":"Schritt 2: Service zug\u00e4nglich machen"},{"location":"Assignments/2XHBGM/persistenttodo/#schritt-3-service-in-razorpages-aufrufen","text":"In der ToDo.razor um den Singleton verwenden zu k\u00f6nnen musst du den Service injecten. Das geht mit @inject servicename variablenname . Im Code-Behind @code{...} . Kannst du dann mit z.B variablenname.LoadItems() auf den Service zugreifen. Die von LoadItems() zur\u00fcckgegebene Liste kan dann verwendet werden um sie in einer foreach-Schleife wie in der vorherigen abgabe anzuzeigen.","title":"Schritt 3: Service in Razorpages aufrufen"},{"location":"Assignments/3XHBGM/Index/","text":"layout: page title: \u00dcbungen 3XHBGM permalink: /Assignments/3XHBGM/Index menubar: false nav_exclude: true exclude: true nav: false","title":"Index"},{"location":"Assignments/3XHIF/WMC/Assignment1/","text":"Ziel Entwickle eine pers\u00f6nliche Webseite mit Remix, die einen von dir Steckbrief pr\u00e4sentiert. Voraussetzungen Grundkenntnisse in JavaScript Node.js und npm installiert Aufgaben 1. Projekt einrichten a) Erstelle ein neues Remix-Projekt mit dem Befehl: npx create-remix@latest Dokumentation siehe remix.run/docs 2. Grundlegendes Routing Erstelle folgende Routen: - / (Startseite) - /about (\u00dcber mich) - /contact (Kontakt) 3. Komponenten erstellen Erstelle eine Header-Komponente mit einem Navigationsmen\u00fc f\u00fcr die oben genannten Routen. Erstelle eine Footer-Komponente mit Copyright-Informationen. F\u00fcge die Komponenten in root.tsx ein 4. Seiteninhalte Startseite: Kurze Begr\u00fc\u00dfung und Einleitung \u00dcber mich: Ihr pers\u00f6nlicher Steckbrief (Name, Alter, Hobbys, F\u00e4higkeiten) Kontakt: Ein einfaches Kontaktformular (Name, E-Mail, Nachricht) Muss die Daten noch nicht tats\u00e4chlich absenden 5. Styling Verwende Tailwind CSS , um deine Komponenten und Seiten zu stylen. Abgabe Lade ein Protokoll auf Moodle hoch Erkl\u00e4re deine Vorgehensweise f\u00fcr jeden Punkt der Angabe Bewertungskriterien Funktionalit\u00e4t der Routing-Struktur Korrekte Implementierung und Verwendung von Komponenten Vollst\u00e4ndigkeit der geforderten Inhalte Code-Qualit\u00e4t und -Organisation Weitere Infos Remix - Quick Start React - Quick Start React - Empfohlene Vorgehensweise Tailwind CSS Beispiele","title":"Erstellen einer pers\u00f6nlichen Webseite mit Remix"},{"location":"Assignments/3XHIF/WMC/Assignment1/#ziel","text":"Entwickle eine pers\u00f6nliche Webseite mit Remix, die einen von dir Steckbrief pr\u00e4sentiert.","title":"Ziel"},{"location":"Assignments/3XHIF/WMC/Assignment1/#voraussetzungen","text":"Grundkenntnisse in JavaScript Node.js und npm installiert","title":"Voraussetzungen"},{"location":"Assignments/3XHIF/WMC/Assignment1/#aufgaben","text":"","title":"Aufgaben"},{"location":"Assignments/3XHIF/WMC/Assignment1/#1-projekt-einrichten","text":"a) Erstelle ein neues Remix-Projekt mit dem Befehl: npx create-remix@latest Dokumentation siehe remix.run/docs","title":"1. Projekt einrichten"},{"location":"Assignments/3XHIF/WMC/Assignment1/#2-grundlegendes-routing","text":"Erstelle folgende Routen: - / (Startseite) - /about (\u00dcber mich) - /contact (Kontakt)","title":"2. Grundlegendes Routing"},{"location":"Assignments/3XHIF/WMC/Assignment1/#3-komponenten-erstellen","text":"Erstelle eine Header-Komponente mit einem Navigationsmen\u00fc f\u00fcr die oben genannten Routen. Erstelle eine Footer-Komponente mit Copyright-Informationen. F\u00fcge die Komponenten in root.tsx ein","title":"3. Komponenten erstellen"},{"location":"Assignments/3XHIF/WMC/Assignment1/#4-seiteninhalte","text":"Startseite: Kurze Begr\u00fc\u00dfung und Einleitung \u00dcber mich: Ihr pers\u00f6nlicher Steckbrief (Name, Alter, Hobbys, F\u00e4higkeiten) Kontakt: Ein einfaches Kontaktformular (Name, E-Mail, Nachricht) Muss die Daten noch nicht tats\u00e4chlich absenden","title":"4. Seiteninhalte"},{"location":"Assignments/3XHIF/WMC/Assignment1/#5-styling","text":"Verwende Tailwind CSS , um deine Komponenten und Seiten zu stylen.","title":"5. Styling"},{"location":"Assignments/3XHIF/WMC/Assignment1/#abgabe","text":"Lade ein Protokoll auf Moodle hoch Erkl\u00e4re deine Vorgehensweise f\u00fcr jeden Punkt der Angabe","title":"Abgabe"},{"location":"Assignments/3XHIF/WMC/Assignment1/#bewertungskriterien","text":"Funktionalit\u00e4t der Routing-Struktur Korrekte Implementierung und Verwendung von Komponenten Vollst\u00e4ndigkeit der geforderten Inhalte Code-Qualit\u00e4t und -Organisation","title":"Bewertungskriterien"},{"location":"Assignments/3XHIF/WMC/Assignment1/#weitere-infos","text":"Remix - Quick Start React - Quick Start React - Empfohlene Vorgehensweise Tailwind CSS Beispiele","title":"Weitere Infos"},{"location":"Assignments/3XHIF/WMC/Assignment2/","text":"Ziel Erweitere deine pers\u00f6nliche Remix-Webseite aus der vorherigen \u00dcbung um eine interaktive, editierbare Liste von F\u00e4higkeiten oder Projekten. Voraussetzungen Abgeschlossene erste Aufgabenstellung (Grundlegende Remix-Webseite) Vertrautheit mit JavaScript-Array-Methoden (filter, map, find) Aufgaben 1. Erstellen einer neuen Route F\u00fcge eine neue Route /skills zu deiner Webseite hinzu. Aktualisiere die Header-Komponente, um einen Link zu dieser neuen Seite einzuf\u00fcgen. 2. Implementierung der Loader Funktion Erstelle eine loader -Funktion, die eine erste Liste von F\u00e4higkeiten oder Projekten vom Server l\u00e4dt. 3. Entwicklung einer wiederverwendbaren Komponente Erstelle eine SkillItem -Komponente, die einen einzelnen Listeneintrag darstellt. Diese Komponente sollte sowohl den Anzeigemodus als auch den Bearbeitungsmodus unterst\u00fctzen (conditional rendering). 4. Implementierung der Skills-Liste Verwende die useLoaderData -Hook, um die Daten aus der loader -Funktion zu erhalten. Rendere die Liste der F\u00e4higkeiten/Projekte unter Verwendung der SkillItem -Komponente und der map -Funktion. 5. Hinzuf\u00fcgen von Interaktivit\u00e4t Implementiere ein Textfeld zum Hinzuf\u00fcgen neuer F\u00e4higkeiten/Projekte. F\u00fcge Funktionen zum Bearbeiten und L\u00f6schen von Eintr\u00e4gen hinzu. 6. Filterung der Liste F\u00fcge ein Suchfeld hinzu, das die Liste der F\u00e4higkeiten/Projekte filtert. Verwende die filter -Methode, um die Suchergebnisse anzuzeigen. Implementiere die Filterung client-seitig f\u00fcr sofortige Ergebnisse. 7. Local Storage Integration Verwende den LocalStorage , um die Liste der F\u00e4higkeiten/Projekte zu speichern. Aktualisiere den Local Storage bei \u00c4nderungen. 8. (Optional) Fortgeschrittenes Styling Verwende CSS-Transitionen oder Animationen, um das Hinzuf\u00fcgen/Entfernen von Listenelementen zu visualisieren. kleiner Tipp: Mit ChatGPT o.\u00e4 geht das ganz gut dass man dem den aktuellen Code gibt und dazuschreibt \"F\u00fcge animationen hinzu\" Implementiere ein responsives Design f\u00fcr die Liste, z.B mit einer Flexbox. Abgabe Ein Protokoll in dem die Umsetzung jedes Punktes beschrieben wird Screenshots der Liste Ausz\u00fcge aus dem Code mit einer Beschreibung was dieser macht Der kopierte Code soll als Text eingef\u00fcgt werden. Dabei soll das Syntax-Highlighting erhalten bleiben. Verwende die Protokollvorlage","title":"Erweiterung der Remix-Webseite mit interaktiver Liste"},{"location":"Assignments/3XHIF/WMC/Assignment2/#ziel","text":"Erweitere deine pers\u00f6nliche Remix-Webseite aus der vorherigen \u00dcbung um eine interaktive, editierbare Liste von F\u00e4higkeiten oder Projekten.","title":"Ziel"},{"location":"Assignments/3XHIF/WMC/Assignment2/#voraussetzungen","text":"Abgeschlossene erste Aufgabenstellung (Grundlegende Remix-Webseite) Vertrautheit mit JavaScript-Array-Methoden (filter, map, find)","title":"Voraussetzungen"},{"location":"Assignments/3XHIF/WMC/Assignment2/#aufgaben","text":"","title":"Aufgaben"},{"location":"Assignments/3XHIF/WMC/Assignment2/#1-erstellen-einer-neuen-route","text":"F\u00fcge eine neue Route /skills zu deiner Webseite hinzu. Aktualisiere die Header-Komponente, um einen Link zu dieser neuen Seite einzuf\u00fcgen.","title":"1. Erstellen einer neuen Route"},{"location":"Assignments/3XHIF/WMC/Assignment2/#2-implementierung-der-loader-funktion","text":"Erstelle eine loader -Funktion, die eine erste Liste von F\u00e4higkeiten oder Projekten vom Server l\u00e4dt.","title":"2. Implementierung der Loader Funktion"},{"location":"Assignments/3XHIF/WMC/Assignment2/#3-entwicklung-einer-wiederverwendbaren-komponente","text":"Erstelle eine SkillItem -Komponente, die einen einzelnen Listeneintrag darstellt. Diese Komponente sollte sowohl den Anzeigemodus als auch den Bearbeitungsmodus unterst\u00fctzen (conditional rendering).","title":"3. Entwicklung einer wiederverwendbaren Komponente"},{"location":"Assignments/3XHIF/WMC/Assignment2/#4-implementierung-der-skills-liste","text":"Verwende die useLoaderData -Hook, um die Daten aus der loader -Funktion zu erhalten. Rendere die Liste der F\u00e4higkeiten/Projekte unter Verwendung der SkillItem -Komponente und der map -Funktion.","title":"4. Implementierung der Skills-Liste"},{"location":"Assignments/3XHIF/WMC/Assignment2/#5-hinzufugen-von-interaktivitat","text":"Implementiere ein Textfeld zum Hinzuf\u00fcgen neuer F\u00e4higkeiten/Projekte. F\u00fcge Funktionen zum Bearbeiten und L\u00f6schen von Eintr\u00e4gen hinzu.","title":"5. Hinzuf\u00fcgen von Interaktivit\u00e4t"},{"location":"Assignments/3XHIF/WMC/Assignment2/#6-filterung-der-liste","text":"F\u00fcge ein Suchfeld hinzu, das die Liste der F\u00e4higkeiten/Projekte filtert. Verwende die filter -Methode, um die Suchergebnisse anzuzeigen. Implementiere die Filterung client-seitig f\u00fcr sofortige Ergebnisse.","title":"6. Filterung der Liste"},{"location":"Assignments/3XHIF/WMC/Assignment2/#7-local-storage-integration","text":"Verwende den LocalStorage , um die Liste der F\u00e4higkeiten/Projekte zu speichern. Aktualisiere den Local Storage bei \u00c4nderungen.","title":"7. Local Storage Integration"},{"location":"Assignments/3XHIF/WMC/Assignment2/#8-optional-fortgeschrittenes-styling","text":"Verwende CSS-Transitionen oder Animationen, um das Hinzuf\u00fcgen/Entfernen von Listenelementen zu visualisieren. kleiner Tipp: Mit ChatGPT o.\u00e4 geht das ganz gut dass man dem den aktuellen Code gibt und dazuschreibt \"F\u00fcge animationen hinzu\" Implementiere ein responsives Design f\u00fcr die Liste, z.B mit einer Flexbox.","title":"8. (Optional) Fortgeschrittenes Styling"},{"location":"Assignments/3XHIF/WMC/Assignment2/#abgabe","text":"Ein Protokoll in dem die Umsetzung jedes Punktes beschrieben wird Screenshots der Liste Ausz\u00fcge aus dem Code mit einer Beschreibung was dieser macht Der kopierte Code soll als Text eingef\u00fcgt werden. Dabei soll das Syntax-Highlighting erhalten bleiben. Verwende die Protokollvorlage","title":"Abgabe"},{"location":"Assignments/3XHIF/WMC/Assignment3/","text":"Aufgabenstellung Erstelle eine Remix-Anwendung f\u00fcr einen einfachen Webshop. Die Anwendung soll Produkte aus einer SQLite-Datenbank laden und anzeigen k\u00f6nnen. Projekt-Setup Erstelle ein neues Remix-Projekt: npx create-remix@latest Installiere die ben\u00f6tigten Abh\u00e4ngigkeiten: npm install sqlite3 --save Lade dir die bestehende Datenbank webshop.db von Moodle herrunter, oder erstelle eine neue SQLite-Datenbank mit einer Produkttabelle: CREATE TABLE products ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL, description TEXT ); Datenbank-Konfiguration Verbinde dich zur Datenbank: const sqlite3 = require('sqlite3') // Connect to the database specified by a single file path const db = new sqlite3.Database('./webshop.db', (err) => { if (err) { console.error('Error opening database', err.message); } else { console.log('Connected to the SQLite database.'); } }); Alle Produkte laden const products = db .prepare(\"SELECT * FROM products\") .all() as Product[]; Einzelnes Produkt laden Erstelle eine Route-Datei app/routes/products.$id.tsx . Verwende useParams um die Daten aus der URL abzufragen. \u00b4 id = useParams... const product = db .prepare(\"SELECT * FROM products WHERE id = ?\") .get(id); if (!product) { throw new Response(\"Produkt nicht gefunden\", { status: 404 }); } Aufgaben f\u00fcrs Protokoll Implementiere die Basisfunktionalit\u00e4t wie oben beschrieben in Remix Erweitere die Anwendung um: Ein Formular zum Hinzuf\u00fcgen neuer Produkte Eine Suchfunktion f\u00fcr Produkte Eine Kategorisierung von Produkten Bonuspunkte (freiwillig) Implementiere Fehlerbehandlung f\u00fcr die Datenbankabfragen F\u00fcge Validierung f\u00fcr das Produktformular hinzu Implementiere eine Sortierfunktion f\u00fcr die Produktliste Erstelle eine Shopping Cart Funktionalit\u00e4t Tipps Nutze die Remix-Dokumentation f\u00fcr Details zur Action-Funktion bei Formularen Denke an die Fehlerbehandlung bei Datenbankoperationen","title":"Remix SQLite \u00dcbungsaufgabe"},{"location":"Assignments/3XHIF/WMC/Assignment3/#aufgabenstellung","text":"Erstelle eine Remix-Anwendung f\u00fcr einen einfachen Webshop. Die Anwendung soll Produkte aus einer SQLite-Datenbank laden und anzeigen k\u00f6nnen.","title":"Aufgabenstellung"},{"location":"Assignments/3XHIF/WMC/Assignment3/#projekt-setup","text":"Erstelle ein neues Remix-Projekt: npx create-remix@latest Installiere die ben\u00f6tigten Abh\u00e4ngigkeiten: npm install sqlite3 --save Lade dir die bestehende Datenbank webshop.db von Moodle herrunter, oder erstelle eine neue SQLite-Datenbank mit einer Produkttabelle: CREATE TABLE products ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, price REAL NOT NULL, description TEXT );","title":"Projekt-Setup"},{"location":"Assignments/3XHIF/WMC/Assignment3/#datenbank-konfiguration","text":"Verbinde dich zur Datenbank: const sqlite3 = require('sqlite3') // Connect to the database specified by a single file path const db = new sqlite3.Database('./webshop.db', (err) => { if (err) { console.error('Error opening database', err.message); } else { console.log('Connected to the SQLite database.'); } });","title":"Datenbank-Konfiguration"},{"location":"Assignments/3XHIF/WMC/Assignment3/#alle-produkte-laden","text":"const products = db .prepare(\"SELECT * FROM products\") .all() as Product[];","title":"Alle Produkte laden"},{"location":"Assignments/3XHIF/WMC/Assignment3/#einzelnes-produkt-laden","text":"Erstelle eine Route-Datei app/routes/products.$id.tsx . Verwende useParams um die Daten aus der URL abzufragen. \u00b4 id = useParams... const product = db .prepare(\"SELECT * FROM products WHERE id = ?\") .get(id); if (!product) { throw new Response(\"Produkt nicht gefunden\", { status: 404 }); }","title":"Einzelnes Produkt laden"},{"location":"Assignments/3XHIF/WMC/Assignment3/#aufgaben-furs-protokoll","text":"Implementiere die Basisfunktionalit\u00e4t wie oben beschrieben in Remix Erweitere die Anwendung um: Ein Formular zum Hinzuf\u00fcgen neuer Produkte Eine Suchfunktion f\u00fcr Produkte Eine Kategorisierung von Produkten","title":"Aufgaben f\u00fcrs Protokoll"},{"location":"Assignments/3XHIF/WMC/Assignment3/#bonuspunkte-freiwillig","text":"Implementiere Fehlerbehandlung f\u00fcr die Datenbankabfragen F\u00fcge Validierung f\u00fcr das Produktformular hinzu Implementiere eine Sortierfunktion f\u00fcr die Produktliste Erstelle eine Shopping Cart Funktionalit\u00e4t","title":"Bonuspunkte (freiwillig)"},{"location":"Assignments/3XHIF/WMC/Assignment3/#tipps","text":"Nutze die Remix-Dokumentation f\u00fcr Details zur Action-Funktion bei Formularen Denke an die Fehlerbehandlung bei Datenbankoperationen","title":"Tipps"},{"location":"Assignments/3XHIF/WMC/Assignment4/","text":"Ziel Erweitere deine Remix-Webseite um ein Kontaktformular mit Validierungs- und Feedback-Funktionen. Voraussetzungen Abgeschlossene erste und zweite Aufgabenstellung Grundlegendes Verst\u00e4ndnis von Remix-Actions und HTML-Formularen Aufgaben 1. Erweiterung der Kontaktseite \u00dcberarbeite die bestehende /contact -Route aus \u00dcbung 1 Implementiere ein erweitertes Kontaktformular mit folgenden Feldern: Name (Vorname, Nachname) E-Mail Telefonnummer (optional) Betreff (Dropdown-Auswahl) Nachricht Bevorzugte Kontaktmethode (Radio-Buttons) 2. Implementierung einer Remix-Action Erstelle eine action -Funktion f\u00fcr die Kontaktseite. Implementiere eine serverseitige Formularvalidierung in der action -Funktion. Simuliere das Senden einer E-Mail (z.B. durch Logging der Formulardaten in einer Datei). 3. Clientseitige Formularvalidierung Implementiere eine reaktive, clientseitige Validierung f\u00fcr alle Formularfelder. Zeige Fehlermeldungen in Echtzeit an, w\u00e4hrend der Benutzer tippt. Deaktiviere den Submit-Button, wenn das Formular ung\u00fcltig ist. 4. Reaktive Daten-Bindung Implementiere ein Two-Way-Binding f\u00fcr alle Formularfelder. Erstelle eine Vorschau der Formulardaten, die sich in Echtzeit aktualisiert. 5. Fortgeschrittenes Fehler-Handling Nutze die useActionData Hook, um Fehlermeldungen vom Server anzuzeigen. Implementiere eine Fehleranzeige f\u00fcr Netzwerkfehler oder serverseitige Probleme. 6. Fortgeschrittene UI-Elemente Implementieren Sie einen mehrstufigen Formularprozess F\u00fcgen Sie einen Ladeindikator hinzu, der w\u00e4hrend der Formular\u00fcbermittlung angezeigt wird. 7. Zug\u00e4nglichkeit und UX Stellen Sie sicher, dass das Formular vollst\u00e4ndig tastaturzug\u00e4nglich ist. TabIndex einstellen Placeholder f\u00fcr die Textfelder Implementieren Sie ARIA-Attribute f\u00fcr verbesserte Zug\u00e4nglichkeit. F\u00fcgen Sie Fokus-Management hinzu, um nach der Formular\u00fcbermittlung auf Fehlermeldungen zu fokussieren. Abgabe Bewertungskriterien Korrekte Implementierung der Remix-Action und clientseitiger Validierung Effektive Nutzung reaktiver Daten-Bindung und Echtzeit-Aktualisierungen Robustes Fehler-Handling und Benutzer-Feedback Zug\u00e4nglichkeit und Benutzerfreundlichkeit des Formulars Code-Qualit\u00e4t und -Organisation","title":"Implementierung eines Kontaktformulars mit Remix"},{"location":"Assignments/3XHIF/WMC/Assignment4/#ziel","text":"Erweitere deine Remix-Webseite um ein Kontaktformular mit Validierungs- und Feedback-Funktionen.","title":"Ziel"},{"location":"Assignments/3XHIF/WMC/Assignment4/#voraussetzungen","text":"Abgeschlossene erste und zweite Aufgabenstellung Grundlegendes Verst\u00e4ndnis von Remix-Actions und HTML-Formularen","title":"Voraussetzungen"},{"location":"Assignments/3XHIF/WMC/Assignment4/#aufgaben","text":"","title":"Aufgaben"},{"location":"Assignments/3XHIF/WMC/Assignment4/#1-erweiterung-der-kontaktseite","text":"\u00dcberarbeite die bestehende /contact -Route aus \u00dcbung 1 Implementiere ein erweitertes Kontaktformular mit folgenden Feldern: Name (Vorname, Nachname) E-Mail Telefonnummer (optional) Betreff (Dropdown-Auswahl) Nachricht Bevorzugte Kontaktmethode (Radio-Buttons)","title":"1. Erweiterung der Kontaktseite"},{"location":"Assignments/3XHIF/WMC/Assignment4/#2-implementierung-einer-remix-action","text":"Erstelle eine action -Funktion f\u00fcr die Kontaktseite. Implementiere eine serverseitige Formularvalidierung in der action -Funktion. Simuliere das Senden einer E-Mail (z.B. durch Logging der Formulardaten in einer Datei).","title":"2. Implementierung einer Remix-Action"},{"location":"Assignments/3XHIF/WMC/Assignment4/#3-clientseitige-formularvalidierung","text":"Implementiere eine reaktive, clientseitige Validierung f\u00fcr alle Formularfelder. Zeige Fehlermeldungen in Echtzeit an, w\u00e4hrend der Benutzer tippt. Deaktiviere den Submit-Button, wenn das Formular ung\u00fcltig ist.","title":"3. Clientseitige Formularvalidierung"},{"location":"Assignments/3XHIF/WMC/Assignment4/#4-reaktive-daten-bindung","text":"Implementiere ein Two-Way-Binding f\u00fcr alle Formularfelder. Erstelle eine Vorschau der Formulardaten, die sich in Echtzeit aktualisiert.","title":"4. Reaktive Daten-Bindung"},{"location":"Assignments/3XHIF/WMC/Assignment4/#5-fortgeschrittenes-fehler-handling","text":"Nutze die useActionData Hook, um Fehlermeldungen vom Server anzuzeigen. Implementiere eine Fehleranzeige f\u00fcr Netzwerkfehler oder serverseitige Probleme.","title":"5. Fortgeschrittenes Fehler-Handling"},{"location":"Assignments/3XHIF/WMC/Assignment4/#6-fortgeschrittene-ui-elemente","text":"Implementieren Sie einen mehrstufigen Formularprozess F\u00fcgen Sie einen Ladeindikator hinzu, der w\u00e4hrend der Formular\u00fcbermittlung angezeigt wird.","title":"6. Fortgeschrittene UI-Elemente"},{"location":"Assignments/3XHIF/WMC/Assignment4/#7-zuganglichkeit-und-ux","text":"Stellen Sie sicher, dass das Formular vollst\u00e4ndig tastaturzug\u00e4nglich ist. TabIndex einstellen Placeholder f\u00fcr die Textfelder Implementieren Sie ARIA-Attribute f\u00fcr verbesserte Zug\u00e4nglichkeit. F\u00fcgen Sie Fokus-Management hinzu, um nach der Formular\u00fcbermittlung auf Fehlermeldungen zu fokussieren.","title":"7. Zug\u00e4nglichkeit und UX"},{"location":"Assignments/3XHIF/WMC/Assignment4/#abgabe","text":"","title":"Abgabe"},{"location":"Assignments/3XHIF/WMC/Assignment4/#bewertungskriterien","text":"Korrekte Implementierung der Remix-Action und clientseitiger Validierung Effektive Nutzung reaktiver Daten-Bindung und Echtzeit-Aktualisierungen Robustes Fehler-Handling und Benutzer-Feedback Zug\u00e4nglichkeit und Benutzerfreundlichkeit des Formulars Code-Qualit\u00e4t und -Organisation","title":"Bewertungskriterien"},{"location":"Assignments/3XHIF/WMC/Index/","text":"\u00dcbung 1 \u00dcbung 2 \u00dcbung 3 \u00dcbung 4 \u00dcbung 5 \u00dcbung 6","title":"Aufgabensammlung f\u00fcr den 3. Jahrgang HIF in WMC"},{"location":"Assignments/3XHIF/WMC/Projektabgabe/","text":"Projektarbeit WMC Folgendes soll das Projekt umfassen: CRUD-Operationen auf eine Datenbank deiner Wahl (MySQL, SQlite, MongoDB etc.) Login-M\u00f6glichkeit (evtl. verschiedene Rollen z.B Gast/User/Admin) Landing-Page (z.B mittels Lovable) Arbeiten zu zweit Abgabe via Github Wer all diese Punkte ordnungsgem\u00e4\u00df erf\u00fcllt bekommt einen 2er. F\u00fcr ein \"Sehr Gut\" m\u00fcssen noch folgende Punkte erf\u00fcllt werden: Deployment auf Azure Vercel o.\u00e4 Am Ende des Projektes muss jedes Projekt in einer 5-Min\u00fctigen Pr\u00e4sentation vorgestellt werden.","title":"Projekt"},{"location":"Assignments/3XHIF/WMC/Projektabgabe/#projektarbeit-wmc","text":"Folgendes soll das Projekt umfassen: CRUD-Operationen auf eine Datenbank deiner Wahl (MySQL, SQlite, MongoDB etc.) Login-M\u00f6glichkeit (evtl. verschiedene Rollen z.B Gast/User/Admin) Landing-Page (z.B mittels Lovable) Arbeiten zu zweit Abgabe via Github Wer all diese Punkte ordnungsgem\u00e4\u00df erf\u00fcllt bekommt einen 2er. F\u00fcr ein \"Sehr Gut\" m\u00fcssen noch folgende Punkte erf\u00fcllt werden: Deployment auf Azure Vercel o.\u00e4 Am Ende des Projektes muss jedes Projekt in einer 5-Min\u00fctigen Pr\u00e4sentation vorgestellt werden.","title":"Projektarbeit WMC"},{"location":"Assignments/3XHIF/WMC/Task2/","text":"State Management mit useState Lernziele Verstehen, was State in React ist und wof\u00fcr er verwendet wird Die useState-Hook korrekt anwenden Einfache und komplexere Zustandsverwaltung in Komponenten implementieren Aufgaben Z\u00e4hler-Komponente erstellen (40 Minuten) Neue Datei app/components/Counter.tsx erstellen Eine Komponente mit einem Z\u00e4hler und Buttons zum Erh\u00f6hen/Verringern implementieren useState f\u00fcr die Verwaltung des Z\u00e4hlerwerts verwenden Die Counter-Komponente in die Home-Seite einbinden Beispiel + offizielle Dokumentation Formular mit kontrollierten Komponenten (40 Minuten) Neue Route app/routes/form.tsx erstellen Ein Formular mit Eingabefeldern (z.B. Name, Email) erstellen useState f\u00fcr jedes Eingabefeld verwenden Die Eingabewerte im State speichern und anzeigen \u00c4hnliches Beispiel Todo-Liste implementieren (40 Minuten) Neue Komponente app/components/TodoList.tsx erstellen useState f\u00fcr die Verwaltung der Todo-Liste und des Eingabefelds verwenden Funktionen zum Hinzuf\u00fcgen und Entfernen von Todos implementieren Die TodoList-Komponente in eine neue Route einbinden >> Zweiter Teil der \u00dcbung","title":"State Management mit useState und Seiteneffekte mit useEffect"},{"location":"Assignments/3XHIF/WMC/Task2/#state-management-mit-usestate","text":"","title":"State Management mit useState"},{"location":"Assignments/3XHIF/WMC/Task2/#lernziele","text":"Verstehen, was State in React ist und wof\u00fcr er verwendet wird Die useState-Hook korrekt anwenden Einfache und komplexere Zustandsverwaltung in Komponenten implementieren","title":"Lernziele"},{"location":"Assignments/3XHIF/WMC/Task2/#aufgaben","text":"Z\u00e4hler-Komponente erstellen (40 Minuten) Neue Datei app/components/Counter.tsx erstellen Eine Komponente mit einem Z\u00e4hler und Buttons zum Erh\u00f6hen/Verringern implementieren useState f\u00fcr die Verwaltung des Z\u00e4hlerwerts verwenden Die Counter-Komponente in die Home-Seite einbinden Beispiel + offizielle Dokumentation Formular mit kontrollierten Komponenten (40 Minuten) Neue Route app/routes/form.tsx erstellen Ein Formular mit Eingabefeldern (z.B. Name, Email) erstellen useState f\u00fcr jedes Eingabefeld verwenden Die Eingabewerte im State speichern und anzeigen \u00c4hnliches Beispiel Todo-Liste implementieren (40 Minuten) Neue Komponente app/components/TodoList.tsx erstellen useState f\u00fcr die Verwaltung der Todo-Liste und des Eingabefelds verwenden Funktionen zum Hinzuf\u00fcgen und Entfernen von Todos implementieren Die TodoList-Komponente in eine neue Route einbinden >> Zweiter Teil der \u00dcbung","title":"Aufgaben"},{"location":"Assignments/3XHIF/WMC/Task2b/","text":"Seiteneffekte mit useEffect Lernziele Verstehen, was Seiteneffekte in React sind und wann man useEffect ben\u00f6tigt useEffect mit verschiedenen Dependency-Arrays anwenden API-Aufrufe mit useEffect durchf\u00fchren und die Ergebnisse im State speichern Beispiele aus der Dokumentation react.dev useEffect Aufgaben Daten von einer API fetchen (40 Minuten) Neue Route app/routes/users.tsx erstellen useEffect verwenden, um Benutzerdaten von der JSONPlaceholder API zu fetchen Die gefetchten Daten im State speichern und in der Komponente anzeigen Einen Lade-Indikator implementieren, w\u00e4hrend die Daten geladen werden useEffect mit verschiedenen Dependencies (40 Minuten) Eine Komponente erstellen, die auf \u00c4nderungen in Props oder State reagiert Verschiedene useEffect-Hooks mit unterschiedlichen Dependency-Arrays implementieren Die Auswirkungen der verschiedenen Dependency-Arrays beobachten und diskutieren Die Komponente kann alles m\u00f6gliche Anzeigen. Z.b ein Profil, einen Termin etc. Cleanup-Funktionen verstehen und anwenden (40 Minuten) Eine Komponente erstellen, die nach einem Buttonklick einen Timer startet (z.B. mit setInterval) useEffect mit einer Cleanup-Funktion implementieren, um den Timer zu stoppen Die Notwendigkeit von Cleanup-Funktionen erkl\u00e4ren und weitere Beispiele diskutieren Die Dauer des Timers soll angezeigt werden","title":"State Management mit useState und Seiteneffekte mit useEffect"},{"location":"Assignments/3XHIF/WMC/Task2b/#seiteneffekte-mit-useeffect","text":"","title":"Seiteneffekte mit useEffect"},{"location":"Assignments/3XHIF/WMC/Task2b/#lernziele","text":"Verstehen, was Seiteneffekte in React sind und wann man useEffect ben\u00f6tigt useEffect mit verschiedenen Dependency-Arrays anwenden API-Aufrufe mit useEffect durchf\u00fchren und die Ergebnisse im State speichern","title":"Lernziele"},{"location":"Assignments/3XHIF/WMC/Task2b/#beispiele-aus-der-dokumentation","text":"react.dev useEffect","title":"Beispiele aus der Dokumentation"},{"location":"Assignments/3XHIF/WMC/Task2b/#aufgaben","text":"Daten von einer API fetchen (40 Minuten) Neue Route app/routes/users.tsx erstellen useEffect verwenden, um Benutzerdaten von der JSONPlaceholder API zu fetchen Die gefetchten Daten im State speichern und in der Komponente anzeigen Einen Lade-Indikator implementieren, w\u00e4hrend die Daten geladen werden useEffect mit verschiedenen Dependencies (40 Minuten) Eine Komponente erstellen, die auf \u00c4nderungen in Props oder State reagiert Verschiedene useEffect-Hooks mit unterschiedlichen Dependency-Arrays implementieren Die Auswirkungen der verschiedenen Dependency-Arrays beobachten und diskutieren Die Komponente kann alles m\u00f6gliche Anzeigen. Z.b ein Profil, einen Termin etc. Cleanup-Funktionen verstehen und anwenden (40 Minuten) Eine Komponente erstellen, die nach einem Buttonklick einen Timer startet (z.B. mit setInterval) useEffect mit einer Cleanup-Funktion implementieren, um den Timer zu stoppen Die Notwendigkeit von Cleanup-Funktionen erkl\u00e4ren und weitere Beispiele diskutieren Die Dauer des Timers soll angezeigt werden","title":"Aufgaben"},{"location":"Assignments/3XHIF/WMC/Task3/","text":"Lernziele Dynamische Routen in Remix erstellen und verwenden Daten mit Remix-spezifischen Funktionen wie loader laden Formulare und Aktionen in Remix implementieren Aufgaben Dynamische Routen in Remix (40 Minuten) Neue dynamische Route app/routes/users/$userId.tsx erstellen Die useParams -Hook verwenden, um die userId aus der URL zu extrahieren Benutzerdetails basierend auf der userId von der JSONPlaceholder API laden Links zu den Benutzerdetailseiten auf der Benutzer\u00fcbersichtsseite hinzuf\u00fcgen Daten laden mit loader-Funktionen (40 Minuten) In der app/routes/users.tsx und app/routes/users/$userId.tsx loader-Funktionen implementieren Die geladenen Daten mit der useLoaderData -Hook in den Komponenten verwenden Den Unterschied zwischen client- und serverseitigem Daten-Fetching in Remix erkl\u00e4ren Formulare und Aktionen in Remix (40 Minuten) Neue Route app/routes/create-user.tsx erstellen Ein Formular zur Erstellung eines neuen Benutzers implementieren Eine action-Funktion erstellen, die die Formulardaten verarbeitet Nach erfolgreicher Erstellung zur Benutzer\u00fcbersichtsseite umleiten","title":"Routing und Datenmanagement"},{"location":"Assignments/3XHIF/WMC/Task3/#lernziele","text":"Dynamische Routen in Remix erstellen und verwenden Daten mit Remix-spezifischen Funktionen wie loader laden Formulare und Aktionen in Remix implementieren","title":"Lernziele"},{"location":"Assignments/3XHIF/WMC/Task3/#aufgaben","text":"Dynamische Routen in Remix (40 Minuten) Neue dynamische Route app/routes/users/$userId.tsx erstellen Die useParams -Hook verwenden, um die userId aus der URL zu extrahieren Benutzerdetails basierend auf der userId von der JSONPlaceholder API laden Links zu den Benutzerdetailseiten auf der Benutzer\u00fcbersichtsseite hinzuf\u00fcgen Daten laden mit loader-Funktionen (40 Minuten) In der app/routes/users.tsx und app/routes/users/$userId.tsx loader-Funktionen implementieren Die geladenen Daten mit der useLoaderData -Hook in den Komponenten verwenden Den Unterschied zwischen client- und serverseitigem Daten-Fetching in Remix erkl\u00e4ren Formulare und Aktionen in Remix (40 Minuten) Neue Route app/routes/create-user.tsx erstellen Ein Formular zur Erstellung eines neuen Benutzers implementieren Eine action-Funktion erstellen, die die Formulardaten verarbeitet Nach erfolgreicher Erstellung zur Benutzer\u00fcbersichtsseite umleiten","title":"Aufgaben"},{"location":"Assignments/3XHIF/WMC/Task4/","text":"Lernziele Prisma in ein Remix-Projekt einrichten und konfigurieren Ein einfaches Datenbankschema erstellen und migrieren CRUD-Operationen mit Prisma implementieren Remix action-Funktionen f\u00fcr Datenbankoperationen verwenden Aufgaben Prisma einrichten und konfigurieren (40 Minuten) Prisma in das Projekt installieren Die Prisma-Konfigurationsdatei erstellen und anpassen Verbindung zur Datenbank herstellen (z.B. SQLite f\u00fcr einfache Entwicklung) Einfaches Datenbankschema erstellen (40 Minuten) Ein Prisma-Schema f\u00fcr eine einfache Entit\u00e4t (z.B. \"Post\" mit Titel und Inhalt) definieren Die erste Migration erstellen und anwenden Den Prisma-Client generieren CRUD-Operationen implementieren (40 Minuten) Neue Route app/routes/posts/index.tsx f\u00fcr die Auflistung aller Posts erstellen Eine loader-Funktion implementieren, die alle Posts aus der Datenbank abruft Neue Route app/routes/posts/new.tsx f\u00fcr das Erstellen neuer Posts erstellen Eine action-Funktion in app/routes/posts/new.tsx implementieren, die einen neuen Post in der Datenbank erstellt Formular f\u00fcr das Erstellen neuer Posts mit der action-Funktion verbinden","title":"Datenbankintegration mit Prisma"},{"location":"Assignments/3XHIF/WMC/Task4/#lernziele","text":"Prisma in ein Remix-Projekt einrichten und konfigurieren Ein einfaches Datenbankschema erstellen und migrieren CRUD-Operationen mit Prisma implementieren Remix action-Funktionen f\u00fcr Datenbankoperationen verwenden","title":"Lernziele"},{"location":"Assignments/3XHIF/WMC/Task4/#aufgaben","text":"Prisma einrichten und konfigurieren (40 Minuten) Prisma in das Projekt installieren Die Prisma-Konfigurationsdatei erstellen und anpassen Verbindung zur Datenbank herstellen (z.B. SQLite f\u00fcr einfache Entwicklung) Einfaches Datenbankschema erstellen (40 Minuten) Ein Prisma-Schema f\u00fcr eine einfache Entit\u00e4t (z.B. \"Post\" mit Titel und Inhalt) definieren Die erste Migration erstellen und anwenden Den Prisma-Client generieren CRUD-Operationen implementieren (40 Minuten) Neue Route app/routes/posts/index.tsx f\u00fcr die Auflistung aller Posts erstellen Eine loader-Funktion implementieren, die alle Posts aus der Datenbank abruft Neue Route app/routes/posts/new.tsx f\u00fcr das Erstellen neuer Posts erstellen Eine action-Funktion in app/routes/posts/new.tsx implementieren, die einen neuen Post in der Datenbank erstellt Formular f\u00fcr das Erstellen neuer Posts mit der action-Funktion verbinden","title":"Aufgaben"},{"location":"Assignments/3XHIF/WMC/Task5/","text":"Lernziele Benutzerregistrierung und Login mit Remix und Prisma implementieren Gesch\u00fctzte Routen erstellen Session-Management in Remix verstehen und anwenden Aufgaben Benutzerregistrierung implementieren (40 Minuten) Prisma-Schema um ein User-Modell erweitern Neue Route app/routes/auth/register.tsx erstellen Registrierungsformular erstellen action-Funktion f\u00fcr die Benutzerregistrierung implementieren, die Benutzerdaten in der Datenbank speichert Login implementieren (40 Minuten) Neue Route app/routes/auth/login.tsx erstellen Login-Formular erstellen action-Funktion f\u00fcr den Login implementieren, die Benutzerdaten \u00fcberpr\u00fcft und eine Session erstellt Session-Cookie mit Remix-Funktionen setzen Gesch\u00fctzte Routen und Session-Management (40 Minuten) Hilfsfunktion zum \u00dcberpr\u00fcfen der Benutzerauthentifizierung erstellen Gesch\u00fctzte Route (z.B. app/routes/dashboard.tsx ) erstellen loader-Funktion f\u00fcr die gesch\u00fctzte Route implementieren, die die Authentifizierung \u00fcberpr\u00fcft Logout-Funktionalit\u00e4t implementieren, die die Session beendet","title":"Authentifizierung in Remix"},{"location":"Assignments/3XHIF/WMC/Task5/#lernziele","text":"Benutzerregistrierung und Login mit Remix und Prisma implementieren Gesch\u00fctzte Routen erstellen Session-Management in Remix verstehen und anwenden","title":"Lernziele"},{"location":"Assignments/3XHIF/WMC/Task5/#aufgaben","text":"Benutzerregistrierung implementieren (40 Minuten) Prisma-Schema um ein User-Modell erweitern Neue Route app/routes/auth/register.tsx erstellen Registrierungsformular erstellen action-Funktion f\u00fcr die Benutzerregistrierung implementieren, die Benutzerdaten in der Datenbank speichert Login implementieren (40 Minuten) Neue Route app/routes/auth/login.tsx erstellen Login-Formular erstellen action-Funktion f\u00fcr den Login implementieren, die Benutzerdaten \u00fcberpr\u00fcft und eine Session erstellt Session-Cookie mit Remix-Funktionen setzen Gesch\u00fctzte Routen und Session-Management (40 Minuten) Hilfsfunktion zum \u00dcberpr\u00fcfen der Benutzerauthentifizierung erstellen Gesch\u00fctzte Route (z.B. app/routes/dashboard.tsx ) erstellen loader-Funktion f\u00fcr die gesch\u00fctzte Route implementieren, die die Authentifizierung \u00fcberpr\u00fcft Logout-Funktionalit\u00e4t implementieren, die die Session beendet","title":"Aufgaben"},{"location":"Assignments/3XHIF/WMC/Task6/","text":"Lernziele Verstehen, was PWAs sind und welche Vorteile sie bieten Einen Service Worker f\u00fcr eine Remix-Anwendung einrichten Offline-Funktionalit\u00e4t implementieren Ein App-Manifest erstellen und konfigurieren Aufgaben Service Worker einrichten (40 Minuten) Einen einfachen Service Worker erstellen Den Service Worker in der Remix-Anwendung registrieren Grundlegende Caching-Strategien implementieren Offline-Funktionalit\u00e4t implementieren (40 Minuten) Statische Assets f\u00fcr Offline-Nutzung cachen Eine Offline-Fallback-Seite erstellen Die Offline-Funktionalit\u00e4t testen und debuggen App-Manifest erstellen und konfigurieren (40 Minuten) Ein Web App Manifest erstellen Das Manifest in der Remix-Anwendung einbinden Die PWA auf verschiedenen Ger\u00e4ten testen und installieren","title":"Progressive Web App (PWA) Grundlagen"},{"location":"Assignments/3XHIF/WMC/Task6/#lernziele","text":"Verstehen, was PWAs sind und welche Vorteile sie bieten Einen Service Worker f\u00fcr eine Remix-Anwendung einrichten Offline-Funktionalit\u00e4t implementieren Ein App-Manifest erstellen und konfigurieren","title":"Lernziele"},{"location":"Assignments/3XHIF/WMC/Task6/#aufgaben","text":"Service Worker einrichten (40 Minuten) Einen einfachen Service Worker erstellen Den Service Worker in der Remix-Anwendung registrieren Grundlegende Caching-Strategien implementieren Offline-Funktionalit\u00e4t implementieren (40 Minuten) Statische Assets f\u00fcr Offline-Nutzung cachen Eine Offline-Fallback-Seite erstellen Die Offline-Funktionalit\u00e4t testen und debuggen App-Manifest erstellen und konfigurieren (40 Minuten) Ein Web App Manifest erstellen Das Manifest in der Remix-Anwendung einbinden Die PWA auf verschiedenen Ger\u00e4ten testen und installieren","title":"Aufgaben"},{"location":"Assignments/4XHBGM/Index/","text":"MGIN 1. Semester \u00dcbung 1 \u00dcbung 2 \u00dcbung 3 \u00dcbung 3A 2. Semester KI Integration in eine Webapplikation","title":"\u00dcbungen 4XHBGM"},{"location":"Assignments/4XHBGM/Index/#mgin","text":"1. Semester \u00dcbung 1 \u00dcbung 2 \u00dcbung 3 \u00dcbung 3A 2. Semester KI Integration in eine Webapplikation","title":"MGIN"},{"location":"Assignments/4XHBGM/MGIN/Lehrstoff/","text":"Semester 1 Titel Tasks Kenntnisse Begriffe Client Server Computing Konzepte von dynamischen Web-Applikationen lernen HTTP Protokoll lernen Was ist eine Web-Applikation? Wie unterscheidet sich eine dynamischen von einer statischen Web-Applikation? Wie ist das HTTP Protokoll aufgebaut? Client, Server, Web Server, Request/Response-Modell, Kommunikationsprotokoll, HTTP HTML HTML-Seite mit verschiedenen Tags erstellen Aufbau einer HTML-Datei Wichtige HTML-Elemente HTML, HTML-Element, tag HTML Formular Erstellen eines HTML-Formular f\u00fcr die gew\u00e4hlte Aufgabe Aufbau eines HTML-Formulars Wichtige Eingabe-Elemente kennen Was passiert beim Absenden eines Formulars? HTML Eingabe-Elemente HTTP-Methoden request-Parameter, session, cookies CSS / BootStrap / Tailwind Formattieren des HTML-Formulars Wie funktioniert die Formattierung einer HTML-Seite mit CSS Wie kann eine HTML-Seite mit BootStrap responsiv gemacht werden CSS: wichtige Formate, Selektoren, box model Bootstrap, responsives Design JavaScript Intro Erstellen eines einfachen Programms mit JavaScript, Ausgabe auf Konsole Grundlagen der Programmierung mit JavaScript Objekte, Klassen, Funktionen, Callbacks Formulardaten als JSON Formulardaten in ein JavaScript Objekt speichern und auf JSON umwandeln Wie greift JavaScript auf Formulardaten zu Wie kann man Daten in ein JavaScript Objekt speichern und daraus einen JSON-String machen JSON Tabelle aus JSON-Array Aus den Daten eines JSON-Arrays eine HTML-Tabelle bef\u00fcllen Applikation, welche die Daten im local storage speichert Wie kann man aus einem JSON-String JavaScript Objekte machen Wie kann man aus den Daten von JavaScript Objekten eine HTML-Tabelle bef\u00fcllen REST-API mit Django Erstellen einer REST-API f\u00fcr die gew\u00e4hlte Funktionalit\u00e4t mit Django Django-Applikation erstellen, REST-Controller erstellen, Endopoints \u00fcber HTTP ansprechen Django, REST, Controller, Endpoint fetch-API (GET) Daten von der REST-API mittels GET abrufen und anzeigen Wie kann man von JavaScript mittels fetch Daten von einer REST-API abrufen und verarbeiten fetch, asynchrone Ausf\u00fchrung, async, await fetch-API (POST/PUT) Daten zur REST-API mittels POST oder PUT senden Wie kann main von JavaScript mittels fetch Daten zu einer REST-API senden fetch f\u00fcr POST/PUT JPA-Repository / REST-API speichert die Daten mittels JPA-Repository in eine MySQL Datenbank Wie funktionieren JPA Repositories (JPA) repository GIT Arbeit an einem gemeinsamen Projekt, Synchronisieren mittels GIT Wie funktioniert die Zusammenarbeit beim Entwickeln mit GIT Versionskontrolle wichtige GIT Befehle Eigenes Projekt Erstellen einer eigenen Webanwendung in 2er Gruppen. Thema sollte im medizinischen Bereich sein Zusammenarbeit in einem Softwareprojekt, Versionskontrolle Semester 2 Titel Tasks Kenntnisse Begriffe Python Starter Installation: Python, PyCharm Grundelemente der Sprache Python lernen Arbeiten mit Python und PyCharm Python, PyCharm Python collections Aufgaben mit Listen, Arrays, \u2026. Kennen und Anwenden der M\u00f6glichkeiten mit Python Mengenoperationen auszuf\u00fchren list, set, dictionary, tuple, array numpy, pandas, matplotlib Aufgaben mit data frames, Arrays Daten visualisieren Kennen und Anwenden der Bibliothekten numpy, pandas, matplotlib Visualisieren von Daten Arbeiten mit data frames dataframe elements of AI Unterlagen zum Thema AI durcharbeiten Grundlegendes Verst\u00e4ndnis von k\u00fcnstlicher Intelligenz und machine learning computer science, data science, artificial intelligence, machine learning, deep learning, \u2026 classificator Klassifikator-Applikation mit Python was ist eine Klassifizierungs-Aufgabe Erstellen eines Modells und optimieren von Parametern Methoden zum Optimieren von Parametern (brute force, gradient descent) classificator machine learning algorithms Dokumentationen zu machine learning Algorithmen lesen Kennen verschiedener machine learning Algorithmen multilayer perceptron (numpy) Programmieren eines Multilayer Perceptrons mit Python / numpy Verstehen der grundlegenden Konzepte von neuronalen Netzen Architektur eines Perceptrons verstehen perceptron, input / output / hidden layer, neuron, weigths, \u2026 multilayer perceptron (keras / tensorflow) Klassifizierungs-Aufgabe mit keras / tensorflow l\u00f6sen Verstehen und Anwenden der Bibliotheken keras und tensorflow Architektur eines Perceptrons verstehen wichtige Keras Klassen CNN Bild-Klassifizierungs-Aufgabe mit einem convolutional neuonal network (CNN) l\u00f6sen Verstehen und Anwenden der Bibliotheken keras und tensorflow, um ein CNN aufzubauen Optimierung eines CNN durchf\u00fchren CNN, wichtige Klassen f\u00fcr CNN Neuronal Network application Komplette Applikation erstellen, welche Bilder klassifiziert (z.B. Web-Applikation) Integration eines trainierten Netzwerks in eine Applikation Weitere Architekturen f\u00fcr neuronale Netze Dokumentationen zu verschiedenen Netzwerk-Architekturen lesen Verstehen der wichtigsten Netzwerk-Architekturen","title":"Erstellung einer Django-Webseite"},{"location":"Assignments/4XHBGM/MGIN/Lehrstoff/#semester-1","text":"Titel Tasks Kenntnisse Begriffe Client Server Computing Konzepte von dynamischen Web-Applikationen lernen HTTP Protokoll lernen Was ist eine Web-Applikation? Wie unterscheidet sich eine dynamischen von einer statischen Web-Applikation? Wie ist das HTTP Protokoll aufgebaut? Client, Server, Web Server, Request/Response-Modell, Kommunikationsprotokoll, HTTP HTML HTML-Seite mit verschiedenen Tags erstellen Aufbau einer HTML-Datei Wichtige HTML-Elemente HTML, HTML-Element, tag HTML Formular Erstellen eines HTML-Formular f\u00fcr die gew\u00e4hlte Aufgabe Aufbau eines HTML-Formulars Wichtige Eingabe-Elemente kennen Was passiert beim Absenden eines Formulars? HTML Eingabe-Elemente HTTP-Methoden request-Parameter, session, cookies CSS / BootStrap / Tailwind Formattieren des HTML-Formulars Wie funktioniert die Formattierung einer HTML-Seite mit CSS Wie kann eine HTML-Seite mit BootStrap responsiv gemacht werden CSS: wichtige Formate, Selektoren, box model Bootstrap, responsives Design JavaScript Intro Erstellen eines einfachen Programms mit JavaScript, Ausgabe auf Konsole Grundlagen der Programmierung mit JavaScript Objekte, Klassen, Funktionen, Callbacks Formulardaten als JSON Formulardaten in ein JavaScript Objekt speichern und auf JSON umwandeln Wie greift JavaScript auf Formulardaten zu Wie kann man Daten in ein JavaScript Objekt speichern und daraus einen JSON-String machen JSON Tabelle aus JSON-Array Aus den Daten eines JSON-Arrays eine HTML-Tabelle bef\u00fcllen Applikation, welche die Daten im local storage speichert Wie kann man aus einem JSON-String JavaScript Objekte machen Wie kann man aus den Daten von JavaScript Objekten eine HTML-Tabelle bef\u00fcllen REST-API mit Django Erstellen einer REST-API f\u00fcr die gew\u00e4hlte Funktionalit\u00e4t mit Django Django-Applikation erstellen, REST-Controller erstellen, Endopoints \u00fcber HTTP ansprechen Django, REST, Controller, Endpoint fetch-API (GET) Daten von der REST-API mittels GET abrufen und anzeigen Wie kann man von JavaScript mittels fetch Daten von einer REST-API abrufen und verarbeiten fetch, asynchrone Ausf\u00fchrung, async, await fetch-API (POST/PUT) Daten zur REST-API mittels POST oder PUT senden Wie kann main von JavaScript mittels fetch Daten zu einer REST-API senden fetch f\u00fcr POST/PUT JPA-Repository / REST-API speichert die Daten mittels JPA-Repository in eine MySQL Datenbank Wie funktionieren JPA Repositories (JPA) repository GIT Arbeit an einem gemeinsamen Projekt, Synchronisieren mittels GIT Wie funktioniert die Zusammenarbeit beim Entwickeln mit GIT Versionskontrolle wichtige GIT Befehle Eigenes Projekt Erstellen einer eigenen Webanwendung in 2er Gruppen. Thema sollte im medizinischen Bereich sein Zusammenarbeit in einem Softwareprojekt, Versionskontrolle","title":"Semester 1"},{"location":"Assignments/4XHBGM/MGIN/Lehrstoff/#semester-2","text":"Titel Tasks Kenntnisse Begriffe Python Starter Installation: Python, PyCharm Grundelemente der Sprache Python lernen Arbeiten mit Python und PyCharm Python, PyCharm Python collections Aufgaben mit Listen, Arrays, \u2026. Kennen und Anwenden der M\u00f6glichkeiten mit Python Mengenoperationen auszuf\u00fchren list, set, dictionary, tuple, array numpy, pandas, matplotlib Aufgaben mit data frames, Arrays Daten visualisieren Kennen und Anwenden der Bibliothekten numpy, pandas, matplotlib Visualisieren von Daten Arbeiten mit data frames dataframe elements of AI Unterlagen zum Thema AI durcharbeiten Grundlegendes Verst\u00e4ndnis von k\u00fcnstlicher Intelligenz und machine learning computer science, data science, artificial intelligence, machine learning, deep learning, \u2026 classificator Klassifikator-Applikation mit Python was ist eine Klassifizierungs-Aufgabe Erstellen eines Modells und optimieren von Parametern Methoden zum Optimieren von Parametern (brute force, gradient descent) classificator machine learning algorithms Dokumentationen zu machine learning Algorithmen lesen Kennen verschiedener machine learning Algorithmen multilayer perceptron (numpy) Programmieren eines Multilayer Perceptrons mit Python / numpy Verstehen der grundlegenden Konzepte von neuronalen Netzen Architektur eines Perceptrons verstehen perceptron, input / output / hidden layer, neuron, weigths, \u2026 multilayer perceptron (keras / tensorflow) Klassifizierungs-Aufgabe mit keras / tensorflow l\u00f6sen Verstehen und Anwenden der Bibliotheken keras und tensorflow Architektur eines Perceptrons verstehen wichtige Keras Klassen CNN Bild-Klassifizierungs-Aufgabe mit einem convolutional neuonal network (CNN) l\u00f6sen Verstehen und Anwenden der Bibliotheken keras und tensorflow, um ein CNN aufzubauen Optimierung eines CNN durchf\u00fchren CNN, wichtige Klassen f\u00fcr CNN Neuronal Network application Komplette Applikation erstellen, welche Bilder klassifiziert (z.B. Web-Applikation) Integration eines trainierten Netzwerks in eine Applikation Weitere Architekturen f\u00fcr neuronale Netze Dokumentationen zu verschiedenen Netzwerk-Architekturen lesen Verstehen der wichtigsten Netzwerk-Architekturen","title":"Semester 2"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/","text":"Zielsetzung In dieser \u00dcbung werdet ihr die Grundlagen f\u00fcr eine Django-Webseite erstellen. Der Fokus liegt zun\u00e4chst auf dem Aufbau der Django-Struktur und der Implementierung grundlegender Funktionen wie Formulareingaben und Datei-Uploads. Zudem soll das Styling mit CSS und entsprechenden Frameworks ge\u00fcbt werden. Aufgabenstellung 1. Django-Projekt einrichten Erstelle ein neues Django-Projekt mit dem Namen \"Uebung1\". Erstelle innerhalb des Projekts eine neue App mit dem Namen \"formDataTransfer\". 2. Modelle erstellen Erstelle in der models.py Datei ein Modell namens FormSubmission mit folgenden Feldern: text_input : Ein TextField f\u00fcr Texteingaben file_upload : Ein FileField f\u00fcr Datei-Uploads submission_date : Ein DateTimeField, das automatisch den Zeitpunkt des Uploads speichert 3. Formulare implementieren Erstelle in einer neuen Datei forms.py ein Django-Formular, das die Eingabe von Text und das Hochladen von Dateien erm\u00f6glicht. 4. Views erstellen Implementiere in views.py zwei Views: submit_form : Verarbeitet das Formular und speichert die Eingaben in der Datenbank. view_submissions : Zeigt alle bisher gemachten Eingaben an. 5. Templates gestalten Erstelle zwei HTML-Templates: submit.html : Enth\u00e4lt das Formular zur Eingabe und zum Datei-Upload. submissions.html : Zeigt alle bisherigen Eingaben tabellarisch an. 6. URLs konfigurieren F\u00fcge in der urls.py Datei die notwendigen URL-Patterns hinzu, um die Views mit den entsprechenden URLs zu verkn\u00fcpfen. 7. Gestaltung und Benutzerfreundlichkeit Verwende Tailwind CSS, um die Webseite ansprechend zu gestalten. Implementiere eine einfache Navigation zwischen den Seiten. 8. Testen Teste deine Anwendung gr\u00fcndlich: K\u00f6nnen Texteingaben gemacht werden? Funktioniert der Datei-Upload? Werden die Eingaben korrekt in der Server-App gespeichert? Werden alle Eingaben auf der \u00dcbersichtsseite angezeigt? Abgabe Dokumentiere in einem Protokoll deine Vorgehensweise Insbesondere die Tests aus Punkt 8 m\u00fcssen durchgef\u00fchrt werden und mit Screenshots demonstriert werden dass alles funktioniert Verwende folgende Protokollvorlage: Protokoll Vorlage.docx Weitere Infos Django Installationsanleitung Erstes Django Projekt File Uploads Tailwind - Getting started Tailwind via CDN (ohne installation)","title":"Erstellung einer Django-Webseite"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#zielsetzung","text":"In dieser \u00dcbung werdet ihr die Grundlagen f\u00fcr eine Django-Webseite erstellen. Der Fokus liegt zun\u00e4chst auf dem Aufbau der Django-Struktur und der Implementierung grundlegender Funktionen wie Formulareingaben und Datei-Uploads. Zudem soll das Styling mit CSS und entsprechenden Frameworks ge\u00fcbt werden.","title":"Zielsetzung"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#aufgabenstellung","text":"","title":"Aufgabenstellung"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#1-django-projekt-einrichten","text":"Erstelle ein neues Django-Projekt mit dem Namen \"Uebung1\". Erstelle innerhalb des Projekts eine neue App mit dem Namen \"formDataTransfer\".","title":"1. Django-Projekt einrichten"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#2-modelle-erstellen","text":"Erstelle in der models.py Datei ein Modell namens FormSubmission mit folgenden Feldern: text_input : Ein TextField f\u00fcr Texteingaben file_upload : Ein FileField f\u00fcr Datei-Uploads submission_date : Ein DateTimeField, das automatisch den Zeitpunkt des Uploads speichert","title":"2. Modelle erstellen"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#3-formulare-implementieren","text":"Erstelle in einer neuen Datei forms.py ein Django-Formular, das die Eingabe von Text und das Hochladen von Dateien erm\u00f6glicht.","title":"3. Formulare implementieren"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#4-views-erstellen","text":"Implementiere in views.py zwei Views: submit_form : Verarbeitet das Formular und speichert die Eingaben in der Datenbank. view_submissions : Zeigt alle bisher gemachten Eingaben an.","title":"4. Views erstellen"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#5-templates-gestalten","text":"Erstelle zwei HTML-Templates: submit.html : Enth\u00e4lt das Formular zur Eingabe und zum Datei-Upload. submissions.html : Zeigt alle bisherigen Eingaben tabellarisch an.","title":"5. Templates gestalten"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#6-urls-konfigurieren","text":"F\u00fcge in der urls.py Datei die notwendigen URL-Patterns hinzu, um die Views mit den entsprechenden URLs zu verkn\u00fcpfen.","title":"6. URLs konfigurieren"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#7-gestaltung-und-benutzerfreundlichkeit","text":"Verwende Tailwind CSS, um die Webseite ansprechend zu gestalten. Implementiere eine einfache Navigation zwischen den Seiten.","title":"7. Gestaltung und Benutzerfreundlichkeit"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#8-testen","text":"Teste deine Anwendung gr\u00fcndlich: K\u00f6nnen Texteingaben gemacht werden? Funktioniert der Datei-Upload? Werden die Eingaben korrekt in der Server-App gespeichert? Werden alle Eingaben auf der \u00dcbersichtsseite angezeigt?","title":"8. Testen"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#abgabe","text":"Dokumentiere in einem Protokoll deine Vorgehensweise Insbesondere die Tests aus Punkt 8 m\u00fcssen durchgef\u00fchrt werden und mit Screenshots demonstriert werden dass alles funktioniert Verwende folgende Protokollvorlage: Protokoll Vorlage.docx","title":"Abgabe"},{"location":"Assignments/4XHBGM/MGIN/Uebung1/#weitere-infos","text":"Django Installationsanleitung Erstes Django Projekt File Uploads Tailwind - Getting started Tailwind via CDN (ohne installation)","title":"Weitere Infos"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/","text":"Aufgabenstellung 1. Projekt-Setup Erstelle ein neues Django-Projekt mit dem Namen \"WienerAdressen\". Richte eine neue App namens \"adressformular\" innerhalb des Projekts ein. Initialisiere ein Git-Repository f\u00fcr dein Projekt. Mach dann auch regelm\u00e4\u00dfig Commits um deinen Fortschritt zu beschreiben und um im Fehlerfall zu einem fr\u00fcheren Zeitpunkt gehen zu k\u00f6nnen 2. Backend-Entwicklung 2.1 Modell erstellen Erstelle in models.py ein Modell Adresse mit folgenden Feldern: strasse hausnummer plz ort bundesland timestamp 2.2 View implementieren Erstelle in views.py zwei Views: adressformular : Rendert das Formular zur Adresseingabe. adresse_speichern : API-Endpunkt zum Speichern der eingegebenen Adresse. adressen_anzeigen :\u00dcbersichtsseite, die alle gespeicherten Adressen anzeigt. 2.3 URL-Konfiguration Konfiguriere die URLs in urls.py , um die Views/Templates mit den entsprechenden Routen zu verbinden. 3. Frontend-Entwicklung 3.1 HTML-Template Erstelle ein HTML-Template adressformular.html mit einem Formular zur Adresseingabe. Implementiere Eingabefelder f\u00fcr Stra\u00dfe, Hausnummer, PLZ und Ort. Erstelle ein HTML-Template adressuebersicht.html , um die gespeicherten Adressen anzuzeigen 3.2 JavaScript-Implementierung Erstelle eine separate JavaScript-Datei adressService.js . Implementiere folgende (oder \u00e4hnliche) Funktionen: fetchAdressData(query) : Nutzt die Fetch API, um Daten vom Wiener Adressservice abzurufen. displaySuggestions(data) : Zeigt die Vorschl\u00e4ge unterhalb der Eingabefelder an. handleInput() : Reagiert auf Benutzereingaben und ruft fetchAdressData auf. selectSuggestion(suggestion) : F\u00fcllt das Formular mit der ausgew\u00e4hlten Adresse aus. 3.3 CSS-Styling Erstelle eine CSS-Datei styles.css zur Gestaltung des Formulars und der Vorschl\u00e4ge. Verwende Flexbox oder CSS Grid f\u00fcr das Layout. Damit ist es einfach m\u00f6glich auch responsive Anwendungen zu erstellen. 4. Integration und Testing 4.1 fetch-Implementierung Verwende die fetch API , um eine HTTP-Anfrage an den adresse_speichern -Endpunkt zu senden, wenn das Formular abgeschickt wird. Zeige eine Erfolgsmeldung an, wenn die Adresse erfolgreich gespeichert wurde. 4.2 Error Handling Implementiere eine Fehlerbehandlung sowohl im Frontend als auch im Backend. Zeige benutzerfreundliche Fehlermeldungen an, wenn etwas schiefgeht. 4.3 Testf\u00e4lle Schreibe Testf\u00e4lle in tests.py um sowohl die eigenen als auch die externen API-Endpunkte zu testen 5. Dokumentation und Versionskontrolle Dokumentiere deinen Code mit Kommentaren. F\u00fchre regelm\u00e4\u00dfige Git-Commits durch, die den Entwicklungsprozess nachvollziehbar machen. Erstelle ein README.md mit einer Kurzbeschreibung des Projekts und Anweisungen zur Installation und Ausf\u00fchrung. Abgabe Erstelle ein ausf\u00fchrliches Protokoll deiner Vorgehensweise. F\u00fcge Screenshots von wichtigen Entwicklungsschritten und der finalen Anwendung hinzu. Exportiere deine Git-Commit-Historie mit git log --pretty=format:\"%h - %an, %ar : %s\" > commit_history.txt und f\u00fcge sie dem Protokoll bei. Hilfreiche Ressourcen Django-Dokumentation Fetch API - MDN Web Docs Wiener Adressservice Dokumentation JavaScript-Promises - MDN Web Docs CSS Flexbox Guide Statische Dateien in Django Wichtig um eine js-Datei einzubinden im Template Viel Erfolg bei der Umsetzung!","title":"Interaktives Adressformular mit Django"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#aufgabenstellung","text":"","title":"Aufgabenstellung"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#1-projekt-setup","text":"Erstelle ein neues Django-Projekt mit dem Namen \"WienerAdressen\". Richte eine neue App namens \"adressformular\" innerhalb des Projekts ein. Initialisiere ein Git-Repository f\u00fcr dein Projekt. Mach dann auch regelm\u00e4\u00dfig Commits um deinen Fortschritt zu beschreiben und um im Fehlerfall zu einem fr\u00fcheren Zeitpunkt gehen zu k\u00f6nnen","title":"1. Projekt-Setup"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#2-backend-entwicklung","text":"","title":"2. Backend-Entwicklung"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#21-modell-erstellen","text":"Erstelle in models.py ein Modell Adresse mit folgenden Feldern: strasse hausnummer plz ort bundesland timestamp","title":"2.1 Modell erstellen"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#22-view-implementieren","text":"Erstelle in views.py zwei Views: adressformular : Rendert das Formular zur Adresseingabe. adresse_speichern : API-Endpunkt zum Speichern der eingegebenen Adresse. adressen_anzeigen :\u00dcbersichtsseite, die alle gespeicherten Adressen anzeigt.","title":"2.2 View implementieren"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#23-url-konfiguration","text":"Konfiguriere die URLs in urls.py , um die Views/Templates mit den entsprechenden Routen zu verbinden.","title":"2.3 URL-Konfiguration"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#3-frontend-entwicklung","text":"","title":"3. Frontend-Entwicklung"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#31-html-template","text":"Erstelle ein HTML-Template adressformular.html mit einem Formular zur Adresseingabe. Implementiere Eingabefelder f\u00fcr Stra\u00dfe, Hausnummer, PLZ und Ort. Erstelle ein HTML-Template adressuebersicht.html , um die gespeicherten Adressen anzuzeigen","title":"3.1 HTML-Template"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#32-javascript-implementierung","text":"Erstelle eine separate JavaScript-Datei adressService.js . Implementiere folgende (oder \u00e4hnliche) Funktionen: fetchAdressData(query) : Nutzt die Fetch API, um Daten vom Wiener Adressservice abzurufen. displaySuggestions(data) : Zeigt die Vorschl\u00e4ge unterhalb der Eingabefelder an. handleInput() : Reagiert auf Benutzereingaben und ruft fetchAdressData auf. selectSuggestion(suggestion) : F\u00fcllt das Formular mit der ausgew\u00e4hlten Adresse aus.","title":"3.2 JavaScript-Implementierung"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#33-css-styling","text":"Erstelle eine CSS-Datei styles.css zur Gestaltung des Formulars und der Vorschl\u00e4ge. Verwende Flexbox oder CSS Grid f\u00fcr das Layout. Damit ist es einfach m\u00f6glich auch responsive Anwendungen zu erstellen.","title":"3.3 CSS-Styling"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#4-integration-und-testing","text":"","title":"4. Integration und Testing"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#41-fetch-implementierung","text":"Verwende die fetch API , um eine HTTP-Anfrage an den adresse_speichern -Endpunkt zu senden, wenn das Formular abgeschickt wird. Zeige eine Erfolgsmeldung an, wenn die Adresse erfolgreich gespeichert wurde.","title":"4.1 fetch-Implementierung"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#42-error-handling","text":"Implementiere eine Fehlerbehandlung sowohl im Frontend als auch im Backend. Zeige benutzerfreundliche Fehlermeldungen an, wenn etwas schiefgeht.","title":"4.2 Error Handling"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#43-testfalle","text":"Schreibe Testf\u00e4lle in tests.py um sowohl die eigenen als auch die externen API-Endpunkte zu testen","title":"4.3 Testf\u00e4lle"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#5-dokumentation-und-versionskontrolle","text":"Dokumentiere deinen Code mit Kommentaren. F\u00fchre regelm\u00e4\u00dfige Git-Commits durch, die den Entwicklungsprozess nachvollziehbar machen. Erstelle ein README.md mit einer Kurzbeschreibung des Projekts und Anweisungen zur Installation und Ausf\u00fchrung.","title":"5. Dokumentation und Versionskontrolle"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#abgabe","text":"Erstelle ein ausf\u00fchrliches Protokoll deiner Vorgehensweise. F\u00fcge Screenshots von wichtigen Entwicklungsschritten und der finalen Anwendung hinzu. Exportiere deine Git-Commit-Historie mit git log --pretty=format:\"%h - %an, %ar : %s\" > commit_history.txt und f\u00fcge sie dem Protokoll bei.","title":"Abgabe"},{"location":"Assignments/4XHBGM/MGIN/Uebung2/#hilfreiche-ressourcen","text":"Django-Dokumentation Fetch API - MDN Web Docs Wiener Adressservice Dokumentation JavaScript-Promises - MDN Web Docs CSS Flexbox Guide Statische Dateien in Django Wichtig um eine js-Datei einzubinden im Template Viel Erfolg bei der Umsetzung!","title":"Hilfreiche Ressourcen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/","text":"Zielsetzung In dieser \u00dcbung werdet ihr die Grundlagen von Web Components kennenlernen und anwenden. Das Adressformular aus der vorherigen \u00dcbung ist als Custom Element zu implementiern. Dieses soll dann in den Code der \u00dcbung 2 integriert werden. Das soll euch einen Einblick in moderne Webtechnologien und komponentenbasierte Entwicklung geben. Komponenten werden bei Frameworks wie React, Vue oder Angular h\u00e4ufig in \u00e4hnlicher Form eingesetzt. Hintergrundinformation Web Components sind ein Standard, der es erm\u00f6glicht, wiederverwendbare und gekapselte Benutzeroberfl\u00e4chen-Komponenten zu erstellen. Wichtig hierf\u00fcr sind: 1. Web Component Specification : Erlaubt die Definition eigener HTML-Elemente 2. Shadow DOM : Bietet eine gekapselte DOM- und Styling-Struktur Das Verst\u00e4ndnis und die Anwendung von Web Components sind aus folgenden Gr\u00fcnden wichtig: - Sie f\u00f6rdern modulares und wiederverwendbares Design - Sie erm\u00f6glichen eine bessere Trennung von Markup, Stil und Logik - Sie verbessern die Wartbarkeit und Skalierbarkeit von Webanwendungen - Werden (wie bereits erw\u00e4hnt) in modernen Webframeworks h\u00e4ufig eingesetzt Aufgabenstellung 1. Projekt-Setup Nimm die \u00dcbung 2 als Grundlage. Bereits erstellte Dateien k\u00f6nnen gerne weiterverwendet werden. Erstelle eine HTML-Datei namens index.html Erstelle eine JavaScript-Datei namens address-form.js Verlinke die JavaScript-Datei in der HTML-Datei 2. Custom Element definieren Definiere in address-form.js eine neue Klasse AddressForm , die von HTMLElement erbt Registriere das neue Custom Element mit dem Namen address-form Beispielcode f\u00fcr eine \u00e4hnliche Komponente hier (editable list MDN) oder (edit-word) 3. Template erstellen F\u00fcge in die HTML-Datei ein <template> -Element hinzu Gestalte innerhalb des Templates ein Adressformular mit folgenden Feldern: Vorname Nachname Stra\u00dfe Hausnummer Postleitzahl Stadt Land F\u00fcgen dem Template auch einen \"Absenden\"-Button hinzu In dem Template soll es einen Bearbeiten-Modus geben sowie einen Anzeige-Modus Beim erstellen eines neuen Eintrags soll der Bearbeiten-Modus verwendet werden In der \u00dcbersicht der gespeicherten Daten der Anzeige-Modus 4. Eventhandling implementieren F\u00fcge einen Event Listener f\u00fcr das \"submit\"-Event des Formulars hinzu Um sofortiges Absenden zu unterdr\u00fccken, verwende preventDefault Implementiere eine Methode, die die eingegebenen Daten sammelt und ausgibt in der Konsole 5. Styling F\u00fcge dem Shadow DOM ein <style> -Element hinzu Definiere CSS-Regeln, um das Formular ansprechend zu gestalten (tobt euch aus!) 6. Verwendung und Test F\u00fcgen in der index.html das neue Custom Element <address-form> ein Testen die Funktionalit\u00e4t des Formulars im Browser Protokolliere die Konsolenausgabe wenn man auf Senden klickt 7. Integration Implementieren eine Methode, um die Formulardaten als JSON Objekt zur\u00fcckzugeben. Das JSON soll dann an die entsprechenden Django-Endpunkte aus \u00dcbung 2 \u00fcbergeben werden zum speichern. Implementierte serverseitig auch eine Update-Funktion um bestehende Adressen \u00e4ndern zu k\u00f6nnen. (Optional) F\u00fcge eine Validierung f\u00fcr die Eingabefelder hinzu (z.B. Pflichtfelder, Format\u00fcberpr\u00fcfung) Abgabe F\u00fcge im Protokoll folgende Dateien ein: index.html address-form.js F\u00fcge Screenshots hinzu, die das funktionierende Formular und die Konsolenausgabe bei der Formular\u00fcbermittlung zeigen Der Code muss mit Syntax-Highlighting formatiert sein Das Word-Template muss nicht als Vorlage genommen werden. Ein mit Markdown geschriebenes PDF nehme ich gerne entgegen. Gibt f\u00fcr VS Code eine Extension Markdown-PDF oder man konvertiert es mit Pandoc Erweiterungen von \u00dcbung 2 /add in den Views (+urls) hinzuf\u00fcgen Sende vom Server zum Client einen Primary Key mit, um zu erkennen ob die \u00fcbermittelten Daten bestehende \u00fcberschreiben sollen. Ist der primary key -1 oder null soll ein neuer Eintrag in der Datenbank gespeichert werden. \u00dcber <input type=\"hidden\" value=\"<key>\" ... /> kann man den Primary Key vor dem User verstecken und trotzdem mit \u00fcbertragen. Erg\u00e4nze im model.py den Vornamen und den Nachnamen. Bundesland kannst du auf Land \u00e4ndern Verwende den neuen Tag <address-form editable=\"false\" ... /> in der \u00dcbersicht sowie beim anlegen neuer Adressen Weitere Ressourcen MDN Web Components Guide Google Developers - Web Components CSS-Tricks - An Introduction to Web Components","title":"Erstellung eines Adressformulars als Web Component"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#zielsetzung","text":"In dieser \u00dcbung werdet ihr die Grundlagen von Web Components kennenlernen und anwenden. Das Adressformular aus der vorherigen \u00dcbung ist als Custom Element zu implementiern. Dieses soll dann in den Code der \u00dcbung 2 integriert werden. Das soll euch einen Einblick in moderne Webtechnologien und komponentenbasierte Entwicklung geben. Komponenten werden bei Frameworks wie React, Vue oder Angular h\u00e4ufig in \u00e4hnlicher Form eingesetzt.","title":"Zielsetzung"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#hintergrundinformation","text":"Web Components sind ein Standard, der es erm\u00f6glicht, wiederverwendbare und gekapselte Benutzeroberfl\u00e4chen-Komponenten zu erstellen. Wichtig hierf\u00fcr sind: 1. Web Component Specification : Erlaubt die Definition eigener HTML-Elemente 2. Shadow DOM : Bietet eine gekapselte DOM- und Styling-Struktur Das Verst\u00e4ndnis und die Anwendung von Web Components sind aus folgenden Gr\u00fcnden wichtig: - Sie f\u00f6rdern modulares und wiederverwendbares Design - Sie erm\u00f6glichen eine bessere Trennung von Markup, Stil und Logik - Sie verbessern die Wartbarkeit und Skalierbarkeit von Webanwendungen - Werden (wie bereits erw\u00e4hnt) in modernen Webframeworks h\u00e4ufig eingesetzt","title":"Hintergrundinformation"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#aufgabenstellung","text":"","title":"Aufgabenstellung"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#1-projekt-setup","text":"Nimm die \u00dcbung 2 als Grundlage. Bereits erstellte Dateien k\u00f6nnen gerne weiterverwendet werden. Erstelle eine HTML-Datei namens index.html Erstelle eine JavaScript-Datei namens address-form.js Verlinke die JavaScript-Datei in der HTML-Datei","title":"1. Projekt-Setup"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#2-custom-element-definieren","text":"Definiere in address-form.js eine neue Klasse AddressForm , die von HTMLElement erbt Registriere das neue Custom Element mit dem Namen address-form Beispielcode f\u00fcr eine \u00e4hnliche Komponente hier (editable list MDN) oder (edit-word)","title":"2. Custom Element definieren"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#3-template-erstellen","text":"F\u00fcge in die HTML-Datei ein <template> -Element hinzu Gestalte innerhalb des Templates ein Adressformular mit folgenden Feldern: Vorname Nachname Stra\u00dfe Hausnummer Postleitzahl Stadt Land F\u00fcgen dem Template auch einen \"Absenden\"-Button hinzu In dem Template soll es einen Bearbeiten-Modus geben sowie einen Anzeige-Modus Beim erstellen eines neuen Eintrags soll der Bearbeiten-Modus verwendet werden In der \u00dcbersicht der gespeicherten Daten der Anzeige-Modus","title":"3. Template erstellen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#4-eventhandling-implementieren","text":"F\u00fcge einen Event Listener f\u00fcr das \"submit\"-Event des Formulars hinzu Um sofortiges Absenden zu unterdr\u00fccken, verwende preventDefault Implementiere eine Methode, die die eingegebenen Daten sammelt und ausgibt in der Konsole","title":"4. Eventhandling implementieren"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#5-styling","text":"F\u00fcge dem Shadow DOM ein <style> -Element hinzu Definiere CSS-Regeln, um das Formular ansprechend zu gestalten (tobt euch aus!)","title":"5. Styling"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#6-verwendung-und-test","text":"F\u00fcgen in der index.html das neue Custom Element <address-form> ein Testen die Funktionalit\u00e4t des Formulars im Browser Protokolliere die Konsolenausgabe wenn man auf Senden klickt","title":"6. Verwendung und Test"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#7-integration","text":"Implementieren eine Methode, um die Formulardaten als JSON Objekt zur\u00fcckzugeben. Das JSON soll dann an die entsprechenden Django-Endpunkte aus \u00dcbung 2 \u00fcbergeben werden zum speichern. Implementierte serverseitig auch eine Update-Funktion um bestehende Adressen \u00e4ndern zu k\u00f6nnen. (Optional) F\u00fcge eine Validierung f\u00fcr die Eingabefelder hinzu (z.B. Pflichtfelder, Format\u00fcberpr\u00fcfung)","title":"7. Integration"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#abgabe","text":"F\u00fcge im Protokoll folgende Dateien ein: index.html address-form.js F\u00fcge Screenshots hinzu, die das funktionierende Formular und die Konsolenausgabe bei der Formular\u00fcbermittlung zeigen Der Code muss mit Syntax-Highlighting formatiert sein Das Word-Template muss nicht als Vorlage genommen werden. Ein mit Markdown geschriebenes PDF nehme ich gerne entgegen. Gibt f\u00fcr VS Code eine Extension Markdown-PDF oder man konvertiert es mit Pandoc","title":"Abgabe"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#erweiterungen-von-ubung-2","text":"/add in den Views (+urls) hinzuf\u00fcgen Sende vom Server zum Client einen Primary Key mit, um zu erkennen ob die \u00fcbermittelten Daten bestehende \u00fcberschreiben sollen. Ist der primary key -1 oder null soll ein neuer Eintrag in der Datenbank gespeichert werden. \u00dcber <input type=\"hidden\" value=\"<key>\" ... /> kann man den Primary Key vor dem User verstecken und trotzdem mit \u00fcbertragen. Erg\u00e4nze im model.py den Vornamen und den Nachnamen. Bundesland kannst du auf Land \u00e4ndern Verwende den neuen Tag <address-form editable=\"false\" ... /> in der \u00dcbersicht sowie beim anlegen neuer Adressen","title":"Erweiterungen von \u00dcbung 2"},{"location":"Assignments/4XHBGM/MGIN/Uebung3/#weitere-ressourcen","text":"MDN Web Components Guide Google Developers - Web Components CSS-Tricks - An Introduction to Web Components","title":"Weitere Ressourcen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/","text":"Einf\u00fchrung In den bisherigen Projekten habt ihr SQLite als Datenbank f\u00fcr eure Django-Anwendungen verwendet. In der professionellen Entwicklung werden jedoch h\u00e4ufig leistungsf\u00e4higere Datenbanksysteme wie MySQL, Postgres, Vitess oder CockroachDB eingesetzt. In diesem Arbeitsauftrag werdet ihr: 1. Einen MySQL-Datenbankserver in einem Docker-Container aufsetzen 2. Euer bestehendes Django-Projekt von SQLite auf MySQL umstellen Hintergrundinformationen - Was ist Docker? Docker ist eine Plattform, die es erm\u00f6glicht, Anwendungen in isolierten Umgebungen (genannt \"Container\") laufen zu lassen. Ein Container ist wie ein eigenst\u00e4ndiger Mini-Computer, der nur das enth\u00e4lt, was die Anwendung braucht. Der Vorteil: Egal auf welchem Computer ihr den Container startet, die Anwendung l\u00e4uft immer gleich. Man Programmiert als nicht das Verhalten eines Programmes sondern dessen Umgebung. Auch genannt IaC Infrastrucutre as Code . Voraussetzungen Euer funktionierendes Django-Projekt mit SQLite Docker Desktop (Installation unter https://www.docker.com/products/docker-desktop ) Aufgaben Teil 1: Docker und MySQL einrichten Docker Installation \u00fcberpr\u00fcfen Starte Docker Desktop und gib in der Konsole folgenden Befehl ein: bash docker --version Dies sollte eine Versionsnummer ausgeben. Docker Compose File erstellen Erstellt im Hauptverzeichnis eures Projekts eine neue Datei namens docker-compose.yml F\u00fcgt folgenden Inhalt ein: ```yaml services: db: image: mysql:latest environment: MYSQL_ROOT_PASSWORD: meinpasswort MYSQL_DATABASE: meinprojekt MYSQL_USER: djangouser MYSQL_PASSWORD: djangopasswort ports: - \"3306:3306\" volumes: - mysql_data:/var/lib/mysql volumes: mysql_data: ``` Erkl\u00e4rung der wichtigsten Elemente: - image: mysql:latest : Verwendet MySQL, die neueste Version - environment : Setzt wichtige Umgebungsvariablen wie Passw\u00f6rter und Datenbankname - ports : Macht MySQL auf Port 3306 erreichbar - volumes : Speichert die Datenbank-Dateien dauerhaft All diese Erkl\u00f6reungen findet ihr f\u00fcr Verschiedenste Container auch auf Docker Hub Teil 2: Django-Projekt anpassen MySQL-Treiber installieren bash pip install mysqlclient settings.py anpassen Sucht in eurer settings.py den DATABASES -Bereich und ersetzt ihn durch: python DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'meinprojekt', 'USER': 'djangouser', 'PASSWORD': 'djangopasswort', 'HOST': '127.0.0.1', 'PORT': '3306', } } wichtig ist dass hier die selben Usernamen, Passw\u00f6rter und andere Einstellungen verwendet werden wie in der Compose-Datei wenn der Container erstellt wird. Quelle dazu aus den Django Docs Teil 3: Datenbank starten und Projekt testen Docker Container starten bash docker-compose up -d Das -d bedeutet \"detached mode\" - der Container l\u00e4uft im Hintergrund Wartet etwa 30 Sekunden, bis MySQL vollst\u00e4ndig gestartet ist. Du kannst auch dem Container zusehen was er alles beim Start macht. Mit docker compose logs -f werden alle logs so lange ausgegeben bis du den Befehl mit Strg + C abbrichst. Migrations ausf\u00fchren bash python manage.py migrate Superuser erstellen bash python manage.py createsuperuser Server starten und testen bash python manage.py runserver Zusatzaufgaben Datenbank-Logs anzeigen bash docker-compose logs db Analysiert die Logs - was seht ihr? Container Status \u00fcberpr\u00fcfen bash docker-compose ps Was bedeuten die verschiedenen Spalten? Hilfreiche Docker-Befehle Container stoppen: docker-compose down Container neustarten: docker-compose restart Alle Container und deren Status anzeigen: docker ps -a In den MySQL-Container einsteigen: bash docker compose exec db bash Abgabe Dokumentiert in einem Protokoll eure Schritte und eventuelle Probleme Erstellt Screenshots von: Erfolgreich laufendem Docker Container Django Admin-Interface mit neuer MySQL-Datenbank Einem erfolgreichen Datenbankzugriff \u00fcber eure Anwendung Bewertungskriterien Funktionalit\u00e4t (40%) MySQL-Container l\u00e4uft fehlerfrei Django-Anwendung verbindet sich erfolgreich Daten werden korrekt gespeichert Dokumentation (30%) Nachvollziehbare Beschreibung der Umsetzung Erkl\u00e4rung aufgetretener Probleme und deren L\u00f6sung Zusatzaufgaben (30%) Verst\u00e4ndnis der Docker-Logs Eigenst\u00e4ndige Erkundung und Erkl\u00e4rung weiterer Docker-Befehle Hilfestellung Bei Problemen erst die Docker-Logs \u00fcberpr\u00fcfen Sicherstellen, dass der MySQL-Container vollst\u00e4ndig gestartet ist \u00dcberpr\u00fcfen, ob die Datenbank-Zugangsdaten in Django und Docker-Compose \u00fcbereinstimmen","title":"Django-Projekt auf MySQL mit Docker umstellen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#einfuhrung","text":"In den bisherigen Projekten habt ihr SQLite als Datenbank f\u00fcr eure Django-Anwendungen verwendet. In der professionellen Entwicklung werden jedoch h\u00e4ufig leistungsf\u00e4higere Datenbanksysteme wie MySQL, Postgres, Vitess oder CockroachDB eingesetzt. In diesem Arbeitsauftrag werdet ihr: 1. Einen MySQL-Datenbankserver in einem Docker-Container aufsetzen 2. Euer bestehendes Django-Projekt von SQLite auf MySQL umstellen","title":"Einf\u00fchrung"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#hintergrundinformationen-was-ist-docker","text":"Docker ist eine Plattform, die es erm\u00f6glicht, Anwendungen in isolierten Umgebungen (genannt \"Container\") laufen zu lassen. Ein Container ist wie ein eigenst\u00e4ndiger Mini-Computer, der nur das enth\u00e4lt, was die Anwendung braucht. Der Vorteil: Egal auf welchem Computer ihr den Container startet, die Anwendung l\u00e4uft immer gleich. Man Programmiert als nicht das Verhalten eines Programmes sondern dessen Umgebung. Auch genannt IaC Infrastrucutre as Code .","title":"Hintergrundinformationen - Was ist Docker?"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#voraussetzungen","text":"Euer funktionierendes Django-Projekt mit SQLite Docker Desktop (Installation unter https://www.docker.com/products/docker-desktop )","title":"Voraussetzungen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#aufgaben","text":"","title":"Aufgaben"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#teil-1-docker-und-mysql-einrichten","text":"Docker Installation \u00fcberpr\u00fcfen Starte Docker Desktop und gib in der Konsole folgenden Befehl ein: bash docker --version Dies sollte eine Versionsnummer ausgeben. Docker Compose File erstellen Erstellt im Hauptverzeichnis eures Projekts eine neue Datei namens docker-compose.yml F\u00fcgt folgenden Inhalt ein: ```yaml services: db: image: mysql:latest environment: MYSQL_ROOT_PASSWORD: meinpasswort MYSQL_DATABASE: meinprojekt MYSQL_USER: djangouser MYSQL_PASSWORD: djangopasswort ports: - \"3306:3306\" volumes: - mysql_data:/var/lib/mysql volumes: mysql_data: ``` Erkl\u00e4rung der wichtigsten Elemente: - image: mysql:latest : Verwendet MySQL, die neueste Version - environment : Setzt wichtige Umgebungsvariablen wie Passw\u00f6rter und Datenbankname - ports : Macht MySQL auf Port 3306 erreichbar - volumes : Speichert die Datenbank-Dateien dauerhaft All diese Erkl\u00f6reungen findet ihr f\u00fcr Verschiedenste Container auch auf Docker Hub","title":"Teil 1: Docker und MySQL einrichten"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#teil-2-django-projekt-anpassen","text":"MySQL-Treiber installieren bash pip install mysqlclient settings.py anpassen Sucht in eurer settings.py den DATABASES -Bereich und ersetzt ihn durch: python DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'meinprojekt', 'USER': 'djangouser', 'PASSWORD': 'djangopasswort', 'HOST': '127.0.0.1', 'PORT': '3306', } } wichtig ist dass hier die selben Usernamen, Passw\u00f6rter und andere Einstellungen verwendet werden wie in der Compose-Datei wenn der Container erstellt wird. Quelle dazu aus den Django Docs","title":"Teil 2: Django-Projekt anpassen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#teil-3-datenbank-starten-und-projekt-testen","text":"Docker Container starten bash docker-compose up -d Das -d bedeutet \"detached mode\" - der Container l\u00e4uft im Hintergrund Wartet etwa 30 Sekunden, bis MySQL vollst\u00e4ndig gestartet ist. Du kannst auch dem Container zusehen was er alles beim Start macht. Mit docker compose logs -f werden alle logs so lange ausgegeben bis du den Befehl mit Strg + C abbrichst. Migrations ausf\u00fchren bash python manage.py migrate Superuser erstellen bash python manage.py createsuperuser Server starten und testen bash python manage.py runserver","title":"Teil 3: Datenbank starten und Projekt testen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#zusatzaufgaben","text":"Datenbank-Logs anzeigen bash docker-compose logs db Analysiert die Logs - was seht ihr? Container Status \u00fcberpr\u00fcfen bash docker-compose ps Was bedeuten die verschiedenen Spalten?","title":"Zusatzaufgaben"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#hilfreiche-docker-befehle","text":"Container stoppen: docker-compose down Container neustarten: docker-compose restart Alle Container und deren Status anzeigen: docker ps -a In den MySQL-Container einsteigen: bash docker compose exec db bash","title":"Hilfreiche Docker-Befehle"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#abgabe","text":"Dokumentiert in einem Protokoll eure Schritte und eventuelle Probleme Erstellt Screenshots von: Erfolgreich laufendem Docker Container Django Admin-Interface mit neuer MySQL-Datenbank Einem erfolgreichen Datenbankzugriff \u00fcber eure Anwendung","title":"Abgabe"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#bewertungskriterien","text":"Funktionalit\u00e4t (40%) MySQL-Container l\u00e4uft fehlerfrei Django-Anwendung verbindet sich erfolgreich Daten werden korrekt gespeichert Dokumentation (30%) Nachvollziehbare Beschreibung der Umsetzung Erkl\u00e4rung aufgetretener Probleme und deren L\u00f6sung Zusatzaufgaben (30%) Verst\u00e4ndnis der Docker-Logs Eigenst\u00e4ndige Erkundung und Erkl\u00e4rung weiterer Docker-Befehle","title":"Bewertungskriterien"},{"location":"Assignments/4XHBGM/MGIN/Uebung3A/#hilfestellung","text":"Bei Problemen erst die Docker-Logs \u00fcberpr\u00fcfen Sicherstellen, dass der MySQL-Container vollst\u00e4ndig gestartet ist \u00dcberpr\u00fcfen, ob die Datenbank-Zugangsdaten in Django und Docker-Compose \u00fcbereinstimmen","title":"Hilfestellung"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/","text":"Einf\u00fchrung In unseren bisherigen Projekten habt ihr Django-Anwendungen direkt auf eurem Computer entwickelt. In der echten Welt m\u00fcssen medizinische Anwendungen aber auf vielen verschiedenen Systemen laufen - vom Entwicklungs-Laptop bis zum Krankenhaus-Server. Hier kommt Docker ins Spiel! Mit Docker k\u00f6nnen wir sicherstellen, dass unsere Anwendung \u00fcberall gleich l\u00e4uft, egal wo sie gestartet wird. Das ist besonders wichtig bei medizinischen Anwendungen, wo Zuverl\u00e4ssigkeit an erster Stelle steht. Was ist Docker? Stellt euch Docker wie eine transportable Mini-Umgebung vor. \u00c4hnlich wie ihr vielleicht schon virtuelle Umgebungen (venv) in Python kennt, geht Docker noch einen Schritt weiter: Es verpackt nicht nur die Python-Pakete, sondern die gesamte Ausf\u00fchrungsumgebung in einen \"Container\". Das ist wie ein eigenst\u00e4ndiger Mini-Computer, der alles enth\u00e4lt, was eure Anwendung braucht. Voraussetzungen Euer funktionierendes Django-Projekt Docker Desktop (Installation: https://www.docker.com/products/docker-desktop ) VS Code oder PyCharm Aufgaben Teil 1: Docker vorbereiten und testen Kann \u00fcbersprungen werden falls es in der vorherigen \u00dcbung bereits funktioniert hat Installiert Docker Desktop Startet Docker Desktop \u00d6ffnet ein Terminal und pr\u00fcft mit docker --version , ob alles funktioniert Teil 2: Django-Projekt containerisieren Dockerfile erstellen Erstellt im Hauptverzeichnis eures Projekts eine neue Datei namens Dockerfile F\u00fcgt folgenden Inhalt ein: ```dockerfile FROM python:3.11 WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . CMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"] ``` Docker Compose erstellen Erstellt eine Datei docker-compose.yml F\u00fcgt folgenden Inhalt ein: ```yaml version: '3.8' services: web: build: . ports: - \"8000:8000\" volumes: - .:/app environment: - DEBUG=1 - DJANGO_SETTINGS_MODULE=myproject.settings ``` Teil 3: Container starten und testen Container bauen und starten bash docker compose up --build Wartet, bis ihr die Meldung seht, dass der Django-Server l\u00e4uft \u00d6ffnet http://localhost:8000 im Browser Fehlersuche Falls etwas nicht funktioniert: Schaut euch die Konsolen-Ausgabe an Pr\u00fcft mit docker compose logs web , was im Container passiert Vergleicht eure Dateien mit den Vorlagen Es fehlt noch die Integration um MySQL-Server aus der vorherigen Aufgabe, recherchiere selbst wie du diese in Compose integrieren kannst und was man in Django um\u00e4ndern muss Zusatzaufgaben f\u00fcr Schnelle Container erkunden bash docker compose exec web bash Schaut euch um - was findet ihr in /app ? Logs verstehen Startet den Container mit docker compose up Macht ein paar Anfragen an eure Webseite Was seht ihr in den Logs? Abgabe Erstellt ein kurzes Protokoll mit: 1. Screenshots von: - Eurem laufenden Container (Docker Desktop) - Eurer Django-Anwendung im Browser 2. Kurze Erkl\u00e4rung (3-4 S\u00e4tze), was ihr gemacht habt 3. Falls ihr Probleme hattet: Wie habt ihr sie gel\u00f6st? Bewertung Funktionalit\u00e4t (40%) Container l\u00e4uft Django-Anwendung ist erreichbar Code ist sauber und folgt den Vorlagen Dokumentation (30%) Protokoll ist vollst\u00e4ndig Probleme und L\u00f6sungen sind dokumentiert Zusatzaufgaben (30%) Erfolgreiche Durchf\u00fchrung Verst\u00e4ndnis der Docker-Umgebung Hilfe Schaut euch die Log-Ausgaben genau an - oft steht dort schon die L\u00f6sung \u00dcberpr\u00fcft, ob Docker Desktop l\u00e4uft Fragt eure Mitsch\u00fcler oder mich, wenn ihr nicht weiterkommt Hilfreiche Docker-Befehle Container stoppen: docker-compose down Logs anzeigen: docker compose logs -f Container neustarten: docker-compose restart Alle Container und deren Status anzeigen: docker ps -a In den Container einsteigen: bash docker compose exec <containername> bash","title":"Docker & Django - Deine erste containerisierte Webanwendung"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#einfuhrung","text":"In unseren bisherigen Projekten habt ihr Django-Anwendungen direkt auf eurem Computer entwickelt. In der echten Welt m\u00fcssen medizinische Anwendungen aber auf vielen verschiedenen Systemen laufen - vom Entwicklungs-Laptop bis zum Krankenhaus-Server. Hier kommt Docker ins Spiel! Mit Docker k\u00f6nnen wir sicherstellen, dass unsere Anwendung \u00fcberall gleich l\u00e4uft, egal wo sie gestartet wird. Das ist besonders wichtig bei medizinischen Anwendungen, wo Zuverl\u00e4ssigkeit an erster Stelle steht.","title":"Einf\u00fchrung"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#was-ist-docker","text":"Stellt euch Docker wie eine transportable Mini-Umgebung vor. \u00c4hnlich wie ihr vielleicht schon virtuelle Umgebungen (venv) in Python kennt, geht Docker noch einen Schritt weiter: Es verpackt nicht nur die Python-Pakete, sondern die gesamte Ausf\u00fchrungsumgebung in einen \"Container\". Das ist wie ein eigenst\u00e4ndiger Mini-Computer, der alles enth\u00e4lt, was eure Anwendung braucht.","title":"Was ist Docker?"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#voraussetzungen","text":"Euer funktionierendes Django-Projekt Docker Desktop (Installation: https://www.docker.com/products/docker-desktop ) VS Code oder PyCharm","title":"Voraussetzungen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#aufgaben","text":"","title":"Aufgaben"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#teil-1-docker-vorbereiten-und-testen","text":"Kann \u00fcbersprungen werden falls es in der vorherigen \u00dcbung bereits funktioniert hat Installiert Docker Desktop Startet Docker Desktop \u00d6ffnet ein Terminal und pr\u00fcft mit docker --version , ob alles funktioniert","title":"Teil 1: Docker vorbereiten und testen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#teil-2-django-projekt-containerisieren","text":"Dockerfile erstellen Erstellt im Hauptverzeichnis eures Projekts eine neue Datei namens Dockerfile F\u00fcgt folgenden Inhalt ein: ```dockerfile FROM python:3.11 WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . CMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"] ``` Docker Compose erstellen Erstellt eine Datei docker-compose.yml F\u00fcgt folgenden Inhalt ein: ```yaml version: '3.8' services: web: build: . ports: - \"8000:8000\" volumes: - .:/app environment: - DEBUG=1 - DJANGO_SETTINGS_MODULE=myproject.settings ```","title":"Teil 2: Django-Projekt containerisieren"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#teil-3-container-starten-und-testen","text":"Container bauen und starten bash docker compose up --build Wartet, bis ihr die Meldung seht, dass der Django-Server l\u00e4uft \u00d6ffnet http://localhost:8000 im Browser Fehlersuche Falls etwas nicht funktioniert: Schaut euch die Konsolen-Ausgabe an Pr\u00fcft mit docker compose logs web , was im Container passiert Vergleicht eure Dateien mit den Vorlagen Es fehlt noch die Integration um MySQL-Server aus der vorherigen Aufgabe, recherchiere selbst wie du diese in Compose integrieren kannst und was man in Django um\u00e4ndern muss","title":"Teil 3: Container starten und testen"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#zusatzaufgaben-fur-schnelle","text":"Container erkunden bash docker compose exec web bash Schaut euch um - was findet ihr in /app ? Logs verstehen Startet den Container mit docker compose up Macht ein paar Anfragen an eure Webseite Was seht ihr in den Logs?","title":"Zusatzaufgaben f\u00fcr Schnelle"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#abgabe","text":"Erstellt ein kurzes Protokoll mit: 1. Screenshots von: - Eurem laufenden Container (Docker Desktop) - Eurer Django-Anwendung im Browser 2. Kurze Erkl\u00e4rung (3-4 S\u00e4tze), was ihr gemacht habt 3. Falls ihr Probleme hattet: Wie habt ihr sie gel\u00f6st?","title":"Abgabe"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#bewertung","text":"Funktionalit\u00e4t (40%) Container l\u00e4uft Django-Anwendung ist erreichbar Code ist sauber und folgt den Vorlagen Dokumentation (30%) Protokoll ist vollst\u00e4ndig Probleme und L\u00f6sungen sind dokumentiert Zusatzaufgaben (30%) Erfolgreiche Durchf\u00fchrung Verst\u00e4ndnis der Docker-Umgebung","title":"Bewertung"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#hilfe","text":"Schaut euch die Log-Ausgaben genau an - oft steht dort schon die L\u00f6sung \u00dcberpr\u00fcft, ob Docker Desktop l\u00e4uft Fragt eure Mitsch\u00fcler oder mich, wenn ihr nicht weiterkommt","title":"Hilfe"},{"location":"Assignments/4XHBGM/MGIN/Uebung3B/#hilfreiche-docker-befehle","text":"Container stoppen: docker-compose down Logs anzeigen: docker compose logs -f Container neustarten: docker-compose restart Alle Container und deren Status anzeigen: docker ps -a In den Container einsteigen: bash docker compose exec <containername> bash","title":"Hilfreiche Docker-Befehle"},{"location":"Assignments/4XHBGM/MGIN/Uebung4/","text":"Zielsetzung Jeder Sch\u00fcler soll: 1. Eine FHIR-Ressource modellieren und Instanzen davon in einer Datenbank speichern. 2. Ein Formular sowie ein Template erstellen, um bestehende Eintr\u00e4ge zu \u00e4ndern, neue Eintr\u00e4ge zu erstellen oder bestehende Eintr\u00e4ge zu l\u00f6schen (CRUD-Funktionalit\u00e4t). 3. Die Implementierung mit Git verwalten und durch viele kleine Commits regelm\u00e4\u00dfig dokumentieren. Technische Anforderungen Programmiersprache : Python Framework : Django FHIR-Bibliothek : fhir.resources (verf\u00fcgbar \u00fcber PyPi) Vorgehensweise Repository klonen 4AHBGM 4BHBGM Branch-Erstellung : Jeder Sch\u00fcler erstellt einen eigenen Branch f\u00fcr die jeweilige FHIR-Ressource. Der Branch muss folgender Namenskonvention folgen: feature/<ressource-name> . Entwicklung : Implementiere f\u00fcr die Ressource, gem\u00e4\u00df den Spezifikationen aus fhir.resources, die erforderlichen Datenbankmodelle, Formulare und Templates. Testing : Teste deine Implementierung gr\u00fcndlich, um sicherzustellen, dass die CRUD-Funktionalit\u00e4ten wie gew\u00fcnscht funktionieren. Verwende dazu das Unittest-Framework von Django W\u00fcnschenswert w\u00e4re eine Coverage von 100%. Nachdem das aber nicht praktikabel ist, reichen auch 75% - 85% Coverage. Push auf Dev-Branch : Sobald die Ressource vollst\u00e4ndig implementiert und getestet ist, pushe deine \u00c4nderungen auf den dev -Branch. Integration : Teste im dev-Branch ob dein Code auch nicht die Funktionalit\u00e4t des Codes deiner Kollegen beeintr\u00e4chtigt. Nach erfolgreichem Testen aller Ressourcen im dev -Branch, erfolgt der Merge in den main -Branch. Erstellt einen Workflow welcher bei jedem pull-request einen Django Test Workflow ausf\u00fchrt. Dokumentation : Jeder Sch\u00fcler erstellt ein Protokoll \u00fcber die eigene Vorgehensweise und dokumentiert dabei alle wesentlichen Schritte. Achte darauf alle Punkte aus den Bewertungsgrundlagen zu erw\u00e4hnen. Die Abgabe des Protokolls kann auch gerne als Text im Pull-Request auf den main-Branch erfolgen. \u00c4nderungsprotokoll : Verwende den folgenden Git-Befehl, um die von dir erstellten Commits anzuzeigen: git log --author=\"<Dein Name>\" Bewertungsgrundlagen FHIR Ressource als Modell erstellt Formular erstellt Template erstellt \u00dcbersicht erstellt mit den Funktionen C reate R ead U pdate D elete Code via Pull-Request auf Dev-Branch gegeben Code via Pull-Request auf main-Branch gegeben Tests erfolgreich","title":"Gruppenprojekt"},{"location":"Assignments/4XHBGM/MGIN/Uebung4/#zielsetzung","text":"Jeder Sch\u00fcler soll: 1. Eine FHIR-Ressource modellieren und Instanzen davon in einer Datenbank speichern. 2. Ein Formular sowie ein Template erstellen, um bestehende Eintr\u00e4ge zu \u00e4ndern, neue Eintr\u00e4ge zu erstellen oder bestehende Eintr\u00e4ge zu l\u00f6schen (CRUD-Funktionalit\u00e4t). 3. Die Implementierung mit Git verwalten und durch viele kleine Commits regelm\u00e4\u00dfig dokumentieren.","title":"Zielsetzung"},{"location":"Assignments/4XHBGM/MGIN/Uebung4/#technische-anforderungen","text":"Programmiersprache : Python Framework : Django FHIR-Bibliothek : fhir.resources (verf\u00fcgbar \u00fcber PyPi)","title":"Technische Anforderungen"},{"location":"Assignments/4XHBGM/MGIN/Uebung4/#vorgehensweise","text":"Repository klonen 4AHBGM 4BHBGM Branch-Erstellung : Jeder Sch\u00fcler erstellt einen eigenen Branch f\u00fcr die jeweilige FHIR-Ressource. Der Branch muss folgender Namenskonvention folgen: feature/<ressource-name> . Entwicklung : Implementiere f\u00fcr die Ressource, gem\u00e4\u00df den Spezifikationen aus fhir.resources, die erforderlichen Datenbankmodelle, Formulare und Templates. Testing : Teste deine Implementierung gr\u00fcndlich, um sicherzustellen, dass die CRUD-Funktionalit\u00e4ten wie gew\u00fcnscht funktionieren. Verwende dazu das Unittest-Framework von Django W\u00fcnschenswert w\u00e4re eine Coverage von 100%. Nachdem das aber nicht praktikabel ist, reichen auch 75% - 85% Coverage. Push auf Dev-Branch : Sobald die Ressource vollst\u00e4ndig implementiert und getestet ist, pushe deine \u00c4nderungen auf den dev -Branch. Integration : Teste im dev-Branch ob dein Code auch nicht die Funktionalit\u00e4t des Codes deiner Kollegen beeintr\u00e4chtigt. Nach erfolgreichem Testen aller Ressourcen im dev -Branch, erfolgt der Merge in den main -Branch. Erstellt einen Workflow welcher bei jedem pull-request einen Django Test Workflow ausf\u00fchrt. Dokumentation : Jeder Sch\u00fcler erstellt ein Protokoll \u00fcber die eigene Vorgehensweise und dokumentiert dabei alle wesentlichen Schritte. Achte darauf alle Punkte aus den Bewertungsgrundlagen zu erw\u00e4hnen. Die Abgabe des Protokolls kann auch gerne als Text im Pull-Request auf den main-Branch erfolgen. \u00c4nderungsprotokoll : Verwende den folgenden Git-Befehl, um die von dir erstellten Commits anzuzeigen: git log --author=\"<Dein Name>\"","title":"Vorgehensweise"},{"location":"Assignments/4XHBGM/MGIN/Uebung4/#bewertungsgrundlagen","text":"FHIR Ressource als Modell erstellt Formular erstellt Template erstellt \u00dcbersicht erstellt mit den Funktionen C reate R ead U pdate D elete Code via Pull-Request auf Dev-Branch gegeben Code via Pull-Request auf main-Branch gegeben Tests erfolgreich","title":"Bewertungsgrundlagen"},{"location":"Assignments/4XHBGM/MGIN/Uebung5/","text":"Entwickle einen Webservice mit Django, der es erm\u00f6glicht, DICOM-Daten anzuzeigen, hochzuladen und zu l\u00f6schen. Als Backend soll das PACS dcm4chee verwendet werden. Schau dir die bereitgestellten Jupyter-Notebooks an, wie diese Daten an das PACS senden und von dort abfragen. Integriere folgenden Funktionen in eine Webanwendung: Hochladen von DICOM-Daten Abfragen von DICOM-Daten Sowohl Voxel(3D)/Pixel(2D) Daten als auch Metadaten \u00c4ndern und erg\u00e4nzen der Metadaten Anforderungen Projektstruktur : Erstelle ein neues Django-Projekt und eine App innerhalb dieses Projekts. DICOM-Daten : Verwende die bereitgestellten DICOM-Daten aus der zip-Datei . Auch ein Beispielcode in Form von Jupyter-Notebooks ist dort enthalten. Views : Implementiere Views, die es erm\u00f6glichen, die DICOM-Daten in einer HTML-Seite anzuzeigen und zu bearbeiten. Templates : Erstelle Templates f\u00fcr die Darstellung der DICOM-Daten. Protokolliere die Implementierung sowie die Tests der Funktionen.","title":"PACS einrichten und ansteuern"},{"location":"Assignments/4XHBGM/MGIN/Uebung5/#anforderungen","text":"Projektstruktur : Erstelle ein neues Django-Projekt und eine App innerhalb dieses Projekts. DICOM-Daten : Verwende die bereitgestellten DICOM-Daten aus der zip-Datei . Auch ein Beispielcode in Form von Jupyter-Notebooks ist dort enthalten. Views : Implementiere Views, die es erm\u00f6glichen, die DICOM-Daten in einer HTML-Seite anzuzeigen und zu bearbeiten. Templates : Erstelle Templates f\u00fcr die Darstellung der DICOM-Daten. Protokolliere die Implementierung sowie die Tests der Funktionen.","title":"Anforderungen"},{"location":"Assignments/4XHBGM/MGIN/whisperIntegration/","text":"Ziel dieser Aufgabe ist es ein bestehendes Modell auszuf\u00fchren und die Ein-/Ausgaben in eine neue Anwendung zu integrieren. Es soll eine Webanwendung erstellt werden in welcher man via dem Browser auf das Mikrofon zugreifen kann. Der aufgenommene Sound soll dann an einen Server gesendet werden welcher diese Daten an das Modell whisper-small weitergibt. Der Output des Modells soll dann als Antwort zum Client der Webanwendung geschickt werden. Die einzelnen Modelle/Technologien kann man ggf. auch einfach f\u00fcr Zuk\u00fcnftige Projekte austauschen. Auf Huggingface gibt es ein riesige Sammlung an Modellen die man so oder so \u00e4hnlich in Anwendungen integrieren kann. Es gibt spezialisierte Modelle f\u00fcr: - Semantische Segmentierung - Bildtext zu Text - Bild zu Text - Objekt-Erkennung - Text-Klassifikation - Zusammenfassungen von Text - Text-\u00c4hnlichkeit - uvm. Auch immer mehr multimodale Modelle f\u00fcr z.B: - Fragen zu einem Dokument beantworten - Text zu Video - uvm. Ben\u00f6tigte Libraries Um diese Aufgabe zu implementieren ben\u00f6tigt man 1. Eine Umgebung zum ausf\u00fchren des neuronalen Netzes 2. Einen JavaScript-Code welcher Sound aufnehmen kann 3. Einen JavaScript-Code welcher Dateien an einen Server schicken kann 4. Einen Server der Audiodateien annimmt, und via HTTP mit Text antwortet Zu Punkt 1: Das bereitgestellte Jupyter-Notebook bzw. die Infoseite auf Huggingface beschreibt wie man Whisper-Small ausf\u00fchren kann. Modelle von Huggingface kann man mit deren transformers library f\u00fcr Python ausf\u00fchren. Diese hat PyTorch als eine dependency. Eine Umfangreiche library \u00e4nlich wie Tensorflow oder JAX zum erstellen, trainieren und ausf\u00fchren von neuronalen Netzen. PyTorch hat eine gr\u00f6\u00dfe von ca. 3 GB und muss daher zuhause schon vorinstalliert werden. Wenn man das nur in der Schule runterl\u00e4dt mit dem langsamen WLAN dort, dann geht sich das innerhalb einer Unterrichtseinheit nicht aus. Hat man eine Nvidia oder AMD Grafikkarte kann man Pytorch auch so installieren dass es einige Berechnungen (vorwiegend Matrix-Multiplikationen) auf die GPU auslagert. Dazu gibt es von den Herstellern der GPUs Programme womit man allgemeine Befehle an die GPU schicken kann. Diese Programme sind CUDA f\u00fcr Nvidia bzw. ROCm f\u00fcr AMD. Bei der installation von PyTorch steht dabei welches Programm damit kompatibel ist. F\u00fcr CUDA ist das aktuell noch die Version 12.1, ROCm (nur auf Linux/WSL) braucht die Version 6. Installiert euch je nach Grafikkarte eine dieser Beschleuniger vor der Installation von PyTorch, dann ist die Ausf\u00fchrung von den neuronalen Netzen ca. 10 mal schneller. Hat man keine der beiden Karten im Laptop kann man auch auf Colab ausweichen oder man f\u00fchrt es auf der CPU aus. Beides ist nicht ideal, die CPU ist langsam und Colab muss man viel (wiederholt) konfigurieren. Auch wird es dann nicht lokal ausgef\u00fchrt. CUDA 12.1 findet ihr hier , ROCm 6 hier . Zu Punkt 2: Um Sound aufzunehmen gibt es einen Beispielcode in folgendem Repository . Zu Punkt 3: Der Soundrekorder aus Punkt 2 erzeugt ein JavaScript Blob -Objekt. Wie man dieses auf einem Server hochladen kann siehe dieses Tutorial . Zu Punkt 4: Um die Sounddaten annehmen zu k\u00f6nnen und an die Transformer-Library weitergeben zu k\u00f6nnen w\u00fcrde sich ein Webserver in Python anbieten. In der FastAPI-Dokumentation gibt es eine Anleitung wie dieser Server Dateien annehmen kann. F\u00fcr die ersten Schritte in FastAPI gibt es ein Tutorial hier . Abgabe Gib ein Protokoll ab in dem du die Erweiterung der vorherigen Aufgabe beschreibst. Die Chat-Anwendung soll um einen \"Aufnehmen\" Button erweitert werden. Das Gesagte soll dann im Textfeld zum absenden der Nachricht eingef\u00fcgt werden.","title":"KI-Integration in eine Webapplikation"},{"location":"Assignments/4XHBGM/MGIN/whisperIntegration/#benotigte-libraries","text":"Um diese Aufgabe zu implementieren ben\u00f6tigt man 1. Eine Umgebung zum ausf\u00fchren des neuronalen Netzes 2. Einen JavaScript-Code welcher Sound aufnehmen kann 3. Einen JavaScript-Code welcher Dateien an einen Server schicken kann 4. Einen Server der Audiodateien annimmt, und via HTTP mit Text antwortet","title":"Ben\u00f6tigte Libraries"},{"location":"Assignments/4XHBGM/MGIN/whisperIntegration/#zu-punkt-1","text":"Das bereitgestellte Jupyter-Notebook bzw. die Infoseite auf Huggingface beschreibt wie man Whisper-Small ausf\u00fchren kann. Modelle von Huggingface kann man mit deren transformers library f\u00fcr Python ausf\u00fchren. Diese hat PyTorch als eine dependency. Eine Umfangreiche library \u00e4nlich wie Tensorflow oder JAX zum erstellen, trainieren und ausf\u00fchren von neuronalen Netzen. PyTorch hat eine gr\u00f6\u00dfe von ca. 3 GB und muss daher zuhause schon vorinstalliert werden. Wenn man das nur in der Schule runterl\u00e4dt mit dem langsamen WLAN dort, dann geht sich das innerhalb einer Unterrichtseinheit nicht aus. Hat man eine Nvidia oder AMD Grafikkarte kann man Pytorch auch so installieren dass es einige Berechnungen (vorwiegend Matrix-Multiplikationen) auf die GPU auslagert. Dazu gibt es von den Herstellern der GPUs Programme womit man allgemeine Befehle an die GPU schicken kann. Diese Programme sind CUDA f\u00fcr Nvidia bzw. ROCm f\u00fcr AMD. Bei der installation von PyTorch steht dabei welches Programm damit kompatibel ist. F\u00fcr CUDA ist das aktuell noch die Version 12.1, ROCm (nur auf Linux/WSL) braucht die Version 6. Installiert euch je nach Grafikkarte eine dieser Beschleuniger vor der Installation von PyTorch, dann ist die Ausf\u00fchrung von den neuronalen Netzen ca. 10 mal schneller. Hat man keine der beiden Karten im Laptop kann man auch auf Colab ausweichen oder man f\u00fchrt es auf der CPU aus. Beides ist nicht ideal, die CPU ist langsam und Colab muss man viel (wiederholt) konfigurieren. Auch wird es dann nicht lokal ausgef\u00fchrt. CUDA 12.1 findet ihr hier , ROCm 6 hier .","title":"Zu Punkt 1:"},{"location":"Assignments/4XHBGM/MGIN/whisperIntegration/#zu-punkt-2","text":"Um Sound aufzunehmen gibt es einen Beispielcode in folgendem Repository .","title":"Zu Punkt 2:"},{"location":"Assignments/4XHBGM/MGIN/whisperIntegration/#zu-punkt-3","text":"Der Soundrekorder aus Punkt 2 erzeugt ein JavaScript Blob -Objekt. Wie man dieses auf einem Server hochladen kann siehe dieses Tutorial .","title":"Zu Punkt 3:"},{"location":"Assignments/4XHBGM/MGIN/whisperIntegration/#zu-punkt-4","text":"Um die Sounddaten annehmen zu k\u00f6nnen und an die Transformer-Library weitergeben zu k\u00f6nnen w\u00fcrde sich ein Webserver in Python anbieten. In der FastAPI-Dokumentation gibt es eine Anleitung wie dieser Server Dateien annehmen kann. F\u00fcr die ersten Schritte in FastAPI gibt es ein Tutorial hier .","title":"Zu Punkt 4:"},{"location":"Assignments/4XHBGM/MGIN/whisperIntegration/#abgabe","text":"Gib ein Protokoll ab in dem du die Erweiterung der vorherigen Aufgabe beschreibst. Die Chat-Anwendung soll um einen \"Aufnehmen\" Button erweitert werden. Das Gesagte soll dann im Textfeld zum absenden der Nachricht eingef\u00fcgt werden.","title":"Abgabe"},{"location":"Assignments/4XHBGM/MIS/Assignment1/","text":"1. Haus\u00fcbung Erstelle ein neues Spring-Basisprojekt auf start.spring.io . Verwende als Javaversion die Nummer 18. Das Projekt soll folgende Metadaten enthalten: Group at.spengergasse Artifact spengermed Name Spengermed Package name at.spengergasse.spengermed F\u00fcge als Dependencies hinzu: - spring web - lombok - mysql driver - rest repositories Falls du welche vergessen hast kannst du sie auch nachtr\u00e4glich \u00fcber die pom.xml nachbessern. Entpacke das Basisprojekt in das Repository. Commite und pushe die neuen Dateien und stelle sicher dass alle Tests erfolgreich waren.","title":"1. Haus\u00fcbung"},{"location":"Assignments/4XHBGM/MIS/Assignment1/#1-hausubung","text":"Erstelle ein neues Spring-Basisprojekt auf start.spring.io . Verwende als Javaversion die Nummer 18. Das Projekt soll folgende Metadaten enthalten: Group at.spengergasse Artifact spengermed Name Spengermed Package name at.spengergasse.spengermed F\u00fcge als Dependencies hinzu: - spring web - lombok - mysql driver - rest repositories Falls du welche vergessen hast kannst du sie auch nachtr\u00e4glich \u00fcber die pom.xml nachbessern. Entpacke das Basisprojekt in das Repository. Commite und pushe die neuen Dateien und stelle sicher dass alle Tests erfolgreich waren.","title":"1. Haus\u00fcbung"},{"location":"Assignments/4XHBGM/MIS/MultiService/","text":"Aktuell l\u00e4uft unser Spring-Projekt komplett eigenst\u00e4ndig. Das macht das Entwickeln leichter, w\u00e4hrend der Ausf\u00fchrung kann das aber zu Schwierigkeiten f\u00fchren. Z.b l\u00f6scht die H2-Datenbank alle Daten wenn sich das Programm beendet. Es gibt keine Backups der Daten und der Server kann nur lokal erreicht werden, zudem ohne TLS-Verschl\u00fcsselung. Eure Aufgabe ist es zun\u00e4chst von H2 auf eine \"richtige\" Datenbank, n\u00e4mlich Postgres, umzusteigen. Dabei k\u00f6nnt ihr schon auf Docker zur\u00fcckgreifen - erstellt zun\u00e4chst mit einem Dockerfile euren eigenen Container - und schaltet den dann mit einem bestehenden Postgres Container in compose zusammen Erstellt ein Protokoll dar\u00fcber wie ihr vorgegangen seid. Ihr k\u00f6nnt alleine oder in 2er-Gruppen arbeiten. In dem Protokoll soll enthalten sein: - Eine Anleitung wie man das safeapi-Projekt auf postgres umstellt - Wie man die Postgres-Version in einen Container packt - Wie man den lokalen Container mit einem externen Postgres Container zusammenarbeiten l\u00e4sst - Aufgekommene Probleme und L\u00f6sungsstrategien - Mit Quellenangaben zu den L\u00f6sungen bzw. verwendeten Anleitungen","title":"Spring in Verbindung mit weiteren Services nutzen"},{"location":"Assignments/4XHBGM/MIS/OIDC/","text":"https - eine Verschl\u00fcsselte Verbindung zum Server einrichten F\u00fcr https brauchen wir Zertifikate. Diese k\u00f6nnen wir w\u00e4hrend der Entwicklung selbst erstellen \u00fcber das java keytool. Wenn die Webseite ver\u00f6ffentlicht wird, brauchen wir auch ein vertrauensw\u00fcrdiges Zertifikat von einer Certificate Authority (CA). LetsEncrypt w\u00fcrde sich hier z.B anbieten. Wir werden zun\u00e4chst ein eigenes Zertifikat erstellen. Es gibt dazu Anleitung hier . Beachte folgendes: Der Alias ist statt \"baeldung\" => \"spengermed\" zu nennen. Nur bis inkl. Kapitel 3 umsetzen, Kapitel 4 brauchen wir nicht. Die Dependencies von hier unten nehmen, nicht spring boot security. die Keystore Dateien musst du noch unter ~ resources/keystore/spengermed.p12 ~ kopieren. Teste es danach unter: https://localhost:443/api/ <deine-ressource> Achtung - die selbst zertifizierten Zertifikate sind f\u00fcr Browser nicht vertrauensw\u00fcrdig. Entweder die Zertifikate in den Browser laden, oder das angezeigte Sicherheitsrisiko akzaptieren. Authentication mit OAuth Als n\u00e4chstes wollen wir eine ganz einfache Login-Funktion mit GitHub einrichten. Dazu ben\u00f6tigst du zuerst einen GitHub Account. Wir orientieren uns an folgender Anleitung: https://spring.io/guides/tutorials/spring-boot-oauth2/ Wobei wir nicht alles ausf\u00fchren m\u00fcssen und die wichtigsten Punkte hier zusammengefasst sind. Wer die Applikation weiter ausbauen m\u00f6chte, kann jedoch die Anleitung weiter umsetzen. Authentication ist der Nachweis, dass der Benutzer auch berechtigt ist, auf die Daten zuzugreifen. Mit OAuth2.0 k\u00f6nnen wir Benutzerkonten von bereits existierenden Seiten Wie GitHub, Google usw. verwenden oder unseren eigenen Authentifizierungs-Server einrichten. Wir verwenden hier den Server von GitHub, wobei auch andere leicht eingebunden werden k\u00f6nnen. Erweiterung der pom.xml <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-oauth2-client</artifactId> </dependency> <dependency> <groupId>org.webjars</groupId> <artifactId>jquery</artifactId> <version>3.4.1</version> </dependency> <dependency> <groupId>org.webjars</groupId> <artifactId>bootstrap</artifactId> <version>4.3.1</version> </dependency> <dependency> <groupId>org.webjars</groupId> <artifactId>webjars-locator-core</artifactId> </dependency> Registrieren der App unter https://github.com/settings/developers Und eintragen der Daten, wobei die Client ID und das Secret nat\u00fcrlich unterschiedlich sein m\u00fcssen. Erg\u00e4nzung der application.properties #start oauth2 spring.security.oauth2.client.registration.github.clientId: <Eigene Client ID hier einf\u00fcgen> spring.security.oauth2.client.registration.github.clientSecret: <Eigenes Secret hier einf\u00fcgen> #end oauth2 Applikation aufrufen mit https://localhost:443 Nach dem Login kannst du auf :https://localhost:443/api/ <deine-ressource> zugreifen. Davor sollte das nicht m\u00f6glich sein. F\u00fcge auch einen Endpunkt https://localhost:443/user/name hinzu welcher deinen Github-Usernamen ausgibt mit dem du dich via OAuth2 authentifiziert hast. Nachdem die FilterChain den Zugriff auf alle Ressourcen ohne OAuth blockieren sollte. Diese k\u00f6nnten z.B so aussehen: @Bean public SecurityFilterChain oauthFilterChain(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(\"/oauth/**\", \"/user/**\", \"/<deine-ressource>/**\") .authenticated() .and() .oauth2Login() .userInfoEndpoint() .oidcUserService(oidcUserService()); return http.build(); } Du kannst ein privates/inkognito Fenster \u00f6ffnen, um den Login erneut zu testen oder die Cookies l\u00f6schen.","title":"Spring OIDC & OAuth"},{"location":"Assignments/4XHBGM/MIS/OIDC/#https-eine-verschlusselte-verbindung-zum-server-einrichten","text":"F\u00fcr https brauchen wir Zertifikate. Diese k\u00f6nnen wir w\u00e4hrend der Entwicklung selbst erstellen \u00fcber das java keytool. Wenn die Webseite ver\u00f6ffentlicht wird, brauchen wir auch ein vertrauensw\u00fcrdiges Zertifikat von einer Certificate Authority (CA). LetsEncrypt w\u00fcrde sich hier z.B anbieten. Wir werden zun\u00e4chst ein eigenes Zertifikat erstellen. Es gibt dazu Anleitung hier . Beachte folgendes: Der Alias ist statt \"baeldung\" => \"spengermed\" zu nennen. Nur bis inkl. Kapitel 3 umsetzen, Kapitel 4 brauchen wir nicht. Die Dependencies von hier unten nehmen, nicht spring boot security. die Keystore Dateien musst du noch unter ~ resources/keystore/spengermed.p12 ~ kopieren. Teste es danach unter: https://localhost:443/api/ <deine-ressource> Achtung - die selbst zertifizierten Zertifikate sind f\u00fcr Browser nicht vertrauensw\u00fcrdig. Entweder die Zertifikate in den Browser laden, oder das angezeigte Sicherheitsrisiko akzaptieren.","title":"https - eine Verschl\u00fcsselte Verbindung zum Server einrichten"},{"location":"Assignments/4XHBGM/MIS/OIDC/#authentication-mit-oauth","text":"Als n\u00e4chstes wollen wir eine ganz einfache Login-Funktion mit GitHub einrichten. Dazu ben\u00f6tigst du zuerst einen GitHub Account. Wir orientieren uns an folgender Anleitung: https://spring.io/guides/tutorials/spring-boot-oauth2/ Wobei wir nicht alles ausf\u00fchren m\u00fcssen und die wichtigsten Punkte hier zusammengefasst sind. Wer die Applikation weiter ausbauen m\u00f6chte, kann jedoch die Anleitung weiter umsetzen. Authentication ist der Nachweis, dass der Benutzer auch berechtigt ist, auf die Daten zuzugreifen. Mit OAuth2.0 k\u00f6nnen wir Benutzerkonten von bereits existierenden Seiten Wie GitHub, Google usw. verwenden oder unseren eigenen Authentifizierungs-Server einrichten. Wir verwenden hier den Server von GitHub, wobei auch andere leicht eingebunden werden k\u00f6nnen.","title":"Authentication mit OAuth"},{"location":"Assignments/4XHBGM/MIS/OIDC/#erweiterung-der-pomxml","text":"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-oauth2-client</artifactId> </dependency> <dependency> <groupId>org.webjars</groupId> <artifactId>jquery</artifactId> <version>3.4.1</version> </dependency> <dependency> <groupId>org.webjars</groupId> <artifactId>bootstrap</artifactId> <version>4.3.1</version> </dependency> <dependency> <groupId>org.webjars</groupId> <artifactId>webjars-locator-core</artifactId> </dependency>","title":"Erweiterung der pom.xml"},{"location":"Assignments/4XHBGM/MIS/OIDC/#registrieren-der-app-unter-httpsgithubcomsettingsdevelopers","text":"Und eintragen der Daten, wobei die Client ID und das Secret nat\u00fcrlich unterschiedlich sein m\u00fcssen.","title":"Registrieren der App unter https://github.com/settings/developers"},{"location":"Assignments/4XHBGM/MIS/OIDC/#erganzung-der-applicationproperties","text":"#start oauth2 spring.security.oauth2.client.registration.github.clientId: <Eigene Client ID hier einf\u00fcgen> spring.security.oauth2.client.registration.github.clientSecret: <Eigenes Secret hier einf\u00fcgen> #end oauth2 Applikation aufrufen mit https://localhost:443 Nach dem Login kannst du auf :https://localhost:443/api/ <deine-ressource> zugreifen. Davor sollte das nicht m\u00f6glich sein. F\u00fcge auch einen Endpunkt https://localhost:443/user/name hinzu welcher deinen Github-Usernamen ausgibt mit dem du dich via OAuth2 authentifiziert hast. Nachdem die FilterChain den Zugriff auf alle Ressourcen ohne OAuth blockieren sollte. Diese k\u00f6nnten z.B so aussehen: @Bean public SecurityFilterChain oauthFilterChain(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(\"/oauth/**\", \"/user/**\", \"/<deine-ressource>/**\") .authenticated() .and() .oauth2Login() .userInfoEndpoint() .oidcUserService(oidcUserService()); return http.build(); } Du kannst ein privates/inkognito Fenster \u00f6ffnen, um den Login erneut zu testen oder die Cookies l\u00f6schen.","title":"Erg\u00e4nzung der application.properties"},{"location":"Assignments/4XHBGM/MIS/SpringDataKRM/","text":"Im Ordner src/test/.../ befindet sich die Datei SpengermedApplicationTests.java . F\u00fchre diese Tests mit dem Kommando mvn test aus um zu pr\u00fcfen ob dein Code die Anforderungen f\u00fcr eine Abgabe erf\u00fcllt. Achte auf eine ordentliche Ordnerstruktur innerhalb des src Ordners. Diese k\u00f6nnte so aussehen src main java at.spengergasse.spengermed controller repository entity ... Anleitung Hier: https://spring.io/guides/gs/accessing-data-jpa/ ist erkl\u00e4rt wie man zu Spring eine Datenbank hinzuf\u00fcgen kann. Ab dem Punkt Define a Simple Entity beginnt der relevante Teil. Alles davor wurde in diesem Repository bereits vorbereitet.","title":"Spring Data Assignment"},{"location":"Assignments/4XHBGM/MIS/SpringDataKRM/#anleitung","text":"Hier: https://spring.io/guides/gs/accessing-data-jpa/ ist erkl\u00e4rt wie man zu Spring eine Datenbank hinzuf\u00fcgen kann. Ab dem Punkt Define a Simple Entity beginnt der relevante Teil. Alles davor wurde in diesem Repository bereits vorbereitet.","title":"Anleitung"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/","text":"Aufgabe Implementiere die Entit\u00e4ten Patient und Practitioner auf eine Weise dass sie mit dem FHIR-Standard kompatibel sind. Diese FHIR-Ressoucen haben auch eine vielzahl an Unterressoucen welche ebenfalls implementiert werden m\u00fcssen. Die Entit\u00e4ten Patient und Practitioner m\u00fcssen nicht komplett umgesetzt werden. Sollte ein Subtyp noch ein JSON-Objekt als Subtyp haben, kann dieses durch einen einfachen Typen (String, Number, Boolean, Null) ersetzt werden. Wird also z.B auf eine Organization verwiesen - eine eher komplexe Ressource - kann diese z.B durch einen String ersetzt werden in dem der Firmenname eingetragen ist. Patient { \"resourceType\" : \"Patient\", // from Resource: id, meta, implicitRules, and language // from DomainResource: text, contained, extension, and modifierExtension \"identifier\" : [{ Identifier }], // An identifier for this patient \"active\" : <boolean>, // Whether this patient's record is in active use \"name\" : [{ HumanName }], // A name associated with the patient \"telecom\" : [{ ContactPoint }], // A contact detail for the individual \"gender\" : \"<code>\", // male | female | other | unknown \"birthDate\" : \"<date>\", // The date of birth for the individual // deceased[x]: Indicates if the individual is deceased or not. One of these 2: \"deceasedBoolean\" : <boolean>, // NICHT NOTWENDIG\"deceasedDateTime\" : \"<dateTime>\", \"address\" : [{ Address }], // An address for the individual // NICHT NOTWENDIG\"maritalStatus\" : { CodeableConcept }, // Marital (civil) status of a patient // multipleBirth[x]: Whether patient is part of a multiple birth. One of these 2: \"multipleBirthBoolean\" : <boolean>, // NICHT NOTWENDIG\"multipleBirthInteger\" : <integer>, // NICHT NOTWENDIG\"photo\" : [{ Attachment }], // Image of the patient \"contact\" : [{ // A contact party (e.g. guardian, partner, friend) for the patient \"relationship\" : [{ CodeableConcept }], // The kind of relationship \"name\" : { HumanName }, // A name associated with the contact person \"telecom\" : [{ ContactPoint }], // A contact detail for the person \"address\" : { Address }, // Address for the contact person // NICHT NOTWENDIG\"gender\" : \"<code>\", // male | female | other | unknown // NICHT NOTWENDIG\"organization\" : { Reference(Organization) }, // C? Organization that is associated with the contact // NICHT NOTWENDIG\"period\" : { Period } // The period during which this contact person or organization is valid to be contacted relating to this patient }], // NICHT NOTWENDIG\"communication\" : [{ // A language which may be used to communicate with the patient about his or her health // NICHT NOTWENDIG \"language\" : { CodeableConcept }, // R! The language which can be used to communicate with the patient about his or her health // NICHT NOTWENDIG \"preferred\" : <boolean> // Language preference indicator }], // NICHT NOTWENDIG\"generalPractitioner\" : [{ Reference(Organization|Practitioner| PractitionerRole) }], // Patient's nominated primary care provider // NICHT NOTWENDIG\"managingOrganization\" : { Reference(Organization) }, // Organization that is the custodian of the patient record // NICHT NOTWENDIG\"link\" : [{ // Link to another patient resource that concerns the same actual person // NICHT NOTWENDIG \"other\" : { Reference(Patient|RelatedPerson) }, // R! The other patient or related person resource that the link refers to // NICHT NOTWENDIG \"type\" : \"<code>\" // R! replaced-by | replaces | refer | seealso }] } Practitioner { \"resourceType\" : \"Practitioner\", // from Resource: id, meta, implicitRules, and language // from DomainResource: text, contained, extension, and modifierExtension \"identifier\" : [{ Identifier }], // An identifier for the person as this agent \"active\" : <boolean>, // Whether this practitioner's record is in active use \"name\" : [{ HumanName }], // The name(s) associated with the practitioner \"telecom\" : [{ ContactPoint }], // A contact detail for the practitioner (that apply to all roles) \"address\" : [{ Address }], // Address(es) of the practitioner that are not role specific (typically home address) \"gender\" : \"<code>\", // male | female | other | unknown \"birthDate\" : \"<date>\", // The date on which the practitioner was born // NICHT NOTWENDIG\"photo\" : [{ Attachment }], // Image of the person // NICHT NOTWENDIG\"qualification\" : [{ // Certification, licenses, or training pertaining to the provision of care // NICHT NOTWENDIG\"identifier\" : [{ Identifier }], // An identifier for this qualification for the practitioner // NICHT NOTWENDIG\"code\" : { CodeableConcept }, // R! Coded representation of the qualification // NICHT NOTWENDIG\"period\" : { Period }, // Period during which the qualification is valid // NICHT NOTWENDIG\"issuer\" : { Reference(Organization) } // Organization that regulates and issues the qualification // NICHT NOTWENDIG}], // NICHT NOTWENDIG\"communication\" : [{ CodeableConcept }] // A language the practitioner can use in patient communication } Lombok Tipp: Um nicht immer Getter, Setter etc. in einer Klasse implementieren zu m\u00fcssen, kann man mit lombok (bereits in dem Projekt eingebunden) sich diese im Java-Bytecode automatisch generieren lassen. Beispiel-Klasse @Getter @Setter @AllArgsConstructor @NoArgsConstructor @ToString class Something { int someProperty = 0; } Tipps f\u00fcr die Implementierung Da sowohl Patient als auch Pracitioner Personen sind w\u00e4re es m\u00f6glich eine abstrakte Basisiklasse Person zu erstellen in der allgemeine Properties bereits enthalten sind. Da beide Controller ein sehr \u00e4hnliches Verhalten haben kann man dieses unabh\u00e4ngig vom Typ mittels generics implementieren. F\u00fcr je Patient als auch Pracitioner kann dann eine Ableitung davon anderen Mappings zugeordnet sein. public class PersonController<T, ...>{ public List<T> getAllEntities(){ ... } ... } @RequestMapping(\"/patient\") public class PatientController { PersonController<Patient> baseController; @GetMapping(\"/\") public List<Patient> getAllEntities() { return baseController.getAllEntities(); } ... } Daten miteinander logisch verkn\u00fcpfen M\u00f6chte man zwei Tabellen in SQL Miteinander verkn\u00fcpfen ist dies \u00fcber einen foreign key constraint mit anschliessendem JOIN m\u00f6glich. Spring (insbesondere die JPA) weis aber nicht wie die Entit\u00e4ten (Klassen mit @Entitiy -Annotation) zusammengeh\u00f6ren, es sei den man gibt ihr die richtigen Annotationen. Hat man also in SQL 2 miteinander verbundende Tabellen: create table Practitioner( id integer PRIMARY KEY fullName text ) create table Patient( id integer PRIMARY KEY fullName text, attendingPractitioner integer references Practitioner(id) ) w\u00e4re das \u00e4quivalent in Java dazu @Entity @Getter @Setter @Table(name = \"Patient\") class Patient{ @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"pat_id\", nullable = false) Long pNr; @Column(name = \"fullName\") String fullName; // join column ist nicht unbedingt notwendig, hier genauer erkl\u00e4rt // -> https://stackoverflow.com/a/37542849/17996814 @JoinColumn(name=\"pract_id\", nullable = false) @OneToOne Practitioner attendingPractitioner; } @Entity @Getter @Setter @Table(name = \"Practitioner\") class Practitioner{ @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"pract_id\", nullable = false) Long pNr; @Column(name = \"fullName\") String fullName; } FHIR FHIR steht f\u00fcr Fast Healthcare Interoperability Resources . Und ist ein von HL7 v er\u00f6ffentlichter Standard um Daten im Gesundheitsbereich Programm\u00fcbergreifend austauschen zu k\u00f6nnen. HL7 ist eine Organisation welche f\u00fcr die ANSI in den USA Standards im Gesundheitswesen entwickelt. Durch diesen Standard k\u00f6nnen alle m\u00f6glichen Gesundheitsprogrammen Daten untereinander austauschen. Tests Implementiere deinen eigenen Test der die Daten auf FHIR-Kompatibilit\u00e4t pr\u00fcft. Du kannst pr\u00fcfen, ob die von dir generierten JSON-Daten f\u00fcr einen Patienten passen, indem du sie mit der Struktur von hier vergleichst. Testdaten im JSON-Format gibt es f\u00fcr den Patienten und den Practitioner bereits von FHIR. Vergleiche mittels Soll- und Ist-Wert (Expected and Actual Value) ob die von Spring generierten Daten den FHIR-Referenzdaten gleichen. Mehr dazu im folgenden Beispiel: Beispiel-Test @Test public void testCompareReturnedPatientJSONtoFHIRCompliantJSON() throws Exception { mockMvc .perform(get(\"/patient/1\")) // get patient with id 1 .andExpect(status().isOk()) // expect 200 HTTP status code .andExpect(content().json('{\"your_patient\": \"test_data\"}')); // returned data should be of type json and // contain the same parameters as the test data } Die initialen Testdaten sollen in der Ressourcendatei import.sql direkt als SQL-Statements eingef\u00fcgt werden. Beim Start deines Programmes werden alle SQL-Befehle in dieser Datei ausgef\u00fchrt. Generierung von SQL-Daten Um zu testen ob deine SQL-Statements funktionieren kannst du sie bevor du sie in import.sql einf\u00fcgst auch manuell ausf\u00fchren. Starte dazu den Webserfer und gehe dann auf die Weboberfl\u00e4che der H2-Datenbank. Diese erreichst du unter: http://localhost:8080/h2-console Wenn dein Befehl ohne Fehler ausgef\u00fchrt wird, kannst du ihn bedenkenlos einf\u00fcgen. Beachte aber, dass im Ausgangszustand dieses Projekts noch keine Entit\u00e4ten angelegt wurden. Es ist also noch kein CREATE TABLE -Statement ausgef\u00fchrt worden. API-Designrichtlinien Bitte beachte bei den HTTP-Requestmethoden den dazugeh\u00f6rigen RFC 7231 Sektion 4.3 Kurz zusammengefasst, wann man was nimmt: GET Wenn man eine Ressource holt POST Wenn man eine nicht identifizierte Ressource hochl\u00e4dt oder \u00e4ndert PUT Wenn man eine identifizierte Ressource hochl\u00e4dt PATCH Wenn man eine identifizierte Ressource teilweise \u00e4ndert. Hat einen extra RFC, siehe: RFC 5789 DELETE Wenn man eine Ressource l\u00f6scht Auch gibt es einen sch\u00f6nen Blogartikel von Stackoverflow in dem gutes API-Design erkl\u00e4rt wird. Blogartikel: Best practises for REST API Design Controller Die Controller sollen es erm\u00f6glichen die Daten mit GET auszulesen. Mit PUT hinzuzuf\u00fcgen. Mit DELETE zu l\u00f6schen und mit PATCH zu ver\u00e4ndern. Also alle CRUD -Methoden (Create, Read, Update, Delete). Microsoft hat das hier etwas genauer erkl\u00e4rt. Jedoch mit anderen Request-Methoden.","title":"Spring FHIR"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#aufgabe","text":"Implementiere die Entit\u00e4ten Patient und Practitioner auf eine Weise dass sie mit dem FHIR-Standard kompatibel sind. Diese FHIR-Ressoucen haben auch eine vielzahl an Unterressoucen welche ebenfalls implementiert werden m\u00fcssen. Die Entit\u00e4ten Patient und Practitioner m\u00fcssen nicht komplett umgesetzt werden. Sollte ein Subtyp noch ein JSON-Objekt als Subtyp haben, kann dieses durch einen einfachen Typen (String, Number, Boolean, Null) ersetzt werden. Wird also z.B auf eine Organization verwiesen - eine eher komplexe Ressource - kann diese z.B durch einen String ersetzt werden in dem der Firmenname eingetragen ist.","title":"Aufgabe"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#patient","text":"{ \"resourceType\" : \"Patient\", // from Resource: id, meta, implicitRules, and language // from DomainResource: text, contained, extension, and modifierExtension \"identifier\" : [{ Identifier }], // An identifier for this patient \"active\" : <boolean>, // Whether this patient's record is in active use \"name\" : [{ HumanName }], // A name associated with the patient \"telecom\" : [{ ContactPoint }], // A contact detail for the individual \"gender\" : \"<code>\", // male | female | other | unknown \"birthDate\" : \"<date>\", // The date of birth for the individual // deceased[x]: Indicates if the individual is deceased or not. One of these 2: \"deceasedBoolean\" : <boolean>, // NICHT NOTWENDIG\"deceasedDateTime\" : \"<dateTime>\", \"address\" : [{ Address }], // An address for the individual // NICHT NOTWENDIG\"maritalStatus\" : { CodeableConcept }, // Marital (civil) status of a patient // multipleBirth[x]: Whether patient is part of a multiple birth. One of these 2: \"multipleBirthBoolean\" : <boolean>, // NICHT NOTWENDIG\"multipleBirthInteger\" : <integer>, // NICHT NOTWENDIG\"photo\" : [{ Attachment }], // Image of the patient \"contact\" : [{ // A contact party (e.g. guardian, partner, friend) for the patient \"relationship\" : [{ CodeableConcept }], // The kind of relationship \"name\" : { HumanName }, // A name associated with the contact person \"telecom\" : [{ ContactPoint }], // A contact detail for the person \"address\" : { Address }, // Address for the contact person // NICHT NOTWENDIG\"gender\" : \"<code>\", // male | female | other | unknown // NICHT NOTWENDIG\"organization\" : { Reference(Organization) }, // C? Organization that is associated with the contact // NICHT NOTWENDIG\"period\" : { Period } // The period during which this contact person or organization is valid to be contacted relating to this patient }], // NICHT NOTWENDIG\"communication\" : [{ // A language which may be used to communicate with the patient about his or her health // NICHT NOTWENDIG \"language\" : { CodeableConcept }, // R! The language which can be used to communicate with the patient about his or her health // NICHT NOTWENDIG \"preferred\" : <boolean> // Language preference indicator }], // NICHT NOTWENDIG\"generalPractitioner\" : [{ Reference(Organization|Practitioner| PractitionerRole) }], // Patient's nominated primary care provider // NICHT NOTWENDIG\"managingOrganization\" : { Reference(Organization) }, // Organization that is the custodian of the patient record // NICHT NOTWENDIG\"link\" : [{ // Link to another patient resource that concerns the same actual person // NICHT NOTWENDIG \"other\" : { Reference(Patient|RelatedPerson) }, // R! The other patient or related person resource that the link refers to // NICHT NOTWENDIG \"type\" : \"<code>\" // R! replaced-by | replaces | refer | seealso }] }","title":"Patient"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#practitioner","text":"{ \"resourceType\" : \"Practitioner\", // from Resource: id, meta, implicitRules, and language // from DomainResource: text, contained, extension, and modifierExtension \"identifier\" : [{ Identifier }], // An identifier for the person as this agent \"active\" : <boolean>, // Whether this practitioner's record is in active use \"name\" : [{ HumanName }], // The name(s) associated with the practitioner \"telecom\" : [{ ContactPoint }], // A contact detail for the practitioner (that apply to all roles) \"address\" : [{ Address }], // Address(es) of the practitioner that are not role specific (typically home address) \"gender\" : \"<code>\", // male | female | other | unknown \"birthDate\" : \"<date>\", // The date on which the practitioner was born // NICHT NOTWENDIG\"photo\" : [{ Attachment }], // Image of the person // NICHT NOTWENDIG\"qualification\" : [{ // Certification, licenses, or training pertaining to the provision of care // NICHT NOTWENDIG\"identifier\" : [{ Identifier }], // An identifier for this qualification for the practitioner // NICHT NOTWENDIG\"code\" : { CodeableConcept }, // R! Coded representation of the qualification // NICHT NOTWENDIG\"period\" : { Period }, // Period during which the qualification is valid // NICHT NOTWENDIG\"issuer\" : { Reference(Organization) } // Organization that regulates and issues the qualification // NICHT NOTWENDIG}], // NICHT NOTWENDIG\"communication\" : [{ CodeableConcept }] // A language the practitioner can use in patient communication }","title":"Practitioner"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#lombok","text":"Tipp: Um nicht immer Getter, Setter etc. in einer Klasse implementieren zu m\u00fcssen, kann man mit lombok (bereits in dem Projekt eingebunden) sich diese im Java-Bytecode automatisch generieren lassen.","title":"Lombok"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#beispiel-klasse","text":"@Getter @Setter @AllArgsConstructor @NoArgsConstructor @ToString class Something { int someProperty = 0; }","title":"Beispiel-Klasse"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#tipps-fur-die-implementierung","text":"Da sowohl Patient als auch Pracitioner Personen sind w\u00e4re es m\u00f6glich eine abstrakte Basisiklasse Person zu erstellen in der allgemeine Properties bereits enthalten sind. Da beide Controller ein sehr \u00e4hnliches Verhalten haben kann man dieses unabh\u00e4ngig vom Typ mittels generics implementieren. F\u00fcr je Patient als auch Pracitioner kann dann eine Ableitung davon anderen Mappings zugeordnet sein. public class PersonController<T, ...>{ public List<T> getAllEntities(){ ... } ... } @RequestMapping(\"/patient\") public class PatientController { PersonController<Patient> baseController; @GetMapping(\"/\") public List<Patient> getAllEntities() { return baseController.getAllEntities(); } ... }","title":"Tipps f\u00fcr die Implementierung"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#daten-miteinander-logisch-verknupfen","text":"M\u00f6chte man zwei Tabellen in SQL Miteinander verkn\u00fcpfen ist dies \u00fcber einen foreign key constraint mit anschliessendem JOIN m\u00f6glich. Spring (insbesondere die JPA) weis aber nicht wie die Entit\u00e4ten (Klassen mit @Entitiy -Annotation) zusammengeh\u00f6ren, es sei den man gibt ihr die richtigen Annotationen. Hat man also in SQL 2 miteinander verbundende Tabellen: create table Practitioner( id integer PRIMARY KEY fullName text ) create table Patient( id integer PRIMARY KEY fullName text, attendingPractitioner integer references Practitioner(id) ) w\u00e4re das \u00e4quivalent in Java dazu @Entity @Getter @Setter @Table(name = \"Patient\") class Patient{ @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"pat_id\", nullable = false) Long pNr; @Column(name = \"fullName\") String fullName; // join column ist nicht unbedingt notwendig, hier genauer erkl\u00e4rt // -> https://stackoverflow.com/a/37542849/17996814 @JoinColumn(name=\"pract_id\", nullable = false) @OneToOne Practitioner attendingPractitioner; } @Entity @Getter @Setter @Table(name = \"Practitioner\") class Practitioner{ @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"pract_id\", nullable = false) Long pNr; @Column(name = \"fullName\") String fullName; }","title":"Daten miteinander logisch verkn\u00fcpfen"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#fhir","text":"FHIR steht f\u00fcr Fast Healthcare Interoperability Resources . Und ist ein von HL7 v er\u00f6ffentlichter Standard um Daten im Gesundheitsbereich Programm\u00fcbergreifend austauschen zu k\u00f6nnen. HL7 ist eine Organisation welche f\u00fcr die ANSI in den USA Standards im Gesundheitswesen entwickelt. Durch diesen Standard k\u00f6nnen alle m\u00f6glichen Gesundheitsprogrammen Daten untereinander austauschen.","title":"FHIR"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#tests","text":"Implementiere deinen eigenen Test der die Daten auf FHIR-Kompatibilit\u00e4t pr\u00fcft. Du kannst pr\u00fcfen, ob die von dir generierten JSON-Daten f\u00fcr einen Patienten passen, indem du sie mit der Struktur von hier vergleichst. Testdaten im JSON-Format gibt es f\u00fcr den Patienten und den Practitioner bereits von FHIR. Vergleiche mittels Soll- und Ist-Wert (Expected and Actual Value) ob die von Spring generierten Daten den FHIR-Referenzdaten gleichen. Mehr dazu im folgenden Beispiel:","title":"Tests"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#beispiel-test","text":"@Test public void testCompareReturnedPatientJSONtoFHIRCompliantJSON() throws Exception { mockMvc .perform(get(\"/patient/1\")) // get patient with id 1 .andExpect(status().isOk()) // expect 200 HTTP status code .andExpect(content().json('{\"your_patient\": \"test_data\"}')); // returned data should be of type json and // contain the same parameters as the test data } Die initialen Testdaten sollen in der Ressourcendatei import.sql direkt als SQL-Statements eingef\u00fcgt werden. Beim Start deines Programmes werden alle SQL-Befehle in dieser Datei ausgef\u00fchrt.","title":"Beispiel-Test"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#generierung-von-sql-daten","text":"Um zu testen ob deine SQL-Statements funktionieren kannst du sie bevor du sie in import.sql einf\u00fcgst auch manuell ausf\u00fchren. Starte dazu den Webserfer und gehe dann auf die Weboberfl\u00e4che der H2-Datenbank. Diese erreichst du unter: http://localhost:8080/h2-console Wenn dein Befehl ohne Fehler ausgef\u00fchrt wird, kannst du ihn bedenkenlos einf\u00fcgen. Beachte aber, dass im Ausgangszustand dieses Projekts noch keine Entit\u00e4ten angelegt wurden. Es ist also noch kein CREATE TABLE -Statement ausgef\u00fchrt worden.","title":"Generierung von SQL-Daten"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#api-designrichtlinien","text":"Bitte beachte bei den HTTP-Requestmethoden den dazugeh\u00f6rigen RFC 7231 Sektion 4.3 Kurz zusammengefasst, wann man was nimmt: GET Wenn man eine Ressource holt POST Wenn man eine nicht identifizierte Ressource hochl\u00e4dt oder \u00e4ndert PUT Wenn man eine identifizierte Ressource hochl\u00e4dt PATCH Wenn man eine identifizierte Ressource teilweise \u00e4ndert. Hat einen extra RFC, siehe: RFC 5789 DELETE Wenn man eine Ressource l\u00f6scht Auch gibt es einen sch\u00f6nen Blogartikel von Stackoverflow in dem gutes API-Design erkl\u00e4rt wird. Blogartikel: Best practises for REST API Design","title":"API-Designrichtlinien"},{"location":"Assignments/4XHBGM/MIS/SpringFHIRKRM/#controller","text":"Die Controller sollen es erm\u00f6glichen die Daten mit GET auszulesen. Mit PUT hinzuzuf\u00fcgen. Mit DELETE zu l\u00f6schen und mit PATCH zu ver\u00e4ndern. Also alle CRUD -Methoden (Create, Read, Update, Delete). Microsoft hat das hier etwas genauer erkl\u00e4rt. Jedoch mit anderen Request-Methoden.","title":"Controller"},{"location":"Assignments/4XHBGM/MIS/greetingAssignment/","text":"Implement a basic request-response mechanism in Spring to communicate via HTTP. --> request: GET /hello/{name} <-- response: HTTP-Statuscode: 200 Body: Hello, {name} More infos on how to implement this here: https://spring.io/guides/gs/spring-boot/ The {name} part is a path Variable. You can read it's value by utilizing the @PathVariable -Annotation. Find out more here: https://www.baeldung.com/spring-pathvariable","title":"Greeting Assignment"},{"location":"Assignments/4XHBGM/WMC/projekt/","text":"WMC Projekt Nachdem ihr in POS ein Backend implementiert habt, wird dieses nun in ein Frontend in WMC integriert. Mit React+Remix haben wir schon eine Webapp erstellt. Ein f\u00fcr mobile Umgebungen optimiertes Framework ist Ionic . In diesem kann man ebenfalls React verwenden (oder Angular bzw. Vue). Erstellt in Ionic eine App welche alle eure Endpunkte ansteuert. Dabei soll mindesten ein Business-Case (Anwendungsfall) durch das Ansteuern der Endpunkte abgedeckt werden. Z.b Zimmer buchen. Buildet die App mit Web als Target. Zus\u00e4tzlich auch mit Android oder iOS als Target. Erstelle jede WMC/POS-Einheit mindestens einen Commit. Je mehr desto besser. Vorbereitung Tutorial: Your first App Abgabe Dokumentiere in einem Protokoll deine Vorgehensweise beim implementieren Dokumentiere die Vorgehensweise beim Build-Prozess Stelle jedes Feature vor Benutze falls Anwendbar Native Ionic Features wie die Kamera, Bluetooth etc. Gib im Protokoll den Link zu deinem Repository an","title":"WMC Projekt"},{"location":"Assignments/4XHBGM/WMC/projekt/#wmc-projekt","text":"Nachdem ihr in POS ein Backend implementiert habt, wird dieses nun in ein Frontend in WMC integriert. Mit React+Remix haben wir schon eine Webapp erstellt. Ein f\u00fcr mobile Umgebungen optimiertes Framework ist Ionic . In diesem kann man ebenfalls React verwenden (oder Angular bzw. Vue). Erstellt in Ionic eine App welche alle eure Endpunkte ansteuert. Dabei soll mindesten ein Business-Case (Anwendungsfall) durch das Ansteuern der Endpunkte abgedeckt werden. Z.b Zimmer buchen. Buildet die App mit Web als Target. Zus\u00e4tzlich auch mit Android oder iOS als Target. Erstelle jede WMC/POS-Einheit mindestens einen Commit. Je mehr desto besser.","title":"WMC Projekt"},{"location":"Assignments/4XHBGM/WMC/projekt/#vorbereitung","text":"Tutorial: Your first App","title":"Vorbereitung"},{"location":"Assignments/4XHBGM/WMC/projekt/#abgabe","text":"Dokumentiere in einem Protokoll deine Vorgehensweise beim implementieren Dokumentiere die Vorgehensweise beim Build-Prozess Stelle jedes Feature vor Benutze falls Anwendbar Native Ionic Features wie die Kamera, Bluetooth etc. Gib im Protokoll den Link zu deinem Repository an","title":"Abgabe"},{"location":"Assignments/5XHBGM/Index/","text":"MIS Angular+nginx Container Spring & MySQL Container CI/CD einrichten Deployment mit Azure VM Terraform Graylog MGIN \u00dcbung 1 \u00dcbung 2 \u00dcbung 3 \u00dcbung 4","title":"\u00dcbungen 5XHBGM"},{"location":"Assignments/5XHBGM/Index/#mis","text":"Angular+nginx Container Spring & MySQL Container CI/CD einrichten Deployment mit Azure VM Terraform Graylog","title":"MIS"},{"location":"Assignments/5XHBGM/Index/#mgin","text":"\u00dcbung 1 \u00dcbung 2 \u00dcbung 3 \u00dcbung 4","title":"MGIN"},{"location":"Assignments/5XHBGM/CICDDeployment/","text":"Deployment Ziel dieser Aufgabe ist des das CRUD-Assignment in eine einizge Docker-Compose Datei zu packen um diese von \u00fcberall aus ausf\u00fchrbar zu machen (deployment). Damit w\u00e4re es im weiteren auch m\u00f6glich diese Datei auf einer Azure-Instanz o.\u00e4 hochzuladen und von dort aus auszuf\u00fchren. Auch Interessant ist diese Art des Deployments f\u00fcr IT-Abteilungen von Firmen mit denen man zusammenarbeitet. Die freuen sich meistens wenn man Ihnen durch so etwas die Arbeit etwas abnimmt und man ist bei (kritischen) Updates nicht auf sie angewiesen, vorausgesetzt es wird immerwieder die neueste Version aus dem Registry gepullt ( docker pull <imagename> ). Dazu braucht es folgende Voraussetzungen: - Dockerfiles - Images - Container-Registry - CI/CD Pipelines - Je ein Workflow f\u00fcr Frontend und Backend - Da Frontend und Backend \u00fcblicherweise in 2 Repos aufgeteilt ist kann man einfach in jeden Repo einen neuen Workflow hinzuf\u00fcgen (bereits exisiterende Repos k\u00f6nnen ggf. weiterverwendet werden) Das Dockerfile f\u00fcrs Frontend k\u00f6nnte z.B so aussehen: # Use a Node.js base image FROM node:16-alpine AS builder # Set the working directory to /app WORKDIR /app # Copy package.json and package-lock.json files to the container COPY package*.json ./ # Install dependencies via ci RUN npm ci # Copy the rest of the application code to the container COPY . . # Build the application and output the files to /dist RUN npm run build --prod # Use a Nginx base image for the final container FROM nginx:1.23-alpine # Copy the /dist directory from the builder container to the final container COPY --from=builder /app/dist/angular-request-assignment /usr/share/nginx/html # Expose port 80 for the web server EXPOSE 80 # Start the Nginx web server ENTRYPOINT [\"nginx\", \"-g\", \"daemon off;\"] F\u00fcge (falls nicht schon vorhanden) sowohl im Frontend als auch im Backend ein Dockerfile hinzu. Im \u00fcbergeordneten Ordner kann sobald beide Dateien Images builden ein docker-compose mit folgender Struktur angelegt werden: ```yml version: \"xy\" services: frontend: build: ./frontend backend: build: ./backend ``` Wenn das funktioniert ist schon ein guter Teil erledigt, jedoch setzt dieses Dockerfile voraus das der Programmcode in den Unterordnern vorliegt. F\u00fcr einen Entwickler ist das kein Problem, aber wenn man das Programm dann beim Auftraggeber installieren m\u00f6chte, m\u00f6chte man dem im Normalfall nicht den Code geben. Deswegen ist es besser wenn man die Images in einer Container-Registry hochl\u00e4dt um von \u00fcberall aus auf diese zugreifen zu k\u00f6nnen. Dann schaut das Compose ca. so aus: version: '3' services: frontend: image: ghcr.io/<your-username>/<your-reponame>:latest ports: - \"80:80\" Um dieses Repository nutzen zu k\u00f6nnen muss man sich jedoch mit Docker bei ghcr (Github Container Registry) anmelden. Das funktioniert folgenderma\u00dfen: - Lege dir einen Personal Access Token an, dieser Braucht nur die Berechtigung read_package Siehe working with container registries und create a personal access token Wenn du einen PAT hast kannst du dich mit dem Befehl docker login anmelden Username: Dein Github-Username Passwort: Dein PAT (Personal Access Token) Aufbau des Image-Feldes ghcr.io Github container registry Dein Username auf Github Der Name des Images der beim CI/CD wird meistens \u00fcber einen Tag in Git festgelegt Zum Vergleich: Ein Image weit verbreiteter Software sieht oft so aus mysql . Dabei wurden f\u00fcr einige der obigen Felder Standardannahmen getroffen. In voller L\u00e4nge w\u00fcrde es so aussehen: docker.io/libraries/mysql:latest . Alles bis auf mysql sind Standardparameter. Siehe: Official Docker Images Einrichten der CI/CD Die Containerimages kann man entweder Lokal builden und ins Registry hochladen oder sich auf den Github-Servern bauen lassen. Bei den Github-Servern ist der Vorteil dass man sicherstellen kann das vor dem Buildprozess Unittests, Sicherheitschecks etc. durchgef\u00fchrt werden. Um die Github Server anzuweisen ist folgende Ordnerstruktur im Repository zu erstellen: .github/workflows/<docker_build_actionname.yml> Die Datei k\u00f6nnte ca. so aussehen: name: Build and Push Docker Image on: # trigger execution on every push push: branches: [ main ] # trigger execution on every tag #tag: jobs: build-and-push: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to GitHub Container Registry uses: docker/login-action@v1 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Build and push Docker image env: DOCKER_BUILDKIT: 1 run: | docker buildx build \\ --progress plain \\ --tag ghcr.io/<your-username/orgname>/<your_reponame>:latest (or ${{github.tag}} when pushed on tag ) \\ --push . Einf\u00fchrung in Container Registries in Github Bewertungsgrundlagen bei der Abgabe F\u00fcge jene docker-compose.yml Datei in das Repository der aktuellen Abgabe hinzu welche auf die von dir erstellten Images verweist. Abzugeben ist : - eine Docker-Compose Datei mit Images - ggf. Logindaten um Zugriff auf die Lokalen Container zu erhalten - NICHT das eigene Passwort sondern nur einen Zugangstoken f\u00fcr die Registries Punkteverteilung - Nach einem Docker-Login von meiner Seite mit eurem Token kann ich die Images pullen 33P - Die Images k\u00f6nnen ausgef\u00fchrt werden ohne sofort abzust\u00fcrzen 33P - Bei den Tags gibt es sowohl eine aktuelle Version vX.Y als auch einen latest -Tag f\u00fcr die neueste Version 25P - Beim Build eines neuen Images gibt es eine Schritt (Step) f\u00fcr das Ausf\u00fchren von Unittests 33P Weitere Unterlagen Docker Images manuell pushen Container Registries in Gitlab","title":"Deployment"},{"location":"Assignments/5XHBGM/CICDDeployment/#deployment","text":"Ziel dieser Aufgabe ist des das CRUD-Assignment in eine einizge Docker-Compose Datei zu packen um diese von \u00fcberall aus ausf\u00fchrbar zu machen (deployment). Damit w\u00e4re es im weiteren auch m\u00f6glich diese Datei auf einer Azure-Instanz o.\u00e4 hochzuladen und von dort aus auszuf\u00fchren. Auch Interessant ist diese Art des Deployments f\u00fcr IT-Abteilungen von Firmen mit denen man zusammenarbeitet. Die freuen sich meistens wenn man Ihnen durch so etwas die Arbeit etwas abnimmt und man ist bei (kritischen) Updates nicht auf sie angewiesen, vorausgesetzt es wird immerwieder die neueste Version aus dem Registry gepullt ( docker pull <imagename> ). Dazu braucht es folgende Voraussetzungen: - Dockerfiles - Images - Container-Registry - CI/CD Pipelines - Je ein Workflow f\u00fcr Frontend und Backend - Da Frontend und Backend \u00fcblicherweise in 2 Repos aufgeteilt ist kann man einfach in jeden Repo einen neuen Workflow hinzuf\u00fcgen (bereits exisiterende Repos k\u00f6nnen ggf. weiterverwendet werden) Das Dockerfile f\u00fcrs Frontend k\u00f6nnte z.B so aussehen: # Use a Node.js base image FROM node:16-alpine AS builder # Set the working directory to /app WORKDIR /app # Copy package.json and package-lock.json files to the container COPY package*.json ./ # Install dependencies via ci RUN npm ci # Copy the rest of the application code to the container COPY . . # Build the application and output the files to /dist RUN npm run build --prod # Use a Nginx base image for the final container FROM nginx:1.23-alpine # Copy the /dist directory from the builder container to the final container COPY --from=builder /app/dist/angular-request-assignment /usr/share/nginx/html # Expose port 80 for the web server EXPOSE 80 # Start the Nginx web server ENTRYPOINT [\"nginx\", \"-g\", \"daemon off;\"] F\u00fcge (falls nicht schon vorhanden) sowohl im Frontend als auch im Backend ein Dockerfile hinzu. Im \u00fcbergeordneten Ordner kann sobald beide Dateien Images builden ein docker-compose mit folgender Struktur angelegt werden: ```yml version: \"xy\" services: frontend: build: ./frontend backend: build: ./backend ``` Wenn das funktioniert ist schon ein guter Teil erledigt, jedoch setzt dieses Dockerfile voraus das der Programmcode in den Unterordnern vorliegt. F\u00fcr einen Entwickler ist das kein Problem, aber wenn man das Programm dann beim Auftraggeber installieren m\u00f6chte, m\u00f6chte man dem im Normalfall nicht den Code geben. Deswegen ist es besser wenn man die Images in einer Container-Registry hochl\u00e4dt um von \u00fcberall aus auf diese zugreifen zu k\u00f6nnen. Dann schaut das Compose ca. so aus: version: '3' services: frontend: image: ghcr.io/<your-username>/<your-reponame>:latest ports: - \"80:80\" Um dieses Repository nutzen zu k\u00f6nnen muss man sich jedoch mit Docker bei ghcr (Github Container Registry) anmelden. Das funktioniert folgenderma\u00dfen: - Lege dir einen Personal Access Token an, dieser Braucht nur die Berechtigung read_package Siehe working with container registries und create a personal access token Wenn du einen PAT hast kannst du dich mit dem Befehl docker login anmelden Username: Dein Github-Username Passwort: Dein PAT (Personal Access Token)","title":"Deployment"},{"location":"Assignments/5XHBGM/CICDDeployment/#aufbau-des-image-feldes","text":"ghcr.io Github container registry Dein Username auf Github Der Name des Images der beim CI/CD wird meistens \u00fcber einen Tag in Git festgelegt Zum Vergleich: Ein Image weit verbreiteter Software sieht oft so aus mysql . Dabei wurden f\u00fcr einige der obigen Felder Standardannahmen getroffen. In voller L\u00e4nge w\u00fcrde es so aussehen: docker.io/libraries/mysql:latest . Alles bis auf mysql sind Standardparameter. Siehe: Official Docker Images","title":"Aufbau des Image-Feldes"},{"location":"Assignments/5XHBGM/CICDDeployment/#einrichten-der-cicd","text":"Die Containerimages kann man entweder Lokal builden und ins Registry hochladen oder sich auf den Github-Servern bauen lassen. Bei den Github-Servern ist der Vorteil dass man sicherstellen kann das vor dem Buildprozess Unittests, Sicherheitschecks etc. durchgef\u00fchrt werden. Um die Github Server anzuweisen ist folgende Ordnerstruktur im Repository zu erstellen: .github/workflows/<docker_build_actionname.yml> Die Datei k\u00f6nnte ca. so aussehen: name: Build and Push Docker Image on: # trigger execution on every push push: branches: [ main ] # trigger execution on every tag #tag: jobs: build-and-push: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to GitHub Container Registry uses: docker/login-action@v1 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Build and push Docker image env: DOCKER_BUILDKIT: 1 run: | docker buildx build \\ --progress plain \\ --tag ghcr.io/<your-username/orgname>/<your_reponame>:latest (or ${{github.tag}} when pushed on tag ) \\ --push . Einf\u00fchrung in Container Registries in Github","title":"Einrichten der CI/CD"},{"location":"Assignments/5XHBGM/CICDDeployment/#bewertungsgrundlagen-bei-der-abgabe","text":"F\u00fcge jene docker-compose.yml Datei in das Repository der aktuellen Abgabe hinzu welche auf die von dir erstellten Images verweist. Abzugeben ist : - eine Docker-Compose Datei mit Images - ggf. Logindaten um Zugriff auf die Lokalen Container zu erhalten - NICHT das eigene Passwort sondern nur einen Zugangstoken f\u00fcr die Registries Punkteverteilung - Nach einem Docker-Login von meiner Seite mit eurem Token kann ich die Images pullen 33P - Die Images k\u00f6nnen ausgef\u00fchrt werden ohne sofort abzust\u00fcrzen 33P - Bei den Tags gibt es sowohl eine aktuelle Version vX.Y als auch einen latest -Tag f\u00fcr die neueste Version 25P - Beim Build eines neuen Images gibt es eine Schritt (Step) f\u00fcr das Ausf\u00fchren von Unittests 33P","title":"Bewertungsgrundlagen bei der Abgabe"},{"location":"Assignments/5XHBGM/CICDDeployment/#weitere-unterlagen","text":"Docker Images manuell pushen Container Registries in Gitlab","title":"Weitere Unterlagen"},{"location":"Assignments/5XHBGM/DockerRegistry/","text":"Registries Haus\u00fcbung Ver\u00f6ffentliche das Backend als Container und pushe es in ein Registry. Verwende dann dein Backend in einer docker-compose.yml.","title":"Registries Haus\u00fcbung"},{"location":"Assignments/5XHBGM/DockerRegistry/#registries-hausubung","text":"Ver\u00f6ffentliche das Backend als Container und pushe es in ein Registry. Verwende dann dein Backend in einer docker-compose.yml.","title":"Registries Haus\u00fcbung"},{"location":"Assignments/5XHBGM/Graylog/","text":"Mit folgender compose-konfiguration kann man graylog in sein System einbinden: version: '3' services: # MongoDB: Required for Graylog mongo: image: mongo:7.0.12 volumes: - mongo_data:/data/db logging: driver: gelf options: gelf-address: \"udp://localhost:12201\" tag: \"mongo\" # Elasticsearch: Required for Graylog elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2 environment: - http.host=0.0.0.0 - transport.host=localhost - network.host=0.0.0.0 - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" volumes: - es_data:/usr/share/elasticsearch/data logging: driver: gelf options: gelf-address: \"udp://localhost:12201\" tag: \"elasticsearch\" ports: - 9200:9200 # not necessary to expose # Graylog graylog: image: graylog/graylog:6.0 environment: - GRAYLOG_PASSWORD_SECRET=somepasswordpepper - GRAYLOG_ROOT_PASSWORD_SHA2=8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918 # admin when unhashed - GRAYLOG_HTTP_EXTERNAL_URI=http://127.0.0.1:9000/ - GRAYLOG_ELASTICSEARCH_HOSTS=http://elasticsearch:9200 # needs to match elastichsearch hostname links: - mongo - elasticsearch depends_on: - mongo - elasticsearch ports: - 9000:9000 # Graylog web interface and REST API - 12201:12201/udp # GELF UDP - 1514:1514 # Syslog TCP volumes: - graylog_data:/usr/share/graylog/data # Example of another service in the compose setup example_service: image: nginx:latest ports: - 80:80 logging: driver: gelf options: gelf-address: \"udp://localhost:12201\" tag: \"example_service\" volumes: mongo_data: es_data: graylog_data: Hier werden die Logs vom example_service , einer nginx -Instanz aufgezeichnet und gespeichert. Dozzle, im Gegensatz zu Graylog, speichert keine Logs. In Graylog und in compose werden die (beinahe) gleichen Logs angezeigt. Input definieren Damit Graylog wei\u00df wo es seine logs herbekommen kann, m\u00fcssen entsprechende Inputs definiert werden: Die Input-Definition muss mit der Logging-Treiber Konfiguration in compose \u00fcbereinstimmen: logging: driver: gelf options: gelf-address: \"udp://localhost:12201\" tag: \"example_service\" Hier wurde ein GELF-Treiber verwendet, welcher \u00fcber UDP mit Graylog kommuniziert. Aufgabe F\u00fcge zu einem Spring-Projekt Graylog, Elasticsearch und MongoDB hinzu, um somit immer auf die Logs deiner Anwendung zugreifen zu k\u00f6nnen.","title":"Graylog zu bestehendem Deployment hinzuf\u00fcgen"},{"location":"Assignments/5XHBGM/Graylog/#input-definieren","text":"Damit Graylog wei\u00df wo es seine logs herbekommen kann, m\u00fcssen entsprechende Inputs definiert werden: Die Input-Definition muss mit der Logging-Treiber Konfiguration in compose \u00fcbereinstimmen: logging: driver: gelf options: gelf-address: \"udp://localhost:12201\" tag: \"example_service\" Hier wurde ein GELF-Treiber verwendet, welcher \u00fcber UDP mit Graylog kommuniziert.","title":"Input definieren"},{"location":"Assignments/5XHBGM/Graylog/#aufgabe","text":"F\u00fcge zu einem Spring-Projekt Graylog, Elasticsearch und MongoDB hinzu, um somit immer auf die Logs deiner Anwendung zugreifen zu k\u00f6nnen.","title":"Aufgabe"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/","text":"Zielsetzung In dieser \u00dcbung werdet ihr die Grundlagen f\u00fcr eine Django-Webseite erstellen, die sp\u00e4ter als Basis f\u00fcr verschiedene bioinformatische Anwendungen dienen wird. Der Fokus liegt zun\u00e4chst auf dem Aufbau der Django-Struktur und der Implementierung grundlegender Funktionen wie Formulareingaben und Datei-Uploads. Aufgabenstellung 1. Django-Projekt einrichten Erstelle ein neues Django-Projekt mit dem Namen \"BioinformaticsToolbox\". Erstelle innerhalb des Projekts eine neue App mit dem Namen \"sequencetools\". 2. Modelle erstellen Erstelle in der models.py Datei ein Modell namens SequenceSubmission mit folgenden Feldern: text_input : Ein TextField f\u00fcr Texteingaben file_upload : Ein FileField f\u00fcr Datei-Uploads submission_date : Ein DateTimeField, das automatisch den Zeitpunkt des Uploads speichert 3. Formulare implementieren Erstelle in einer neuen Datei forms.py ein Django-Formular, das die Eingabe von Text und das Hochladen von Dateien erm\u00f6glicht. 4. Views erstellen Implementiere in views.py zwei Views: submit_sequence : Verarbeitet das Formular und speichert die Eingaben in der Datenbank. view_submissions : Zeigt alle bisher gemachten Eingaben an. 5. Templates gestalten Erstelle zwei HTML-Templates: submit.html : Enth\u00e4lt das Formular zur Eingabe und zum Datei-Upload. submissions.html : Zeigt alle bisherigen Eingaben tabellarisch an. 6. URLs konfigurieren F\u00fcge in der urls.py Datei die notwendigen URL-Patterns hinzu, um die Views mit den entsprechenden URLs zu verkn\u00fcpfen. 7. Gestaltung und Benutzerfreundlichkeit Verwende Bootstrap, Tailwind oder ein anderes CSS-Framework, um die Webseite ansprechend zu gestalten. Implementiere eine einfache Navigation zwischen den Seiten. 8. Testen Teste deine Anwendung gr\u00fcndlich: K\u00f6nnen Texteingaben gemacht werden? Funktioniert der Datei-Upload? Werden die Eingaben korrekt in der Datenbank gespeichert? Werden alle Eingaben auf der \u00dcbersichtsseite angezeigt? Zusatzaufgaben (optional) Implementiere eine einfache Validierung f\u00fcr die Texteingabe (z.B. nur DNA-Sequenzen erlauben). F\u00fcge eine Funktion hinzu, die die L\u00e4nge der eingegebenen Sequenz berechnet und anzeigt. Abgabe Dokumentiere in einem Protokoll deine Vorgehensweise Insbesondere die Tests aus Punkt 8 m\u00fcssen durchgef\u00fchrt werden und mit Screenshots demonstriert werden dass alles funktioniert Verwende folgende Protokollvorlage: Protokoll Vorlage.docx Weitere Infos Django Installationsanleitung Erstes Django Projekt Youtube Tutorial f\u00fcr Django in Kombination mit SQLite File Uploads","title":"Erstellung einer Django-Webseite f\u00fcr Bioinformatik-Anwendungen"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#zielsetzung","text":"In dieser \u00dcbung werdet ihr die Grundlagen f\u00fcr eine Django-Webseite erstellen, die sp\u00e4ter als Basis f\u00fcr verschiedene bioinformatische Anwendungen dienen wird. Der Fokus liegt zun\u00e4chst auf dem Aufbau der Django-Struktur und der Implementierung grundlegender Funktionen wie Formulareingaben und Datei-Uploads.","title":"Zielsetzung"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#aufgabenstellung","text":"","title":"Aufgabenstellung"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#1-django-projekt-einrichten","text":"Erstelle ein neues Django-Projekt mit dem Namen \"BioinformaticsToolbox\". Erstelle innerhalb des Projekts eine neue App mit dem Namen \"sequencetools\".","title":"1. Django-Projekt einrichten"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#2-modelle-erstellen","text":"Erstelle in der models.py Datei ein Modell namens SequenceSubmission mit folgenden Feldern: text_input : Ein TextField f\u00fcr Texteingaben file_upload : Ein FileField f\u00fcr Datei-Uploads submission_date : Ein DateTimeField, das automatisch den Zeitpunkt des Uploads speichert","title":"2. Modelle erstellen"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#3-formulare-implementieren","text":"Erstelle in einer neuen Datei forms.py ein Django-Formular, das die Eingabe von Text und das Hochladen von Dateien erm\u00f6glicht.","title":"3. Formulare implementieren"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#4-views-erstellen","text":"Implementiere in views.py zwei Views: submit_sequence : Verarbeitet das Formular und speichert die Eingaben in der Datenbank. view_submissions : Zeigt alle bisher gemachten Eingaben an.","title":"4. Views erstellen"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#5-templates-gestalten","text":"Erstelle zwei HTML-Templates: submit.html : Enth\u00e4lt das Formular zur Eingabe und zum Datei-Upload. submissions.html : Zeigt alle bisherigen Eingaben tabellarisch an.","title":"5. Templates gestalten"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#6-urls-konfigurieren","text":"F\u00fcge in der urls.py Datei die notwendigen URL-Patterns hinzu, um die Views mit den entsprechenden URLs zu verkn\u00fcpfen.","title":"6. URLs konfigurieren"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#7-gestaltung-und-benutzerfreundlichkeit","text":"Verwende Bootstrap, Tailwind oder ein anderes CSS-Framework, um die Webseite ansprechend zu gestalten. Implementiere eine einfache Navigation zwischen den Seiten.","title":"7. Gestaltung und Benutzerfreundlichkeit"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#8-testen","text":"Teste deine Anwendung gr\u00fcndlich: K\u00f6nnen Texteingaben gemacht werden? Funktioniert der Datei-Upload? Werden die Eingaben korrekt in der Datenbank gespeichert? Werden alle Eingaben auf der \u00dcbersichtsseite angezeigt?","title":"8. Testen"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#zusatzaufgaben-optional","text":"Implementiere eine einfache Validierung f\u00fcr die Texteingabe (z.B. nur DNA-Sequenzen erlauben). F\u00fcge eine Funktion hinzu, die die L\u00e4nge der eingegebenen Sequenz berechnet und anzeigt.","title":"Zusatzaufgaben (optional)"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#abgabe","text":"Dokumentiere in einem Protokoll deine Vorgehensweise Insbesondere die Tests aus Punkt 8 m\u00fcssen durchgef\u00fchrt werden und mit Screenshots demonstriert werden dass alles funktioniert Verwende folgende Protokollvorlage: Protokoll Vorlage.docx","title":"Abgabe"},{"location":"Assignments/5XHBGM/MGIN/Uebung1/#weitere-infos","text":"Django Installationsanleitung Erstes Django Projekt Youtube Tutorial f\u00fcr Django in Kombination mit SQLite File Uploads","title":"Weitere Infos"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/","text":"Einf\u00fchrung In dieser \u00dcbung wirst du deine Django-Webseite um eine ORF (Open Reading Frame) Finder-Funktionalit\u00e4t erweitern. Zus\u00e4tzlich wirst du einen separaten Docker-Container erstellen, der die Verarbeitung der ORF-Suche \u00fcbernimmt. Dies simuliert ein verteiltes System, bei dem rechenintensive Aufgaben ausgelagert werden. Wie man es oft in der Bioinformatik braucht. Den Container k\u00f6nnte man ja im sp\u00e4teren Verlauf in einem Rechenzentrum starten z.B. Aufgabenstellung Teil 1: Erweiterung der Django-Webseite Erweitere deine bestehende Django-Webseite um ein neues Formular f\u00fcr die ORF-Suche. Das Formular soll folgende Eingabefelder haben: Textfeld f\u00fcr die DNA-Sequenz Numerisches Feld f\u00fcr die minimale ORF-L\u00e4nge Checkbox f\u00fcr die Suche auf beiden DNA-Str\u00e4ngen (vorw\u00e4rts und r\u00fcckw\u00e4rts) Erstelle eine neue View, die die Formulardaten entgegennimmt und an den Docker-Container sendet. Implementiere eine Ergebnisseite, die die gefundenen ORFs anzeigt. Dabei kannst du dich am ORF Finder von NCBI orientieren. Teil 2: Erstellung des Docker-Containers f\u00fcr die ORF-Suche Erstelle einen neuen Docker-Container, der Python und die notwendigen Bibliotheken f\u00fcr die ORF-Suche enth\u00e4lt. Implementiere einen einfachen Web-Server (z.B. mit fastAPI, Flask, pure Python etc.) im Container, der POST-Anfragen mit DNA-Sequenzen entgegennimmt. Schreibe eine Python-Funktion zur ORF-Suche, die folgende Schritte durchf\u00fchrt: Identifizierung von Start- (ATG) und Stopp-Codons (TAA, TAG, TGA) Suche nach ORFs mit der angegebenen Mindestl\u00e4nge Optional: Suche auf beiden DNA-Str\u00e4ngen Der Container soll die gefundenen ORFs im JSON-Format zur\u00fcckgeben. Teil 3: Integration und Kommunikation Konfiguriere die Django-Anwendung so, dass sie mit dem Docker-Container kommunizieren kann. Implementiere die Daten\u00fcbertragung zwischen Django und dem Docker-Container mittels HTTP-Requests. Verarbeite die JSON-Antwort des Containers in Django und stelle die Ergebnisse auf der Webseite dar. Teil 4: Protokollierung Dokumentiere Deinen Code ausf\u00fchrlich. Schreibe kurz \u00fcber die Vor- und Nachteile diese Verteilten ansatzes. Teste die Funktionalit\u00e4t mit bestehenden ORF-Findern und vergleiche die Ergebnisse - ORF Finder von NCBI Verwende folgende Protokollvorlage: Protokoll Vorlage.docx Bonusaufgaben F\u00fcge eine Option hinzu, um die gefundenen ORFs als Datei herunterzuladen. Bewertungskriterien Funktionalit\u00e4t der Django-Webseite und des Docker-Containers Korrektheit der ORF-Suche Code-Qualit\u00e4t und Dokumentation Verst\u00e4ndnis und Umsetzung des verteilten Systemkonzepts Benutzerfreundlichkeit der Weboberfl\u00e4che Weitere Unterlagen Container der einen anderen startet FastAPI Tutorial","title":"\u00dcbungsaufgabe - ORF-Finder mit Django und Docker"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#einfuhrung","text":"In dieser \u00dcbung wirst du deine Django-Webseite um eine ORF (Open Reading Frame) Finder-Funktionalit\u00e4t erweitern. Zus\u00e4tzlich wirst du einen separaten Docker-Container erstellen, der die Verarbeitung der ORF-Suche \u00fcbernimmt. Dies simuliert ein verteiltes System, bei dem rechenintensive Aufgaben ausgelagert werden. Wie man es oft in der Bioinformatik braucht. Den Container k\u00f6nnte man ja im sp\u00e4teren Verlauf in einem Rechenzentrum starten z.B.","title":"Einf\u00fchrung"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#aufgabenstellung","text":"","title":"Aufgabenstellung"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#teil-1-erweiterung-der-django-webseite","text":"Erweitere deine bestehende Django-Webseite um ein neues Formular f\u00fcr die ORF-Suche. Das Formular soll folgende Eingabefelder haben: Textfeld f\u00fcr die DNA-Sequenz Numerisches Feld f\u00fcr die minimale ORF-L\u00e4nge Checkbox f\u00fcr die Suche auf beiden DNA-Str\u00e4ngen (vorw\u00e4rts und r\u00fcckw\u00e4rts) Erstelle eine neue View, die die Formulardaten entgegennimmt und an den Docker-Container sendet. Implementiere eine Ergebnisseite, die die gefundenen ORFs anzeigt. Dabei kannst du dich am ORF Finder von NCBI orientieren.","title":"Teil 1: Erweiterung der Django-Webseite"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#teil-2-erstellung-des-docker-containers-fur-die-orf-suche","text":"Erstelle einen neuen Docker-Container, der Python und die notwendigen Bibliotheken f\u00fcr die ORF-Suche enth\u00e4lt. Implementiere einen einfachen Web-Server (z.B. mit fastAPI, Flask, pure Python etc.) im Container, der POST-Anfragen mit DNA-Sequenzen entgegennimmt. Schreibe eine Python-Funktion zur ORF-Suche, die folgende Schritte durchf\u00fchrt: Identifizierung von Start- (ATG) und Stopp-Codons (TAA, TAG, TGA) Suche nach ORFs mit der angegebenen Mindestl\u00e4nge Optional: Suche auf beiden DNA-Str\u00e4ngen Der Container soll die gefundenen ORFs im JSON-Format zur\u00fcckgeben.","title":"Teil 2: Erstellung des Docker-Containers f\u00fcr die ORF-Suche"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#teil-3-integration-und-kommunikation","text":"Konfiguriere die Django-Anwendung so, dass sie mit dem Docker-Container kommunizieren kann. Implementiere die Daten\u00fcbertragung zwischen Django und dem Docker-Container mittels HTTP-Requests. Verarbeite die JSON-Antwort des Containers in Django und stelle die Ergebnisse auf der Webseite dar.","title":"Teil 3: Integration und Kommunikation"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#teil-4-protokollierung","text":"Dokumentiere Deinen Code ausf\u00fchrlich. Schreibe kurz \u00fcber die Vor- und Nachteile diese Verteilten ansatzes. Teste die Funktionalit\u00e4t mit bestehenden ORF-Findern und vergleiche die Ergebnisse - ORF Finder von NCBI Verwende folgende Protokollvorlage: Protokoll Vorlage.docx","title":"Teil 4: Protokollierung"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#bonusaufgaben","text":"F\u00fcge eine Option hinzu, um die gefundenen ORFs als Datei herunterzuladen.","title":"Bonusaufgaben"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#bewertungskriterien","text":"Funktionalit\u00e4t der Django-Webseite und des Docker-Containers Korrektheit der ORF-Suche Code-Qualit\u00e4t und Dokumentation Verst\u00e4ndnis und Umsetzung des verteilten Systemkonzepts Benutzerfreundlichkeit der Weboberfl\u00e4che","title":"Bewertungskriterien"},{"location":"Assignments/5XHBGM/MGIN/Uebung2/#weitere-unterlagen","text":"Container der einen anderen startet FastAPI Tutorial","title":"Weitere Unterlagen"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/","text":"Zielsetzung In dieser \u00dcbung erweiterst du deine Django-Webseite um Funktionen f\u00fcr Multiple Sequence Alignment (MSA). Diese Funktionalit\u00e4t wird von einem bereits bestehenden separaten Container bereitgestellt. Realistische Testdaten kannst du dir z.B von der NCBI dbVar oder der NCBI Nucleotide DB holen. Aufgabenstellung Erweiterung der Django-Webseite F\u00fcge der bestehenden Django-Webseite ein neues Formular hinzu, das mehrere Sequenzen f\u00fcr das MSA akzeptiert Implementiere eine neue View, die die eingegebenen Sequenzen an den MSA-Service sendet F\u00fcr das Alignment kannst du z.B einen Muscle-Container verwenden Auch andere Container wie Emboss sind m\u00f6glich Hier eine Liste von Programmen die Biopython unterst\u00fctzt Integration und Kommunikation Implementiere die Kommunikation zwischen der Django-Webseite und dem MSA-Service Starte die Container direkt \u00fcber os.system (Einfach Kommandos wie 'docker run' \u00fcbergeben) oder \u00fcber das offizielle docker -Pythoninterface von der Docker Foundation Datenverarbeitung und Visualisierung Verarbeite die vom Service zur\u00fcckgegebenen Daten in Django - Erstelle eine Ansicht zur Darstellung des MSA mit einer JavaScript-Bibliothek wie MSAViewer Im letzten MSA-Viewer Release muss man eine Zeile auskommentieren, dann funktionierts wieder. Deren Beispielcode muss nur etwas abge\u00e4ndert werden Der MSAViewer von BioJS braucht eine Datei im CLUSTAL_Format Da MUSCLE nur im FASTA-Format arbeiten kann ist eine Umwandlung zwischen den Dateiformaten notwendig. Es ist auch m\u00f6glich statt dem MAS-Viewer bei muscle den Parameter -html hinzuzuf\u00fcgen. Dann wird ebenfalls eine grafische Version des Alignments generiert. Container-Orchestrierung Erstelle ein Docker-Compose-File, das sowohl den Django-Container als auch den MSA-Service-Container beinhaltet Zusatzaufgaben (optional) Implementiere verschiedene MSA-Algorithmen (z.B. ClustalW, MUSCLE) und lasse den Benutzer w\u00e4hlen Implementiere eine Funktion zum Exportieren der Ergebnisse in g\u00e4ngige Formate (FASTA, Clustal etc.) und biete die Dateien zum Download an. Abgabe Stelle sicher, dass doe Docker-Container korrekt konfiguriert und lauff\u00e4hig sind Dokumentiere die API-Endpunkte des MSA-Services. Protokolliere die Durchf\u00fchrung und die Ergebnisse Verwende folgende Protokollvorlage: Protokoll Vorlage.docx","title":"\u00dcbung Multiple Sequence Alignment"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/#zielsetzung","text":"In dieser \u00dcbung erweiterst du deine Django-Webseite um Funktionen f\u00fcr Multiple Sequence Alignment (MSA). Diese Funktionalit\u00e4t wird von einem bereits bestehenden separaten Container bereitgestellt. Realistische Testdaten kannst du dir z.B von der NCBI dbVar oder der NCBI Nucleotide DB holen.","title":"Zielsetzung"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/#aufgabenstellung","text":"","title":"Aufgabenstellung"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/#erweiterung-der-django-webseite","text":"F\u00fcge der bestehenden Django-Webseite ein neues Formular hinzu, das mehrere Sequenzen f\u00fcr das MSA akzeptiert Implementiere eine neue View, die die eingegebenen Sequenzen an den MSA-Service sendet F\u00fcr das Alignment kannst du z.B einen Muscle-Container verwenden Auch andere Container wie Emboss sind m\u00f6glich Hier eine Liste von Programmen die Biopython unterst\u00fctzt","title":"Erweiterung der Django-Webseite"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/#integration-und-kommunikation","text":"Implementiere die Kommunikation zwischen der Django-Webseite und dem MSA-Service Starte die Container direkt \u00fcber os.system (Einfach Kommandos wie 'docker run' \u00fcbergeben) oder \u00fcber das offizielle docker -Pythoninterface von der Docker Foundation","title":"Integration und Kommunikation"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/#datenverarbeitung-und-visualisierung","text":"Verarbeite die vom Service zur\u00fcckgegebenen Daten in Django - Erstelle eine Ansicht zur Darstellung des MSA mit einer JavaScript-Bibliothek wie MSAViewer Im letzten MSA-Viewer Release muss man eine Zeile auskommentieren, dann funktionierts wieder. Deren Beispielcode muss nur etwas abge\u00e4ndert werden Der MSAViewer von BioJS braucht eine Datei im CLUSTAL_Format Da MUSCLE nur im FASTA-Format arbeiten kann ist eine Umwandlung zwischen den Dateiformaten notwendig. Es ist auch m\u00f6glich statt dem MAS-Viewer bei muscle den Parameter -html hinzuzuf\u00fcgen. Dann wird ebenfalls eine grafische Version des Alignments generiert.","title":"Datenverarbeitung und Visualisierung"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/#container-orchestrierung","text":"Erstelle ein Docker-Compose-File, das sowohl den Django-Container als auch den MSA-Service-Container beinhaltet","title":"Container-Orchestrierung"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/#zusatzaufgaben-optional","text":"Implementiere verschiedene MSA-Algorithmen (z.B. ClustalW, MUSCLE) und lasse den Benutzer w\u00e4hlen Implementiere eine Funktion zum Exportieren der Ergebnisse in g\u00e4ngige Formate (FASTA, Clustal etc.) und biete die Dateien zum Download an.","title":"Zusatzaufgaben (optional)"},{"location":"Assignments/5XHBGM/MGIN/Uebung3/#abgabe","text":"Stelle sicher, dass doe Docker-Container korrekt konfiguriert und lauff\u00e4hig sind Dokumentiere die API-Endpunkte des MSA-Services. Protokolliere die Durchf\u00fchrung und die Ergebnisse Verwende folgende Protokollvorlage: Protokoll Vorlage.docx","title":"Abgabe"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/","text":"Zielsetzung In dieser \u00dcbung erweiterst du deine Django-Webseite um Funktionen zur Erstellung eines phylogenetischen Baums. Diese Funktionalit\u00e4t wird von einem bereits bestehenden separaten Container bereitgestellt. Realistische Testdaten kannst du dir z.B von der NCBI dbVar oder der NCBI Nucleotide DB holen. Aufgabenstellung Erweiterung der Django-Webseite F\u00fcge der bestehenden Django-Webseite ein neues Formular hinzu, das mehrere Sequenzen f\u00fcr die phylogenetische Analyse akzeptiert Implementiere eine neue View, die die eingegebenen Sequenzen an den Phylogenie-Service sendet Implementierung des Phylogenie-Services Verwende einen bestehenden Microservice f\u00fcr den Phylogenie-Service M\u00f6glich w\u00e4re z.B der raxml -Container F\u00fcr DNA-Sequenzen wird die Methode GTRGAMMA ben\u00f6tigt Integration und Kommunikation Implementiere die Kommunikation zwischen der Django-Webseite und dem Phylogenie-Service Starte die Container direkt \u00fcber os.system (Einfach Kommandos wie docker run \u00fcbergeben) oder \u00fcber das offizielle docker -Pythoninterface von der Docker Foundation Datenverarbeitung und Visualisierung Nutzen Bio.Phylo f\u00fcr die Erstellung des phylogenetischen Baums. Dieser kann dort direkt gezeichnet und als Bild ausgegeben werden. Schicke das Bild dann zum Client Container-Orchestrierung Erstelle ein Docker-Compose-File, das sowohl den Django-Container als auch den Phylogenie-Service-Container beinhaltet Zusatzaufgaben (optional) F\u00fcge Optionen f\u00fcr verschiedene phylogenetische Baum-Konstruktionsmethoden hinzu. Implementiere eine Funktion zum Exportieren der Ergebnisse in g\u00e4ngige Formate (z.B. Newick f\u00fcr B\u00e4ume, FASTA f\u00fcr Alignments). Abgabe Stelle sicher, dass doe Docker-Container korrekt konfiguriert und lauff\u00e4hig sind Dokumentiere die API-Endpunkte des MSA-Services. Protokolliere die Durchf\u00fchrung und die Ergebnisse Verwende folgende Protokollvorlage: Protokoll Vorlage.docx","title":"\u00dcbung Phylogenetischer Baum"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#zielsetzung","text":"In dieser \u00dcbung erweiterst du deine Django-Webseite um Funktionen zur Erstellung eines phylogenetischen Baums. Diese Funktionalit\u00e4t wird von einem bereits bestehenden separaten Container bereitgestellt. Realistische Testdaten kannst du dir z.B von der NCBI dbVar oder der NCBI Nucleotide DB holen.","title":"Zielsetzung"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#aufgabenstellung","text":"","title":"Aufgabenstellung"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#erweiterung-der-django-webseite","text":"F\u00fcge der bestehenden Django-Webseite ein neues Formular hinzu, das mehrere Sequenzen f\u00fcr die phylogenetische Analyse akzeptiert Implementiere eine neue View, die die eingegebenen Sequenzen an den Phylogenie-Service sendet","title":"Erweiterung der Django-Webseite"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#implementierung-des-phylogenie-services","text":"Verwende einen bestehenden Microservice f\u00fcr den Phylogenie-Service M\u00f6glich w\u00e4re z.B der raxml -Container F\u00fcr DNA-Sequenzen wird die Methode GTRGAMMA ben\u00f6tigt","title":"Implementierung des Phylogenie-Services"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#integration-und-kommunikation","text":"Implementiere die Kommunikation zwischen der Django-Webseite und dem Phylogenie-Service Starte die Container direkt \u00fcber os.system (Einfach Kommandos wie docker run \u00fcbergeben) oder \u00fcber das offizielle docker -Pythoninterface von der Docker Foundation","title":"Integration und Kommunikation"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#datenverarbeitung-und-visualisierung","text":"Nutzen Bio.Phylo f\u00fcr die Erstellung des phylogenetischen Baums. Dieser kann dort direkt gezeichnet und als Bild ausgegeben werden. Schicke das Bild dann zum Client","title":"Datenverarbeitung und Visualisierung"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#container-orchestrierung","text":"Erstelle ein Docker-Compose-File, das sowohl den Django-Container als auch den Phylogenie-Service-Container beinhaltet","title":"Container-Orchestrierung"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#zusatzaufgaben-optional","text":"F\u00fcge Optionen f\u00fcr verschiedene phylogenetische Baum-Konstruktionsmethoden hinzu. Implementiere eine Funktion zum Exportieren der Ergebnisse in g\u00e4ngige Formate (z.B. Newick f\u00fcr B\u00e4ume, FASTA f\u00fcr Alignments).","title":"Zusatzaufgaben (optional)"},{"location":"Assignments/5XHBGM/MGIN/Uebung3A/#abgabe","text":"Stelle sicher, dass doe Docker-Container korrekt konfiguriert und lauff\u00e4hig sind Dokumentiere die API-Endpunkte des MSA-Services. Protokolliere die Durchf\u00fchrung und die Ergebnisse Verwende folgende Protokollvorlage: Protokoll Vorlage.docx","title":"Abgabe"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/","text":"Einf\u00fchrung In dieser \u00dcbung wirst du dein ORF-Finder-Projekt um eine NCBI BLAST-Funktionalit\u00e4t erweitern. Die gefundenen ORF-Sequenzen werden an den NCBI BLAST-Service gesendet, um nach \u00e4hnlichen Sequenzen in der NCBI-Datenbank zu suchen. Die \u00dcbung soll verdeutlichen, wie man externe Dienste in bestehende Systeme integriert und die Ergebnisse verarbeitet. Lernziele Integration externer Dienste (NCBI BLAST) in ein bestehendes Django-Projekt Verwendung von Biopython f\u00fcr BLAST-Abfragen Asynchrone Verarbeitung von zeitaufw\u00e4ndigen Aufgaben (Requests) Darstellung und Interpretation von BLAST-Ergebnissen Aufgabenstellung 1. Vorbereitung Installiere Biopython f\u00fcr die Projekte in den entsprechenden ContainernS [Optional] Registriere dich f\u00fcr einen NCBI API-Schl\u00fcssel (https://www.ncbi.nlm.nih.gov/account/settings/). Vorzugsweise mit der Spengergassen-Email Nur Notwendig wenn die Requests blockiert werden, sollte auch ohne gehen 2. Erweiterung des ORF-Finder-Containers Implementiere eine neue Funktion im ORF-Finder-Container, die die gefundenen ORFs entgegennimmt und BLAST-Abfragen durchf\u00fchrt. Verwende z.B die Biopython Bio.Blast.NCBIWWW -Module f\u00fcr die BLAST-Suche. Verwende blastx als Algorithmus. Mehr Infos zu den unterschieden hier Die Funktion sollte folgende Parameter akzeptieren: ORF-Sequenz E-value Threshold Maximale Anzahl von Treffern 3. Anpassung der Django-Webanwendung Erweiter die ORF-Ergebnisseite um eine Option, f\u00fcr jeden gefundenen ORF eine BLAST-Suche zu starten. Erstelle eine neue View zur Anzeige der BLAST-Ergebnisse. 4. Ergebnisdarstellung Zeige die BLAST-Ergebnisse in einer \u00fcbersichtlichen Tabelle an, die folgende Informationen enth\u00e4lt: Sequenzbeschreibung E-value Bit-Score Identit\u00e4t (in Prozent) Alignmentl\u00e4nge Implementiere eine Option, um detaillierte Alignment-Informationen f\u00fcr jeden Treffer anzuzeigen. Was halt von der API als Ergebnis noch zur\u00fcckkommt. 5. Fehlerbehandlung und Benutzerfreundlichkeit Implementiere eine robuste Fehlerbehandlung f\u00fcr Netzwerkprobleme oder NCBI-Serverfehler. Erm\u00f6gliche das Abbrechen laufender BLAST-Abfragen. 6. Protokollierung Dokumentiere den Code ausf\u00fchrlich, insbesondere die Integration mit NCBI BLAST. Schreibe kurz und die Vor- und Nachteile der Nutzung externer Dienste wie NCBI BLAST. Kann man BLAST auch lokal hosten? Verwende folgende Protokollvorlage: Protokoll Vorlage.docx Bonusaufgaben Implementiere eine Funktion zum Herunterladen der BLAST-Ergebnisse als CSV- oder Excel-Datei. Erweiter die Anwendung um die M\u00f6glichkeit, verschiedene BLAST-Programme (blastn, blastp, blastx, tblastn, tblastx) auszuw\u00e4hlen. Bewertungskriterien Korrekte Implementation der BLAST-Funktionalit\u00e4t mit Biopython Effektive Integration des BLAST-Services in die bestehende Django-Anwendung Benutzerfreundlichkeit und \u00dcbersichtlichkeit der BLAST-Ergebnisdarstellung Robustheit der Implementierung (Fehlerbehandlung, Asynchronit\u00e4t) Qualit\u00e4t der Dokumentation und der Reflexion Hinweise Beachten die Rate-Limits bei der Implementierung. Testen deine Anwendung gr\u00fcndlich mit verschiedenen Eingabesequenzen und Parametern. Achten auf eine effiziente Verarbeitung, insbesondere bei gro\u00dfen Sequenzen oder vielen ORFs. Weitere Unterlagen Blast Quick Start","title":"\u00dcbungsaufgabe - Integration von NCBI BLAST in den ORF-Finder"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#einfuhrung","text":"In dieser \u00dcbung wirst du dein ORF-Finder-Projekt um eine NCBI BLAST-Funktionalit\u00e4t erweitern. Die gefundenen ORF-Sequenzen werden an den NCBI BLAST-Service gesendet, um nach \u00e4hnlichen Sequenzen in der NCBI-Datenbank zu suchen. Die \u00dcbung soll verdeutlichen, wie man externe Dienste in bestehende Systeme integriert und die Ergebnisse verarbeitet.","title":"Einf\u00fchrung"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#lernziele","text":"Integration externer Dienste (NCBI BLAST) in ein bestehendes Django-Projekt Verwendung von Biopython f\u00fcr BLAST-Abfragen Asynchrone Verarbeitung von zeitaufw\u00e4ndigen Aufgaben (Requests) Darstellung und Interpretation von BLAST-Ergebnissen","title":"Lernziele"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#aufgabenstellung","text":"","title":"Aufgabenstellung"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#1-vorbereitung","text":"Installiere Biopython f\u00fcr die Projekte in den entsprechenden ContainernS [Optional] Registriere dich f\u00fcr einen NCBI API-Schl\u00fcssel (https://www.ncbi.nlm.nih.gov/account/settings/). Vorzugsweise mit der Spengergassen-Email Nur Notwendig wenn die Requests blockiert werden, sollte auch ohne gehen","title":"1. Vorbereitung"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#2-erweiterung-des-orf-finder-containers","text":"Implementiere eine neue Funktion im ORF-Finder-Container, die die gefundenen ORFs entgegennimmt und BLAST-Abfragen durchf\u00fchrt. Verwende z.B die Biopython Bio.Blast.NCBIWWW -Module f\u00fcr die BLAST-Suche. Verwende blastx als Algorithmus. Mehr Infos zu den unterschieden hier Die Funktion sollte folgende Parameter akzeptieren: ORF-Sequenz E-value Threshold Maximale Anzahl von Treffern","title":"2. Erweiterung des ORF-Finder-Containers"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#3-anpassung-der-django-webanwendung","text":"Erweiter die ORF-Ergebnisseite um eine Option, f\u00fcr jeden gefundenen ORF eine BLAST-Suche zu starten. Erstelle eine neue View zur Anzeige der BLAST-Ergebnisse.","title":"3. Anpassung der Django-Webanwendung"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#4-ergebnisdarstellung","text":"Zeige die BLAST-Ergebnisse in einer \u00fcbersichtlichen Tabelle an, die folgende Informationen enth\u00e4lt: Sequenzbeschreibung E-value Bit-Score Identit\u00e4t (in Prozent) Alignmentl\u00e4nge Implementiere eine Option, um detaillierte Alignment-Informationen f\u00fcr jeden Treffer anzuzeigen. Was halt von der API als Ergebnis noch zur\u00fcckkommt.","title":"4. Ergebnisdarstellung"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#5-fehlerbehandlung-und-benutzerfreundlichkeit","text":"Implementiere eine robuste Fehlerbehandlung f\u00fcr Netzwerkprobleme oder NCBI-Serverfehler. Erm\u00f6gliche das Abbrechen laufender BLAST-Abfragen.","title":"5. Fehlerbehandlung und Benutzerfreundlichkeit"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#6-protokollierung","text":"Dokumentiere den Code ausf\u00fchrlich, insbesondere die Integration mit NCBI BLAST. Schreibe kurz und die Vor- und Nachteile der Nutzung externer Dienste wie NCBI BLAST. Kann man BLAST auch lokal hosten? Verwende folgende Protokollvorlage: Protokoll Vorlage.docx","title":"6. Protokollierung"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#bonusaufgaben","text":"Implementiere eine Funktion zum Herunterladen der BLAST-Ergebnisse als CSV- oder Excel-Datei. Erweiter die Anwendung um die M\u00f6glichkeit, verschiedene BLAST-Programme (blastn, blastp, blastx, tblastn, tblastx) auszuw\u00e4hlen.","title":"Bonusaufgaben"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#bewertungskriterien","text":"Korrekte Implementation der BLAST-Funktionalit\u00e4t mit Biopython Effektive Integration des BLAST-Services in die bestehende Django-Anwendung Benutzerfreundlichkeit und \u00dcbersichtlichkeit der BLAST-Ergebnisdarstellung Robustheit der Implementierung (Fehlerbehandlung, Asynchronit\u00e4t) Qualit\u00e4t der Dokumentation und der Reflexion","title":"Bewertungskriterien"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#hinweise","text":"Beachten die Rate-Limits bei der Implementierung. Testen deine Anwendung gr\u00fcndlich mit verschiedenen Eingabesequenzen und Parametern. Achten auf eine effiziente Verarbeitung, insbesondere bei gro\u00dfen Sequenzen oder vielen ORFs.","title":"Hinweise"},{"location":"Assignments/5XHBGM/MGIN/Uebung4/#weitere-unterlagen","text":"Blast Quick Start","title":"Weitere Unterlagen"},{"location":"Assignments/5XHBGM/MIS/AngularAdressservice/","text":"Angular Adressformular Angular Material Um nicht selbst ein Formular von Grund auf bauen und stylen zu m\u00fcssen eignet sich Angular Material . Zum einen gibt es dort vorgefertigte Komponenten welche sch\u00f6n aussehen. Zudem gibt es auch sogenannte Schematics welche Vorlagen f\u00fcr ganze Komponenten sind. Angular Material bietet daf\u00fcr die Schematics Table und AddressForm an. Erstelle ein neues Angular-Projekt: Anleitung hier F\u00fcge Material als Dependency in deinem Projekt hinzu: Erste Schritte mit Angular Material: hier Erstelle einen AddressForm Komponenten aus einer Schematic: Beispiel wie man eine Tabellen-Komponente aus einem Schema erstellt: hier Beispiel wie man eine AddressForm-Komponente aus einem Schema erstellt: hier Die fertige Schematic-Komponente sieht dann ungef\u00e4hr so aus: Adressservice der Stadt Wien Auf data.gv.at stellt die Stadt Wien eine API zur Verf\u00fcgung welche es erm\u00f6glicht weitere Infos zu einer Adresse zu bekommen. Damit ist es m\u00f6glich Autovervollst\u00e4ndigungen f\u00fcr alle Adressen in Wien vorzuschlagen. Hier die API_URL . Um diese API nutzen zu k\u00f6nnen bietet sich ein Service an. Beachte dass Angular bei der Erstellung eines Services das Wort Service als Postfix anh\u00e4ngt. Anleitung zum Services erstellen hier Der Service braucht auch Zugriff auf das HTTP-Modul, um mit der API kommunizieren zu k\u00f6nnen. Anleitung zur Kommunikation via HTTPModule hier Schreibe nun im AddressService eine Funktion welche eine Adresse an die API von Wien schickt und das Ergebnis davon asynchron retourniert. Das Ergebnis - ein JSON-String - sollte dann noch in einen Typ umgewandelt werden. Man kann mit json2ts aus einem JSON-String sich Typescript-Interfaces generieren lassen. F\u00fcge im Adressformular einen Button hinzu (wie im obigen Screenshot) welcher die Daten die sich in der Adresszeile befinden an den Adressservice schickt. Wenn sinnvolle Daten zur\u00fcckkommen sollen diese in die restlichen Felder wie PLZ, Ort etc. eingef\u00fcgt werden. Alternativer Adressservice Das Land Ober\u00f6sterreich betreibt mit Doris einen Service der \u00d6sterreichweit Adressen findet: hier Test- & Bewertungsgrundlagen Angular Material verwendet (30%) Eigenen Service verwendet (10%) Response typisiert und auf Validit\u00e4t \u00fcberpr\u00fcft (10%) Eingabe einer beliebigen Wiener Adresse, die restlichen Daten wie Bezirk, PLZ etc. sollen dann ausgef\u00fcllt werden. (50%) Zusatzaufgabe: Doris API verwendet (+10%)","title":"Angular Adressformular"},{"location":"Assignments/5XHBGM/MIS/AngularAdressservice/#angular-adressformular","text":"","title":"Angular Adressformular"},{"location":"Assignments/5XHBGM/MIS/AngularAdressservice/#angular-material","text":"Um nicht selbst ein Formular von Grund auf bauen und stylen zu m\u00fcssen eignet sich Angular Material . Zum einen gibt es dort vorgefertigte Komponenten welche sch\u00f6n aussehen. Zudem gibt es auch sogenannte Schematics welche Vorlagen f\u00fcr ganze Komponenten sind. Angular Material bietet daf\u00fcr die Schematics Table und AddressForm an. Erstelle ein neues Angular-Projekt: Anleitung hier F\u00fcge Material als Dependency in deinem Projekt hinzu: Erste Schritte mit Angular Material: hier Erstelle einen AddressForm Komponenten aus einer Schematic: Beispiel wie man eine Tabellen-Komponente aus einem Schema erstellt: hier Beispiel wie man eine AddressForm-Komponente aus einem Schema erstellt: hier Die fertige Schematic-Komponente sieht dann ungef\u00e4hr so aus:","title":"Angular Material"},{"location":"Assignments/5XHBGM/MIS/AngularAdressservice/#adressservice-der-stadt-wien","text":"Auf data.gv.at stellt die Stadt Wien eine API zur Verf\u00fcgung welche es erm\u00f6glicht weitere Infos zu einer Adresse zu bekommen. Damit ist es m\u00f6glich Autovervollst\u00e4ndigungen f\u00fcr alle Adressen in Wien vorzuschlagen. Hier die API_URL . Um diese API nutzen zu k\u00f6nnen bietet sich ein Service an. Beachte dass Angular bei der Erstellung eines Services das Wort Service als Postfix anh\u00e4ngt. Anleitung zum Services erstellen hier Der Service braucht auch Zugriff auf das HTTP-Modul, um mit der API kommunizieren zu k\u00f6nnen. Anleitung zur Kommunikation via HTTPModule hier Schreibe nun im AddressService eine Funktion welche eine Adresse an die API von Wien schickt und das Ergebnis davon asynchron retourniert. Das Ergebnis - ein JSON-String - sollte dann noch in einen Typ umgewandelt werden. Man kann mit json2ts aus einem JSON-String sich Typescript-Interfaces generieren lassen. F\u00fcge im Adressformular einen Button hinzu (wie im obigen Screenshot) welcher die Daten die sich in der Adresszeile befinden an den Adressservice schickt. Wenn sinnvolle Daten zur\u00fcckkommen sollen diese in die restlichen Felder wie PLZ, Ort etc. eingef\u00fcgt werden.","title":"Adressservice der Stadt Wien"},{"location":"Assignments/5XHBGM/MIS/AngularAdressservice/#alternativer-adressservice","text":"Das Land Ober\u00f6sterreich betreibt mit Doris einen Service der \u00d6sterreichweit Adressen findet: hier","title":"Alternativer Adressservice"},{"location":"Assignments/5XHBGM/MIS/AngularAdressservice/#test-bewertungsgrundlagen","text":"Angular Material verwendet (30%) Eigenen Service verwendet (10%) Response typisiert und auf Validit\u00e4t \u00fcberpr\u00fcft (10%) Eingabe einer beliebigen Wiener Adresse, die restlichen Daten wie Bezirk, PLZ etc. sollen dann ausgef\u00fcllt werden. (50%) Zusatzaufgabe: Doris API verwendet (+10%)","title":"Test- &amp; Bewertungsgrundlagen"},{"location":"Assignments/5XHBGM/MIS/AngularCRUDAssignment/","text":"AngularMaterialCRUD In diesem Repository sollen die vollen F\u00e4higkeiten des Backends ausgesch\u00f6pft werden. Beim Request-Assignment wurden nur die bestehenden Patienten gelesen und in einer Liste - ohne Angular Material - angezeigt. Anforderungen an das Frontend Im Frontend ist bereits eine package.json mit Angular Material vorkonfiguriert. Erstelle mittels eines Schematic zun\u00e4chst eine Navigation. Darin dann eine AddressForm und erweitere diese um alle Felder um einen Patient (mit mehreren Namen, Anschriften, etc. ) hinzuzuf\u00fcgen. Befehl zum generieren der Navigation Schematic ng generate @angular/material:navigation <component-name> Befehl zum generieren der Address-Form Schematic ng generate @angular/material:address-form <component-name> Zum Anzeigen bestehender Patienten kannst du den Quellcode aus dem AngularRequestAssignment teilweise wiederverwenden. Diese Liste soll als Tabelle angezeigt werden. Daf\u00fcr gibt es ebenfalls eine Schematic. Befehl zum generieren der Table Schematic ng generate @angular/material:table <component-name> Nachdem im Backend folgendes m\u00f6glich ist (bzw. sein sollte) - Alle Patienten abfragen - Patient mit bestimmter ID abfragen - Patient hinzuf\u00fcgen - Patient \u00e4ndern - Patient l\u00f6schen sind diese Funktionen im Frontend zu implementieren: - Alle Patienten anzeigen (je Patient ein Bearbeiten & L\u00f6schen-Button) - Klickt man auf einen Patienten kommt man via einem RouterLink in die Detailansicht - Detailansicht eines Patienten die ID wird aus der URL ausgelesen - Dort sollen die Daten als reiner Text (in divs oder spans) angezeigt werden - Mit Klick auf einen bearbeiten-Button soll die Komponente ausgetauscht werden gegen ein Formular welches alle Felder bearbeitbar macht Anforderungen an das Backend Damit die Anwendung unabh\u00e4ngig von der lokalen Umgebung (also XAMPP) ausf\u00fchrbar ist, ist die Datenbank in einer von beiden Varianten bereitzustellen: - Als Docker-Container - Die H2-Datenbank verwenden (oder eine andere In-Memory Datenbank) Der Server ist im Backend einzuf\u00fcgen. Entweder das komplette Maven-Projekt oder nur die Jar-Datei. Abgaberichtlinien Gib in einem Unterordner deinen Server dazu, mit mvn spring-boot:run werde ich diesen dann ausf\u00fchren um das Frontend zu unterst\u00fctzen. Achte darauf dass die .gitignore -Datei angewandt wird um die Repository-Gr\u00f6\u00dfe m\u00f6glichst gering zu halten. Das Frontend werde ich mit ng serve starten. Auch hier bitte darauf achten dass node_modules , .angular etc. nicht commited werden. Bewertungsgrundlagen Backend ausf\u00fchrbar 10P Der Befehl mvn spring-boot:run st\u00fcrzt nicht ab Frontend ausf\u00fchrbar 10P Der Befehl ng serve st\u00fcrzt nicht ab Patientenliste wird angezeigt 20P Ein HTTP-Request mit den Patienten kommt an und ausgew\u00e4hlte Spalten k\u00f6nnen in der \u00dcbersicht angezeigt werden. Patient kann bearbeitet werden 20P Klickt man in der \u00dcbersicht auf Edit kommt man auf /patient/:id . Dort sind ganz viele bearbeitbare Textfelder Patient kann gel\u00f6scht werden 20P Durch einen klick auf Delete wird ein HTTP Request ausgef\u00fchrt um den Patienten mit gegebener Id aus der DB zu entfernen Einzelner Patient wird angezeigt 20P Gleich wie Edit nur das die Felder nicht bearbeitbar sein sollten Lade in deinem Repository eine Datei bewertung.md hoch in der du jedem der obigen Anforderungen (Teil)Punkte gibst. Begr\u00fcnde deine Punkteanzahl. Tutorials zum Nachlesen Routing Schematics HTTP","title":"AngularMaterialCRUD"},{"location":"Assignments/5XHBGM/MIS/AngularCRUDAssignment/#angularmaterialcrud","text":"In diesem Repository sollen die vollen F\u00e4higkeiten des Backends ausgesch\u00f6pft werden. Beim Request-Assignment wurden nur die bestehenden Patienten gelesen und in einer Liste - ohne Angular Material - angezeigt.","title":"AngularMaterialCRUD"},{"location":"Assignments/5XHBGM/MIS/AngularCRUDAssignment/#anforderungen-an-das-frontend","text":"Im Frontend ist bereits eine package.json mit Angular Material vorkonfiguriert. Erstelle mittels eines Schematic zun\u00e4chst eine Navigation. Darin dann eine AddressForm und erweitere diese um alle Felder um einen Patient (mit mehreren Namen, Anschriften, etc. ) hinzuzuf\u00fcgen. Befehl zum generieren der Navigation Schematic ng generate @angular/material:navigation <component-name> Befehl zum generieren der Address-Form Schematic ng generate @angular/material:address-form <component-name> Zum Anzeigen bestehender Patienten kannst du den Quellcode aus dem AngularRequestAssignment teilweise wiederverwenden. Diese Liste soll als Tabelle angezeigt werden. Daf\u00fcr gibt es ebenfalls eine Schematic. Befehl zum generieren der Table Schematic ng generate @angular/material:table <component-name> Nachdem im Backend folgendes m\u00f6glich ist (bzw. sein sollte) - Alle Patienten abfragen - Patient mit bestimmter ID abfragen - Patient hinzuf\u00fcgen - Patient \u00e4ndern - Patient l\u00f6schen sind diese Funktionen im Frontend zu implementieren: - Alle Patienten anzeigen (je Patient ein Bearbeiten & L\u00f6schen-Button) - Klickt man auf einen Patienten kommt man via einem RouterLink in die Detailansicht - Detailansicht eines Patienten die ID wird aus der URL ausgelesen - Dort sollen die Daten als reiner Text (in divs oder spans) angezeigt werden - Mit Klick auf einen bearbeiten-Button soll die Komponente ausgetauscht werden gegen ein Formular welches alle Felder bearbeitbar macht","title":"Anforderungen an das Frontend"},{"location":"Assignments/5XHBGM/MIS/AngularCRUDAssignment/#anforderungen-an-das-backend","text":"Damit die Anwendung unabh\u00e4ngig von der lokalen Umgebung (also XAMPP) ausf\u00fchrbar ist, ist die Datenbank in einer von beiden Varianten bereitzustellen: - Als Docker-Container - Die H2-Datenbank verwenden (oder eine andere In-Memory Datenbank) Der Server ist im Backend einzuf\u00fcgen. Entweder das komplette Maven-Projekt oder nur die Jar-Datei.","title":"Anforderungen an das Backend"},{"location":"Assignments/5XHBGM/MIS/AngularCRUDAssignment/#abgaberichtlinien","text":"Gib in einem Unterordner deinen Server dazu, mit mvn spring-boot:run werde ich diesen dann ausf\u00fchren um das Frontend zu unterst\u00fctzen. Achte darauf dass die .gitignore -Datei angewandt wird um die Repository-Gr\u00f6\u00dfe m\u00f6glichst gering zu halten. Das Frontend werde ich mit ng serve starten. Auch hier bitte darauf achten dass node_modules , .angular etc. nicht commited werden.","title":"Abgaberichtlinien"},{"location":"Assignments/5XHBGM/MIS/AngularCRUDAssignment/#bewertungsgrundlagen","text":"Backend ausf\u00fchrbar 10P Der Befehl mvn spring-boot:run st\u00fcrzt nicht ab Frontend ausf\u00fchrbar 10P Der Befehl ng serve st\u00fcrzt nicht ab Patientenliste wird angezeigt 20P Ein HTTP-Request mit den Patienten kommt an und ausgew\u00e4hlte Spalten k\u00f6nnen in der \u00dcbersicht angezeigt werden. Patient kann bearbeitet werden 20P Klickt man in der \u00dcbersicht auf Edit kommt man auf /patient/:id . Dort sind ganz viele bearbeitbare Textfelder Patient kann gel\u00f6scht werden 20P Durch einen klick auf Delete wird ein HTTP Request ausgef\u00fchrt um den Patienten mit gegebener Id aus der DB zu entfernen Einzelner Patient wird angezeigt 20P Gleich wie Edit nur das die Felder nicht bearbeitbar sein sollten Lade in deinem Repository eine Datei bewertung.md hoch in der du jedem der obigen Anforderungen (Teil)Punkte gibst. Begr\u00fcnde deine Punkteanzahl.","title":"Bewertungsgrundlagen"},{"location":"Assignments/5XHBGM/MIS/AngularCRUDAssignment/#tutorials-zum-nachlesen","text":"Routing Schematics HTTP","title":"Tutorials zum Nachlesen"},{"location":"Assignments/5XHBGM/MIS/AngularRequestAssignment/","text":"Angular Request Assignment In dem Projekt befindet sich der Stand der vorherigen Unterrichtseinheit. Es wird also die eigene IP-Adresse von ifconfig.me abgefragt. Folgendes ist zu implementieren: Erzeuge mittels http.get(...) einen Request auf deine REST-API aus dem 4. Jahrgang. Die URL f\u00fcr alle Patienten sollte ungef\u00e4hr so aussehen: http://localhost:8080/api/patient . Speichere die zur\u00fcckgelieferten JSON-Rohdaten in einem eigens angelegten Typ. Dieser k\u00f6nnte ungef\u00e4hr so aussehen: // https://hl7.org/fhir/patient.html interface Patient{ name: string; gender: string; birthDate: Date; ... } Wenn dann von /api/patient mehrere Patienten zur\u00fcckgemeldet werden sollen diese in einem Patienten-Array ( Patient[] ) gespeichert werden. Zeige eine Tabelle mit allen Patienteninfos in einer \u00dcbersicht an. Diese k\u00f6nnte so aussehen: Name Geschlecht Geburtsdatum ... Max Mustermann 1.1.2000 ... ... Abgabe Abzugeben sind neben dem Quellcode auch Screenshots in denen ersichtlich ist - dass Patienten alle Patienten in einer Tabelle angezeigt werden - dass das Spring-Backend im Hintergrund l\u00e4uft - dass Angular einen Request an Spring gesendet hat - m\u00f6glich \u00fcber den Netzwerkanalyse-Tab in Firefox In dem Ordner screenshots sollen diese gespeichert werden. \u00dcber die GIthub-Actions wird automatisch ein Test ausgef\u00fchrt welcher pr\u00fcft ob die HTTP-Anfrage korrekt durchgef\u00fchrt wurde. Um zu sehen ob der Test lokal funktioniert kannst du mit npm run test dies lokal pr\u00fcfen. Auch kannst du mit Webstorm die Tests in der Datei app.component.spec.ts direkt ausf\u00fchren. Anleitungen: Neues Angular Projekt anlegen: https://angular.io/tutorial/toh-pt0 HTTPClient in Angular: https://angular.io/guide/http","title":"Angular Request Assignment"},{"location":"Assignments/5XHBGM/MIS/AngularRequestAssignment/#angular-request-assignment","text":"In dem Projekt befindet sich der Stand der vorherigen Unterrichtseinheit. Es wird also die eigene IP-Adresse von ifconfig.me abgefragt. Folgendes ist zu implementieren: Erzeuge mittels http.get(...) einen Request auf deine REST-API aus dem 4. Jahrgang. Die URL f\u00fcr alle Patienten sollte ungef\u00e4hr so aussehen: http://localhost:8080/api/patient . Speichere die zur\u00fcckgelieferten JSON-Rohdaten in einem eigens angelegten Typ. Dieser k\u00f6nnte ungef\u00e4hr so aussehen: // https://hl7.org/fhir/patient.html interface Patient{ name: string; gender: string; birthDate: Date; ... } Wenn dann von /api/patient mehrere Patienten zur\u00fcckgemeldet werden sollen diese in einem Patienten-Array ( Patient[] ) gespeichert werden. Zeige eine Tabelle mit allen Patienteninfos in einer \u00dcbersicht an. Diese k\u00f6nnte so aussehen: Name Geschlecht Geburtsdatum ... Max Mustermann 1.1.2000 ... ...","title":"Angular Request Assignment"},{"location":"Assignments/5XHBGM/MIS/AngularRequestAssignment/#abgabe","text":"Abzugeben sind neben dem Quellcode auch Screenshots in denen ersichtlich ist - dass Patienten alle Patienten in einer Tabelle angezeigt werden - dass das Spring-Backend im Hintergrund l\u00e4uft - dass Angular einen Request an Spring gesendet hat - m\u00f6glich \u00fcber den Netzwerkanalyse-Tab in Firefox In dem Ordner screenshots sollen diese gespeichert werden. \u00dcber die GIthub-Actions wird automatisch ein Test ausgef\u00fchrt welcher pr\u00fcft ob die HTTP-Anfrage korrekt durchgef\u00fchrt wurde. Um zu sehen ob der Test lokal funktioniert kannst du mit npm run test dies lokal pr\u00fcfen. Auch kannst du mit Webstorm die Tests in der Datei app.component.spec.ts direkt ausf\u00fchren.","title":"Abgabe"},{"location":"Assignments/5XHBGM/MIS/AngularRequestAssignment/#anleitungen","text":"Neues Angular Projekt anlegen: https://angular.io/tutorial/toh-pt0 HTTPClient in Angular: https://angular.io/guide/http","title":"Anleitungen:"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/","text":"Rest-API absichern Um eine API abzusichern muss vor dem Zugriff auf die Daten eine Authentifizierung sowie eine Autorisierung stattfinden. Als Projektgrundlage kannst du ein bereits bestehendes Projekt verwenden, falls du ein neues anlegen m\u00f6chtest/musst beachte bitte folgendes: Erstelle auf https://start.spring.io/ ein Spring-Projekt mit den folgenden Dependencies: - H2 Database - Spring Security - Spring Data JPA - Lombok - Rest Repositories - ... wahlweise weitere Libraries, Datenbanktreiber etc. Authentifizierung Damit ist gemeint, dass sich ein Benutzer einloggen muss, um die Identit\u00e4t festzustellen. Dazu reicht eine Basic Authentication . Um Username+Passwort mitzusenden muss man den Request folgenderma\u00dfen aufbauen: ```http request get ressource GET http://localhost:8080/ Authorization: Basic ``` Die \u00fcbermittelten Daten kann man dann mit einem Authentication -Objekt abfangen beim Endpunkt. Allerdings sind die Daten darin NICHT verifiziert. Den Schritt muss man noch selber \u00fcbernehmen. @GetMapping(\"/<secret-ressource>\") Iterator<Object> getSecretRessource(Authentication auth){ if(isAuthValid(auth)){ # todo implement a authentication check return repo.findAll(); } } Speichern der Benutzerdaten In Spring gibt es bereits das Interface UserDetails . Dieses speichert den Benutzernamen, das Passwort und enth\u00e4lt eine Liste von GrantedAuthoritiy . Damit kann man Berechtigungen festhalten. Es werden auch noch einige weitere Details gespeichert, die sind aber f\u00fcr die aktuelle Aufgabenstellung nicht wichtig. Die Interfaces direkt kann man nicht speichern, jedoch kann man in je einer Klasse die Interfaces implementieren und mit einer @Entity -Annotation dazu wird es speicherbar. Authorization Um festzustellen ob ein bereits eingeloggter Benutzer auch tats\u00e4chlich berectigt ist bestimmte Ressorucen aufzurufen kann man mittels einer Access Control List pro Objekt und Benutzer berechtigungen vergeben. https://en.wikipedia.org/wiki/Access-control_list Speichern von Berechtigungen Bei Access-Control Lists kann f\u00fcr jedes Objekt seperat gespeichert werden wer welche Zugriffsberechtigung darauf hat. Bei der Klasse die das speichert, w\u00fcrde es sich anbieten dass diese GrantedAuthority implementiert. In dieser GrantedAuthority sollten folgende Daten enthalten sein: - Id des Benutzers der Zugriff hat - Id des Objects auf welches sich die Berechtigung bezieht - Vollst\u00e4ndiger Name des Objekts (z.B at.spengermed.patient) - Die Berechtigung - ... wahlweise weitere Daten, falls Hilfreich Zu testende Situationen Die folgenden Aufgaben sollen \u00fcber das Aufrufen von Endpoints geschehen (via mockMvc): ~~Ein Benutzer wird neu angelegt (20P)~~ ~~Ein Administrator teilt dem neuen Benutzer Berechtigungen zu (20P)~~ Anlegen eines neuen FHIR-Objekts (z.B Patient, oder eine kleinere FHIR Ressource) (20P) Freigeben des neuen FHIR-Objekts von einem Benutzer zu einem anderen (20P) Aufrufen eines neuen FHIR-Objekts mit Berechtigungen (10P) Versuch ein Objekt mit fehlenden Berechtigungen aufzurufen (10P) Checkliste vor der Abgabe Sind alle \u00c4nderungen comitted? Sind alle \u00c4nderungen gepusht und hat jeder den gleichen Stand? St\u00fcrzt das Programm ab wenn man es startet oder die neuen Endpunkte (mit Fehlerhaften Daten) aufruft? Werden alle Tests erfolgreich ausgef\u00fchrt in der Konsole mit den Befehlen gradle test oder mvn test ? Hat jeder ungef\u00e4hr gleich viel comitted? Anleitungen Beispielrepository + Anleitung Aktuelles Spring Security Tutorial - Die Basic Authentication Kapitel sind wichtig","title":"Rest-API absichern"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#rest-api-absichern","text":"Um eine API abzusichern muss vor dem Zugriff auf die Daten eine Authentifizierung sowie eine Autorisierung stattfinden. Als Projektgrundlage kannst du ein bereits bestehendes Projekt verwenden, falls du ein neues anlegen m\u00f6chtest/musst beachte bitte folgendes: Erstelle auf https://start.spring.io/ ein Spring-Projekt mit den folgenden Dependencies: - H2 Database - Spring Security - Spring Data JPA - Lombok - Rest Repositories - ... wahlweise weitere Libraries, Datenbanktreiber etc.","title":"Rest-API absichern"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#authentifizierung","text":"Damit ist gemeint, dass sich ein Benutzer einloggen muss, um die Identit\u00e4t festzustellen. Dazu reicht eine Basic Authentication . Um Username+Passwort mitzusenden muss man den Request folgenderma\u00dfen aufbauen: ```http request","title":"Authentifizierung"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#get-ressource","text":"GET http://localhost:8080/ Authorization: Basic ``` Die \u00fcbermittelten Daten kann man dann mit einem Authentication -Objekt abfangen beim Endpunkt. Allerdings sind die Daten darin NICHT verifiziert. Den Schritt muss man noch selber \u00fcbernehmen. @GetMapping(\"/<secret-ressource>\") Iterator<Object> getSecretRessource(Authentication auth){ if(isAuthValid(auth)){ # todo implement a authentication check return repo.findAll(); } }","title":"get ressource"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#speichern-der-benutzerdaten","text":"In Spring gibt es bereits das Interface UserDetails . Dieses speichert den Benutzernamen, das Passwort und enth\u00e4lt eine Liste von GrantedAuthoritiy . Damit kann man Berechtigungen festhalten. Es werden auch noch einige weitere Details gespeichert, die sind aber f\u00fcr die aktuelle Aufgabenstellung nicht wichtig. Die Interfaces direkt kann man nicht speichern, jedoch kann man in je einer Klasse die Interfaces implementieren und mit einer @Entity -Annotation dazu wird es speicherbar.","title":"Speichern der Benutzerdaten"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#authorization","text":"Um festzustellen ob ein bereits eingeloggter Benutzer auch tats\u00e4chlich berectigt ist bestimmte Ressorucen aufzurufen kann man mittels einer Access Control List pro Objekt und Benutzer berechtigungen vergeben. https://en.wikipedia.org/wiki/Access-control_list","title":"Authorization"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#speichern-von-berechtigungen","text":"Bei Access-Control Lists kann f\u00fcr jedes Objekt seperat gespeichert werden wer welche Zugriffsberechtigung darauf hat. Bei der Klasse die das speichert, w\u00fcrde es sich anbieten dass diese GrantedAuthority implementiert. In dieser GrantedAuthority sollten folgende Daten enthalten sein: - Id des Benutzers der Zugriff hat - Id des Objects auf welches sich die Berechtigung bezieht - Vollst\u00e4ndiger Name des Objekts (z.B at.spengermed.patient) - Die Berechtigung - ... wahlweise weitere Daten, falls Hilfreich","title":"Speichern von Berechtigungen"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#zu-testende-situationen","text":"Die folgenden Aufgaben sollen \u00fcber das Aufrufen von Endpoints geschehen (via mockMvc): ~~Ein Benutzer wird neu angelegt (20P)~~ ~~Ein Administrator teilt dem neuen Benutzer Berechtigungen zu (20P)~~ Anlegen eines neuen FHIR-Objekts (z.B Patient, oder eine kleinere FHIR Ressource) (20P) Freigeben des neuen FHIR-Objekts von einem Benutzer zu einem anderen (20P) Aufrufen eines neuen FHIR-Objekts mit Berechtigungen (10P) Versuch ein Objekt mit fehlenden Berechtigungen aufzurufen (10P)","title":"Zu testende Situationen"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#checkliste-vor-der-abgabe","text":"Sind alle \u00c4nderungen comitted? Sind alle \u00c4nderungen gepusht und hat jeder den gleichen Stand? St\u00fcrzt das Programm ab wenn man es startet oder die neuen Endpunkte (mit Fehlerhaften Daten) aufruft? Werden alle Tests erfolgreich ausgef\u00fchrt in der Konsole mit den Befehlen gradle test oder mvn test ? Hat jeder ungef\u00e4hr gleich viel comitted?","title":"Checkliste vor der Abgabe"},{"location":"Assignments/5XHBGM/MIS/SafeAPIAssignment/#anleitungen","text":"Beispielrepository + Anleitung Aktuelles Spring Security Tutorial - Die Basic Authentication Kapitel sind wichtig","title":"Anleitungen"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/","text":"Rest-API absichern Um eine API abzusichern muss vor dem Zugriff auf die Daten eine Authentifizierung sowie eine Autorisierung stattfinden. Als Projektgrundlage kannst du ein bereits bestehendes Projekt verwenden, falls du ein neues anlegen m\u00f6chtest/musst beachte bitte folgendes: Erstelle auf https://start.spring.io/ ein Spring-Projekt mit den folgenden Dependencies: - H2 Database - Spring Security - Spring Data JPA - Lombok - Rest Repositories - ... wahlweise weitere Libraries, Datenbanktreiber etc. Authentifizierung Damit ist gemeint, dass sich ein Benutzer einloggen muss, um die Identit\u00e4t festzustellen. Dazu reicht eine Basic Authentication . Um Username+Passwort mitzusenden muss man den Request folgenderma\u00dfen aufbauen: ```http request get ressource GET http://localhost:8080/ Authorization: Basic ``` Die \u00fcbermittelten Daten kann man dann mit einem Authentication -Objekt abfangen beim Endpunkt. Allerdings sind die Daten darin NICHT verifiziert. Den Schritt muss man noch selber \u00fcbernehmen. @GetMapping(\"/<secret-ressource>\") Iterator<Object> getSecretRessource(Authentication auth){ if(isAuthValid(auth)){ # todo implement a authentication check return repo.findAll(); } } Speichern der Benutzerdaten In Spring gibt es bereits das Interface UserDetails . Dieses speichert den Benutzernamen, das Passwort und enth\u00e4lt eine Liste von GrantedAuthoritiy . Damit kann man Berechtigungen festhalten. Es werden auch noch einige weitere Details gespeichert, die sind aber f\u00fcr die aktuelle Aufgabenstellung nicht wichtig. Die Interfaces direkt kann man nicht speichern, jedoch kann man in je einer Klasse die Interfaces implementieren und mit einer @Entity -Annotation dazu wird es speicherbar. Authorization Um festzustellen ob ein bereits eingeloggter Benutzer auch tats\u00e4chlich berectigt ist bestimmte Ressorucen aufzurufen kann man mittels einer Access Control List pro Objekt und Benutzer berechtigungen vergeben. https://en.wikipedia.org/wiki/Access-control_list Speichern von Berechtigungen Bei Access-Control Lists kann f\u00fcr jedes Objekt seperat gespeichert werden wer welche Zugriffsberechtigung darauf hat. Bei der Klasse die das speichert, w\u00fcrde es sich anbieten dass diese GrantedAuthority implementiert. In dieser GrantedAuthority sollten folgende Daten enthalten sein: - Id des Benutzers der Zugriff hat - Id des Objects auf welches sich die Berechtigung bezieht - Vollst\u00e4ndiger Name des Objekts (z.B at.spengermed.patient) - Die Berechtigung - ... wahlweise weitere Daten, falls Hilfreich Zu testende Situationen Die folgenden Aufgaben sollen \u00fcber das Aufrufen von Endpoints geschehen (via mockMvc): ~~Ein Benutzer wird neu angelegt (20P)~~ ~~Ein Administrator teilt dem neuen Benutzer Berechtigungen zu (20P)~~ Anlegen eines neuen FHIR-Objekts (z.B Patient, oder eine kleinere FHIR Ressource) (20P) Freigeben des neuen FHIR-Objekts von einem Benutzer zu einem anderen (20P) Aufrufen eines neuen FHIR-Objekts mit Berechtigungen (10P) Versuch ein Objekt mit fehlenden Berechtigungen aufzurufen (10P) Checkliste vor der Abgabe Sind alle \u00c4nderungen comitted? Sind alle \u00c4nderungen gepusht und hat jeder den gleichen Stand? St\u00fcrzt das Programm ab wenn man es startet oder die neuen Endpunkte (mit Fehlerhaften Daten) aufruft? Werden alle Tests erfolgreich ausgef\u00fchrt in der Konsole mit den Befehlen gradle test oder mvn test ? Hat jeder ungef\u00e4hr gleich viel comitted? Anleitungen Beispielrepository + Anleitung Aktuelles Spring Security Tutorial - Die Basic Authentication Kapitel sind wichtig","title":"Rest-API absichern"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#rest-api-absichern","text":"Um eine API abzusichern muss vor dem Zugriff auf die Daten eine Authentifizierung sowie eine Autorisierung stattfinden. Als Projektgrundlage kannst du ein bereits bestehendes Projekt verwenden, falls du ein neues anlegen m\u00f6chtest/musst beachte bitte folgendes: Erstelle auf https://start.spring.io/ ein Spring-Projekt mit den folgenden Dependencies: - H2 Database - Spring Security - Spring Data JPA - Lombok - Rest Repositories - ... wahlweise weitere Libraries, Datenbanktreiber etc.","title":"Rest-API absichern"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#authentifizierung","text":"Damit ist gemeint, dass sich ein Benutzer einloggen muss, um die Identit\u00e4t festzustellen. Dazu reicht eine Basic Authentication . Um Username+Passwort mitzusenden muss man den Request folgenderma\u00dfen aufbauen: ```http request","title":"Authentifizierung"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#get-ressource","text":"GET http://localhost:8080/ Authorization: Basic ``` Die \u00fcbermittelten Daten kann man dann mit einem Authentication -Objekt abfangen beim Endpunkt. Allerdings sind die Daten darin NICHT verifiziert. Den Schritt muss man noch selber \u00fcbernehmen. @GetMapping(\"/<secret-ressource>\") Iterator<Object> getSecretRessource(Authentication auth){ if(isAuthValid(auth)){ # todo implement a authentication check return repo.findAll(); } }","title":"get ressource"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#speichern-der-benutzerdaten","text":"In Spring gibt es bereits das Interface UserDetails . Dieses speichert den Benutzernamen, das Passwort und enth\u00e4lt eine Liste von GrantedAuthoritiy . Damit kann man Berechtigungen festhalten. Es werden auch noch einige weitere Details gespeichert, die sind aber f\u00fcr die aktuelle Aufgabenstellung nicht wichtig. Die Interfaces direkt kann man nicht speichern, jedoch kann man in je einer Klasse die Interfaces implementieren und mit einer @Entity -Annotation dazu wird es speicherbar.","title":"Speichern der Benutzerdaten"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#authorization","text":"Um festzustellen ob ein bereits eingeloggter Benutzer auch tats\u00e4chlich berectigt ist bestimmte Ressorucen aufzurufen kann man mittels einer Access Control List pro Objekt und Benutzer berechtigungen vergeben. https://en.wikipedia.org/wiki/Access-control_list","title":"Authorization"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#speichern-von-berechtigungen","text":"Bei Access-Control Lists kann f\u00fcr jedes Objekt seperat gespeichert werden wer welche Zugriffsberechtigung darauf hat. Bei der Klasse die das speichert, w\u00fcrde es sich anbieten dass diese GrantedAuthority implementiert. In dieser GrantedAuthority sollten folgende Daten enthalten sein: - Id des Benutzers der Zugriff hat - Id des Objects auf welches sich die Berechtigung bezieht - Vollst\u00e4ndiger Name des Objekts (z.B at.spengermed.patient) - Die Berechtigung - ... wahlweise weitere Daten, falls Hilfreich","title":"Speichern von Berechtigungen"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#zu-testende-situationen","text":"Die folgenden Aufgaben sollen \u00fcber das Aufrufen von Endpoints geschehen (via mockMvc): ~~Ein Benutzer wird neu angelegt (20P)~~ ~~Ein Administrator teilt dem neuen Benutzer Berechtigungen zu (20P)~~ Anlegen eines neuen FHIR-Objekts (z.B Patient, oder eine kleinere FHIR Ressource) (20P) Freigeben des neuen FHIR-Objekts von einem Benutzer zu einem anderen (20P) Aufrufen eines neuen FHIR-Objekts mit Berechtigungen (10P) Versuch ein Objekt mit fehlenden Berechtigungen aufzurufen (10P)","title":"Zu testende Situationen"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#checkliste-vor-der-abgabe","text":"Sind alle \u00c4nderungen comitted? Sind alle \u00c4nderungen gepusht und hat jeder den gleichen Stand? St\u00fcrzt das Programm ab wenn man es startet oder die neuen Endpunkte (mit Fehlerhaften Daten) aufruft? Werden alle Tests erfolgreich ausgef\u00fchrt in der Konsole mit den Befehlen gradle test oder mvn test ? Hat jeder ungef\u00e4hr gleich viel comitted?","title":"Checkliste vor der Abgabe"},{"location":"Assignments/5XHBGM/MIS/Security/SafeAPIAssignment/#anleitungen","text":"Beispielrepository + Anleitung Aktuelles Spring Security Tutorial - Die Basic Authentication Kapitel sind wichtig","title":"Anleitungen"},{"location":"Assignments/5XHBGM/NodeContainer/","text":"Ausgangssituation Du hast eine bestehende Spring-Anwendung, die bisher lokal auf deinem Entwicklungsrechner l\u00e4uft. Die Anwendung verwendet eine MySQL-Datenbank zur Datenspeicherung. Ziel Deine Aufgabe ist es, die Spring-Anwendung in einen Docker-Container zu verpacken und sie mit einer MySQL-Datenbank in einem separaten Container zu verbinden. Dies soll mithilfe von Docker Compose geschehen. Spring und MySQL m\u00fcssen nicht unbedingt verwendet werden. Eine beliebige Anwendung und eine beliebige Datenbank reichen aus. Anforderungen 1. Dockerisierung der Spring-Anwendung Erstelle ein Dockerfile f\u00fcr deine Spring-Anwendung. Passe die Anwendungskonfiguration an, damit sie im Container-Umfeld funktioniert. Stelle sicher, dass alle notwendigen Abh\u00e4ngigkeiten im Container vorhanden sind. 2. Docker Compose Konfiguration Erstelle eine docker-compose.yml Datei. Definiere zwei Services: Deine Spring-Anwendung MySQL-Datenbank Setze entsprechende Umgebungsvariablen f\u00fcr die Datenbankverbindung. Lege ein Volume f\u00fcr die Datenbank an, und nutze dieses im Service 3. Anpassung der Anwendungskonfiguration Passe das Anwendungsprofil f\u00fcr die Produktionsumgebung an ( application-docker.properties ). Konfiguriere die Datenbankverbindung so, dass sie den Docker-Netzwerknamen verwendet. 4. Testen und Dokumentation Teste deine Dockerisierte Anwendung lokal. Dokumentiere die notwendigen Schritte zum Starten der Anwendung mit Docker Compose. Beschreibe eventuell aufgetretene Probleme und deren L\u00f6sungen. Zeige dass die Daten auch nach einem Neustart der Container noch vorhanden sind Abgabe Dein Protokoll soll enthalten: Dockerfile docker-compose.yml Angepasste Anwendungskonfigurationsdateien Kurze Dokumentation (max. 1 Seite des Protokolls) mit Erkl\u00e4rungen und Anweisungen zum Starten der Anwendung Beurteilungskriterien Docker Volumes werden verwendet und beschrieben Umgebungsvariablen, geladen aus einer .env-Datei werden geladen H\u00e4ufige Probleme und L\u00f6sungen: Container startet nicht: Logs pr\u00fcfen: sudo docker compose logs Build-Logs pr\u00fcfen: sudo docker compose build --no-cache Ports pr\u00fcfen: sudo netstat -tulpn Statische Assets werden nicht geladen: Verifizieren Sie die Build-Ausgabe: sudo docker exec -it <container-id> ls /<path> Performance-Probleme: Container Ressourcen \u00fcberpr\u00fcfen: sudo docker stats System-Ressourcen monitoren (nur Linux): htop \u00c4nderungen werden nicht \u00fcbernommen Container muss bei jeder \u00c4nderung neu gebaut werden Start mit docker compose up -d --build","title":"Dockerisierung einer Spring-Anwendung mit MySQL-Datenbank"},{"location":"Assignments/5XHBGM/NodeContainer/#ausgangssituation","text":"Du hast eine bestehende Spring-Anwendung, die bisher lokal auf deinem Entwicklungsrechner l\u00e4uft. Die Anwendung verwendet eine MySQL-Datenbank zur Datenspeicherung.","title":"Ausgangssituation"},{"location":"Assignments/5XHBGM/NodeContainer/#ziel","text":"Deine Aufgabe ist es, die Spring-Anwendung in einen Docker-Container zu verpacken und sie mit einer MySQL-Datenbank in einem separaten Container zu verbinden. Dies soll mithilfe von Docker Compose geschehen. Spring und MySQL m\u00fcssen nicht unbedingt verwendet werden. Eine beliebige Anwendung und eine beliebige Datenbank reichen aus.","title":"Ziel"},{"location":"Assignments/5XHBGM/NodeContainer/#anforderungen","text":"","title":"Anforderungen"},{"location":"Assignments/5XHBGM/NodeContainer/#1-dockerisierung-der-spring-anwendung","text":"Erstelle ein Dockerfile f\u00fcr deine Spring-Anwendung. Passe die Anwendungskonfiguration an, damit sie im Container-Umfeld funktioniert. Stelle sicher, dass alle notwendigen Abh\u00e4ngigkeiten im Container vorhanden sind.","title":"1. Dockerisierung der Spring-Anwendung"},{"location":"Assignments/5XHBGM/NodeContainer/#2-docker-compose-konfiguration","text":"Erstelle eine docker-compose.yml Datei. Definiere zwei Services: Deine Spring-Anwendung MySQL-Datenbank Setze entsprechende Umgebungsvariablen f\u00fcr die Datenbankverbindung. Lege ein Volume f\u00fcr die Datenbank an, und nutze dieses im Service","title":"2. Docker Compose Konfiguration"},{"location":"Assignments/5XHBGM/NodeContainer/#3-anpassung-der-anwendungskonfiguration","text":"Passe das Anwendungsprofil f\u00fcr die Produktionsumgebung an ( application-docker.properties ). Konfiguriere die Datenbankverbindung so, dass sie den Docker-Netzwerknamen verwendet.","title":"3. Anpassung der Anwendungskonfiguration"},{"location":"Assignments/5XHBGM/NodeContainer/#4-testen-und-dokumentation","text":"Teste deine Dockerisierte Anwendung lokal. Dokumentiere die notwendigen Schritte zum Starten der Anwendung mit Docker Compose. Beschreibe eventuell aufgetretene Probleme und deren L\u00f6sungen. Zeige dass die Daten auch nach einem Neustart der Container noch vorhanden sind","title":"4. Testen und Dokumentation"},{"location":"Assignments/5XHBGM/NodeContainer/#abgabe","text":"Dein Protokoll soll enthalten: Dockerfile docker-compose.yml Angepasste Anwendungskonfigurationsdateien Kurze Dokumentation (max. 1 Seite des Protokolls) mit Erkl\u00e4rungen und Anweisungen zum Starten der Anwendung","title":"Abgabe"},{"location":"Assignments/5XHBGM/NodeContainer/#beurteilungskriterien","text":"Docker Volumes werden verwendet und beschrieben Umgebungsvariablen, geladen aus einer .env-Datei werden geladen","title":"Beurteilungskriterien"},{"location":"Assignments/5XHBGM/NodeContainer/#haufige-probleme-und-losungen","text":"Container startet nicht: Logs pr\u00fcfen: sudo docker compose logs Build-Logs pr\u00fcfen: sudo docker compose build --no-cache Ports pr\u00fcfen: sudo netstat -tulpn Statische Assets werden nicht geladen: Verifizieren Sie die Build-Ausgabe: sudo docker exec -it <container-id> ls /<path> Performance-Probleme: Container Ressourcen \u00fcberpr\u00fcfen: sudo docker stats System-Ressourcen monitoren (nur Linux): htop \u00c4nderungen werden nicht \u00fcbernommen Container muss bei jeder \u00c4nderung neu gebaut werden Start mit docker compose up -d --build","title":"H\u00e4ufige Probleme und L\u00f6sungen:"},{"location":"Assignments/5XHBGM/NodeContainer/node/","text":"Ausgangssituation Du hast eine bestehende Angular-Anwendung, die das Frontend f\u00fcr eine Webanwendung bereitstellt. Bisher l\u00e4uft diese Anwendung lokal auf deinem Entwicklungsrechner. Ziel Deine Aufgabe ist es, die Angular-Anwendung in einen Docker-Container zu verpacken, sodass sie einfach deployed und in verschiedenen Umgebungen ausgef\u00fchrt werden kann. Anforderungen 1. Vorbereitung der Angular-Anwendung Stelle sicher, dass deine Angular-Anwendung fertig entwickelt und lauff\u00e4hig ist. Es sollte keine Warnungen oder Fehler in der Konsole am Server geben Auch keine in der Javascript Konsole im Browser Optimiere die Anwendung f\u00fcr den Produktivbetrieb (Production Build).c Siehe ng build 2. Erstellung des Dockerfiles Erstelle ein Dockerfile f\u00fcr deine Angular-Anwendung. Verwende ein geeignetes Basis-Image (z.B. node:lts f\u00fcr den Build-Prozess und nginx:alpine f\u00fcr das Bereitstellen der statischen Dateien). Implementiere einen mehrstufigen Build-Prozess (Multi-stage build): Stage 1: Baue die Angular-Anwendung mit Node.js. Stage 2: Kopiere die gebauten Dateien in ein nginx-Image zum Bereitstellen. 3. Konfiguration des Webservers Konfiguriere nginx, um deine Angular-Anwendung korrekt zu servieren. Stelle sicher, dass das Routing f\u00fcr Single Page Applications (SPA) korrekt funktioniert. 4. Optimierung des Docker-Images Minimiere die Gr\u00f6\u00dfe des finalen Docker-Images, z.B durch Nutzung von Alpine Implementiere Best Practices f\u00fcr Dockerfiles weniger Layer Nutzung von .dockerignore (selbes Prinzip wie .gitignore) 5. Testen und Dokumentation Teste deine dockerisierte Anwendung lokal. Dokumentiere die notwendigen Schritte zum Bauen und Starten des Containers. Beschreibe eventuell aufgetretene Probleme und deren L\u00f6sungen. Abgabe Im Protokoll soll enthalten sein: - Dockerfile - Angepasste nginx-Konfiguration (falls notwendig) - Kurze Dokumentation (max. 1 Seite des Protokolls) mit Erkl\u00e4rungen und Anweisungen zum Bauen und Starten des Containers Zusatzaufgabe (freiwillig) Implementiere einen einfachen Gesundheitscheck (Health Check) f\u00fcr deinen Container.","title":"Dockerisierung einer Angular-Frontend-Anwendung"},{"location":"Assignments/5XHBGM/NodeContainer/node/#ausgangssituation","text":"Du hast eine bestehende Angular-Anwendung, die das Frontend f\u00fcr eine Webanwendung bereitstellt. Bisher l\u00e4uft diese Anwendung lokal auf deinem Entwicklungsrechner.","title":"Ausgangssituation"},{"location":"Assignments/5XHBGM/NodeContainer/node/#ziel","text":"Deine Aufgabe ist es, die Angular-Anwendung in einen Docker-Container zu verpacken, sodass sie einfach deployed und in verschiedenen Umgebungen ausgef\u00fchrt werden kann.","title":"Ziel"},{"location":"Assignments/5XHBGM/NodeContainer/node/#anforderungen","text":"","title":"Anforderungen"},{"location":"Assignments/5XHBGM/NodeContainer/node/#1-vorbereitung-der-angular-anwendung","text":"Stelle sicher, dass deine Angular-Anwendung fertig entwickelt und lauff\u00e4hig ist. Es sollte keine Warnungen oder Fehler in der Konsole am Server geben Auch keine in der Javascript Konsole im Browser Optimiere die Anwendung f\u00fcr den Produktivbetrieb (Production Build).c Siehe ng build","title":"1. Vorbereitung der Angular-Anwendung"},{"location":"Assignments/5XHBGM/NodeContainer/node/#2-erstellung-des-dockerfiles","text":"Erstelle ein Dockerfile f\u00fcr deine Angular-Anwendung. Verwende ein geeignetes Basis-Image (z.B. node:lts f\u00fcr den Build-Prozess und nginx:alpine f\u00fcr das Bereitstellen der statischen Dateien). Implementiere einen mehrstufigen Build-Prozess (Multi-stage build): Stage 1: Baue die Angular-Anwendung mit Node.js. Stage 2: Kopiere die gebauten Dateien in ein nginx-Image zum Bereitstellen.","title":"2. Erstellung des Dockerfiles"},{"location":"Assignments/5XHBGM/NodeContainer/node/#3-konfiguration-des-webservers","text":"Konfiguriere nginx, um deine Angular-Anwendung korrekt zu servieren. Stelle sicher, dass das Routing f\u00fcr Single Page Applications (SPA) korrekt funktioniert.","title":"3. Konfiguration des Webservers"},{"location":"Assignments/5XHBGM/NodeContainer/node/#4-optimierung-des-docker-images","text":"Minimiere die Gr\u00f6\u00dfe des finalen Docker-Images, z.B durch Nutzung von Alpine Implementiere Best Practices f\u00fcr Dockerfiles weniger Layer Nutzung von .dockerignore (selbes Prinzip wie .gitignore)","title":"4. Optimierung des Docker-Images"},{"location":"Assignments/5XHBGM/NodeContainer/node/#5-testen-und-dokumentation","text":"Teste deine dockerisierte Anwendung lokal. Dokumentiere die notwendigen Schritte zum Bauen und Starten des Containers. Beschreibe eventuell aufgetretene Probleme und deren L\u00f6sungen.","title":"5. Testen und Dokumentation"},{"location":"Assignments/5XHBGM/NodeContainer/node/#abgabe","text":"Im Protokoll soll enthalten sein: - Dockerfile - Angepasste nginx-Konfiguration (falls notwendig) - Kurze Dokumentation (max. 1 Seite des Protokolls) mit Erkl\u00e4rungen und Anweisungen zum Bauen und Starten des Containers","title":"Abgabe"},{"location":"Assignments/5XHBGM/NodeContainer/node/#zusatzaufgabe-freiwillig","text":"Implementiere einen einfachen Gesundheitscheck (Health Check) f\u00fcr deinen Container.","title":"Zusatzaufgabe (freiwillig)"},{"location":"TeachingMaterials/AI/AngularGPT/angularCRUD-GPT/","text":"Angular Anwendung mit GPT erstellen Im folgenden habe ich den Chatverlauf zwischen mir und ChatGPT exportiert. Anweisungen von mir sind kursiv . Als JSON habe ich das Beispiel-JSON f\u00fcr einen Patienten von der FHIR-Dokumentationsseite genommen. Es funktioniert m\u00e4\u00dfig gut mit so gro\u00dfen Datenstrukturen. Mittendrin h\u00f6rt die KI einfach auf Text zu generieren. Auch macht sich der etwas \u00e4ltere Datenstand bemerkbar. Als ich am Schluss das package.json generiert habe verwendete es Angular 13. Use the latest Angular Version and build a form with the formBuilder that is able to represent the JSON I will provide for you. It is an example of a FHIR Patient. For each list in the JSON add a add and remove button. Also please use angular material. Here is the JSON: {\"resourceType\":\"Patient\",\"id\":\"example\",\"text\":{\"status\":\"generated\",\"div\":\"html goes here\"},\"identifier\":[{\"use\":\"usual\",\"type\":{\"coding\":[{\"system\":\"http://terminology.hl7.org/CodeSystem/v2-0203\",\"code\":\"MR\"}]},\"system\":\"urn:oid:1.2.36.146.595.217.0.1\",\"value\":\"12345\",\"period\":{\"start\":\"2001-05-06\"},\"assigner\":{\"display\":\"Acme Healthcare\"}}],\"active\":true,\"name\":[{\"use\":\"official\",\"family\":\"Chalmers\",\"given\":[\"Peter\",\"James\"]},{\"use\":\"usual\",\"given\":[\"Jim\"]},{\"use\":\"maiden\",\"family\":\"Windsor\",\"given\":[\"Peter\",\"James\"],\"period\":{\"end\":\"2002\"}}],\"telecom\":[{\"use\":\"home\"},{\"system\":\"phone\",\"value\":\"(03) 5555 6473\",\"use\":\"work\",\"rank\":1},{\"system\":\"phone\",\"value\":\"(03) 3410 5613\",\"use\":\"mobile\",\"rank\":2},{\"system\":\"phone\",\"value\":\"(03) 5555 8834\",\"use\":\"old\",\"period\":{\"end\":\"2014\"}}],\"gender\":\"male\",\"birthDate\":\"1974-12-25\",\"_birthDate\":{\"extension\":[{\"url\":\"http://hl7.org/fhir/StructureDefinition/patient-birthTime\",\"valueDateTime\":\"1974-12-25T14:35:45-05:00\"}]},\"deceasedBoolean\":false,\"address\":[{\"use\":\"home\",\"type\":\"both\",\"text\":\"534 Erewhon St PeasantVille, Rainbow, Vic 3999\",\"line\":[\"534 Erewhon St\"],\"city\":\"PleasantVille\",\"district\":\"Rainbow\",\"state\":\"Vic\",\"postalCode\":\"3999\",\"period\":{\"start\":\"1974-12-25\"}}],\"contact\":[{\"relationship\":[{\"coding\":[{\"system\":\"http://terminology.hl7.org/CodeSystem/v2-0131\",\"code\":\"N\"}]}],\"name\":{\"family\":\"du March\u00e9\",\"_family\":{\"extension\":[{\"url\":\"http://hl7.org/fhir/StructureDefinition/humanname-own-prefix\",\"valueString\":\"VV\"}]},\"given\":[\"B\u00e9n\u00e9dicte\"]},\"telecom\":[{\"system\":\"phone\",\"value\":\"+33 (237) 998327\"}],\"address\":{\"use\":\"home\",\"type\":\"both\",\"line\":[\"534 Erewhon St\"],\"city\":\"PleasantVille\",\"district\":\"Rainbow\",\"state\":\"Vic\",\"postalCode\":\"3999\",\"period\":{\"start\":\"1974-12-25\"}},\"gender\":\"female\",\"period\":{\"start\":\"2012\"}}],\"managingOrganization\":{\"reference\":\"Organization/1\"}} To build a form with Angular version 13 and Angular Material, we will need to create a new Angular project and install the necessary dependencies. Afterward, we will create a new component and use the FormBuilder class to create a reactive form that represents the provided JSON object. Here are the steps to follow: Create a new Angular project and navigate to the project directory: ng new angular-fhir-form cd angular-fhir-form Install the required dependencies: npm install @angular/material @angular/cdk @angular/animations npm install --save-dev @angular-devkit/build-angular Import the BrowserAnimationsModule and MatInputModule modules in the app module: import { BrowserAnimationsModule } from '@angular/platform-browser/animations'; import { MatInputModule } from '@angular/material/input'; @NgModule({ imports: [ BrowserAnimationsModule, MatInputModule ], declarations: [AppComponent], bootstrap: [AppComponent] }) export class AppModule {} Create a new component using the Angular CLI: ng generate component patient-form In the patient-form.component.ts file, import the FormBuilder class, the FormGroup and FormArray classes from @angular/forms, and the JSON object: import { Component } from '@angular/core'; import { FormBuilder, FormGroup, FormArray } from '@angular/forms'; import patientData from './patient.json'; Create the form using the FormBuilder class: @Component({ selector: 'app-patient-form', templateUrl: './patient-form.component.html', styleUrls: ['./patient-form.component.css'] }) export class PatientFormComponent { patientForm: FormGroup; constructor(private fb: FormBuilder) { this.patientForm = this.fb.group({ id: [patientData.id], identifier: this.fb.array([]), active: [patientData.active], name: this.fb.array([]), telecom: this.fb.array([]), gender: [patientData.gender], birthDate: [patientData.birthDate], deceasedBoolean: [patientData.deceasedBoolean], address: this.fb.array([]), contact: this.fb.array([]), managingOrganization: [patientData.managingOrganization.reference] }); } } Add the FormArray for the identifier, name, telecom, address, and contact fields and set their initial values: constructor(private fb: FormBuilder) { this.patientForm = this.fb.group({ id: [patientData.id], identifier: this.fb.array([ this.fb.group({ use: [patientData.identifier[0].use], type: this.fb.group({ coding: this.fb.array([ this.fb.group({ system: [patientData.identifier[0].type.coding[0].system], code: [patientData.identifier[0].type.coding[0].code] }) ]) }), system: [patientData.identifier[0].system], value: [patientData.identifier[0].value], period: this.fb.group({ start: [patientData.identifier[0].period.start] }), assigner: this.fb.group({ display: [patientData.identifier[0].assigner.display] }) }) ]), active: [patientData.active], name: this.fb.array([ this.fb.group({ use: [patientData.name[0].use], family: [patientData.name[0].family], given: this.fb.array([ [patientData.name[0].given[ continue Now that we have our Angular project set up and the necessary dependencies installed, we can start building our form using the FormBuilder and Angular Material. First, let's create a new component for our form. Run the following command in your terminal to generate a new component named \"patient-form\": ng generate component patient-form Next, let's open up the patient-form.component.ts file and start by importing the necessary modules and defining our patient form: import { Component, OnInit } from '@angular/core'; import { FormBuilder, FormGroup, FormArray } from '@angular/forms'; import { MatFormFieldModule } from '@angular/material/form-field'; import { MatInputModule } from '@angular/material/input'; import { MatButtonModule } from '@angular/material/button'; @Component({ selector: 'app-patient-form', templateUrl: './patient-form.component.html', styleUrls: ['./patient-form.component.css'] }) export class PatientFormComponent implements OnInit { patientForm: FormGroup; constructor(private fb: FormBuilder) { } ngOnInit() { this.patientForm = this.fb.group({ id: [''], active: [''], name: this.fb.array([]), identifier: this.fb.array([]), telecom: this.fb.array([]), gender: [''], birthDate: [''], deceasedBoolean: [''], address: this.fb.array([]), contact: this.fb.array([]), managingOrganization: [''] }); } } Here we have defined our patientForm as a FormGroup, and included FormArrays for the name, identifier, telecom, address, and contact fields, since each of these fields can have multiple values in the FHIR Patient resource. Next, let's add our HTML template to the patient-form.component.html file: <form [formGroup]=\"patientForm\"> <mat-form-field> <mat-label>ID</mat-label> <input matInput formControlName=\"id\"> </mat-form-field> <mat-form-field> <mat-label>Active</mat-label> <input matInput formControlName=\"active\"> </mat-form-field> <div formArrayName=\"name\"> <div *ngFor=\"let name of patientForm.get('name').controls; let i = index\"> <div [formGroupName]=\"i\"> <mat-form-field> <mat-label>Use</mat-label> <input matInput formControlName=\"use\"> </mat-form-field> <mat-form-field> <mat-label>Family</mat-label> <input matInput formControlName=\"family\"> </mat-form-field> <mat-form-field> <mat-label>Given</mat-label> <div formArrayName=\"given\"> <div *ngFor=\"let given of name.get('given').controls; let j = index\"> <input matInput [formControlName]=\"j\"> <button mat-icon-button color=\"warn\" *ngIf=\"j != 0\" (click)=\"removeGiven(i, j)\"> <mat-icon>remove_circle</mat-icon> </button> <button mat-icon-button color=\"primary\" *ngIf=\"j == name.get('given').length - 1\" (click)=\"addGiven(i)\"> <mat-icon>add_circle</mat-icon> </button> </div> </div> </mat-form-field> <button mat-icon-button color=\"warn\" *ngIf=\"i != 0\" (click)=\"removeName(i)\"> <mat-icon>remove_circle</mat-icon> </button> </div> </div> <button mat-icon-button color=\"primary\" (click whats the contents of the package.json? The contents of the package.json file typically include metadata about a Node.js project, as well as a list of its dependencies and scripts that can be run to perform various tasks. Here is an example of what a package.json file might look like for an Angular project: { \"name\": \"my-app\", \"version\": \"0.0.1\", \"scripts\": { \"ng\": \"ng\", \"start\": \"ng serve\", \"build\": \"ng build\", \"test\": \"ng test\", \"lint\": \"ng lint\", \"e2e\": \"ng e2e\" }, \"dependencies\": { \"@angular/animations\": \"~13.1.0\", \"@angular/common\": \"~13.1.0\", \"@angular/compiler\": \"~13.1.0\", \"@angular/core\": \"~13.1.0\", \"@angular/forms\": \"~13.1.0\", \"@angular/platform-browser\": \"~13.1.0\", \"@angular/platform-browser-dynamic\": \"~13.1.0\", \"@angular/router\": \"~13.1.0\", \"@fortawesome/fontawesome-free\": \"^5.15.4\", \"rxjs\": \"~7.5.3\", \"tslib\": \"^2.3.1\", \"zone.js\": \"~0.11.4\" }, \"devDependencies\": { \"@angular-devkit/build-angular\": \"~13.1.0\", \"@angular/cli\": \"~13.1.0\", \"@angular/compiler-cli\": \"~13.1.0\", \"@types/jasmine\": \"~3.10.0\", \"@types/node\": \"^16.11.15\", \"codelyzer\": \"^6.0.2\", \"jasmine-core\": \"~3.10.0\", \"jasmine-spec-reporter\": \"~7.0.0\", \"karma\": \"~6.3.8\", \"karma-chrome-launcher\": \"~3.1.0\", \"karma-coverage\": \"~2.1.0\", \"karma-jasmine\": \"~4.0.0\", \"karma-jasmine-html-reporter\": \"^1.7.0\", \"protractor\": \"~7.0.0\", \"ts-node\": \"~9.1.1\", \"tslint\": \"~6.1.3\", \"typescript\": \"~4.5.4\" } } This file specifies various dependencies that are required for the project, as well as scripts that can be run with the npm command to start the development server, build the project, run tests, and more.","title":"Angular Anwendung mit GPT erstellen"},{"location":"TeachingMaterials/AI/AngularGPT/angularCRUD-GPT/#angular-anwendung-mit-gpt-erstellen","text":"Im folgenden habe ich den Chatverlauf zwischen mir und ChatGPT exportiert. Anweisungen von mir sind kursiv . Als JSON habe ich das Beispiel-JSON f\u00fcr einen Patienten von der FHIR-Dokumentationsseite genommen. Es funktioniert m\u00e4\u00dfig gut mit so gro\u00dfen Datenstrukturen. Mittendrin h\u00f6rt die KI einfach auf Text zu generieren. Auch macht sich der etwas \u00e4ltere Datenstand bemerkbar. Als ich am Schluss das package.json generiert habe verwendete es Angular 13. Use the latest Angular Version and build a form with the formBuilder that is able to represent the JSON I will provide for you. It is an example of a FHIR Patient. For each list in the JSON add a add and remove button. Also please use angular material. Here is the JSON: {\"resourceType\":\"Patient\",\"id\":\"example\",\"text\":{\"status\":\"generated\",\"div\":\"html goes here\"},\"identifier\":[{\"use\":\"usual\",\"type\":{\"coding\":[{\"system\":\"http://terminology.hl7.org/CodeSystem/v2-0203\",\"code\":\"MR\"}]},\"system\":\"urn:oid:1.2.36.146.595.217.0.1\",\"value\":\"12345\",\"period\":{\"start\":\"2001-05-06\"},\"assigner\":{\"display\":\"Acme Healthcare\"}}],\"active\":true,\"name\":[{\"use\":\"official\",\"family\":\"Chalmers\",\"given\":[\"Peter\",\"James\"]},{\"use\":\"usual\",\"given\":[\"Jim\"]},{\"use\":\"maiden\",\"family\":\"Windsor\",\"given\":[\"Peter\",\"James\"],\"period\":{\"end\":\"2002\"}}],\"telecom\":[{\"use\":\"home\"},{\"system\":\"phone\",\"value\":\"(03) 5555 6473\",\"use\":\"work\",\"rank\":1},{\"system\":\"phone\",\"value\":\"(03) 3410 5613\",\"use\":\"mobile\",\"rank\":2},{\"system\":\"phone\",\"value\":\"(03) 5555 8834\",\"use\":\"old\",\"period\":{\"end\":\"2014\"}}],\"gender\":\"male\",\"birthDate\":\"1974-12-25\",\"_birthDate\":{\"extension\":[{\"url\":\"http://hl7.org/fhir/StructureDefinition/patient-birthTime\",\"valueDateTime\":\"1974-12-25T14:35:45-05:00\"}]},\"deceasedBoolean\":false,\"address\":[{\"use\":\"home\",\"type\":\"both\",\"text\":\"534 Erewhon St PeasantVille, Rainbow, Vic 3999\",\"line\":[\"534 Erewhon St\"],\"city\":\"PleasantVille\",\"district\":\"Rainbow\",\"state\":\"Vic\",\"postalCode\":\"3999\",\"period\":{\"start\":\"1974-12-25\"}}],\"contact\":[{\"relationship\":[{\"coding\":[{\"system\":\"http://terminology.hl7.org/CodeSystem/v2-0131\",\"code\":\"N\"}]}],\"name\":{\"family\":\"du March\u00e9\",\"_family\":{\"extension\":[{\"url\":\"http://hl7.org/fhir/StructureDefinition/humanname-own-prefix\",\"valueString\":\"VV\"}]},\"given\":[\"B\u00e9n\u00e9dicte\"]},\"telecom\":[{\"system\":\"phone\",\"value\":\"+33 (237) 998327\"}],\"address\":{\"use\":\"home\",\"type\":\"both\",\"line\":[\"534 Erewhon St\"],\"city\":\"PleasantVille\",\"district\":\"Rainbow\",\"state\":\"Vic\",\"postalCode\":\"3999\",\"period\":{\"start\":\"1974-12-25\"}},\"gender\":\"female\",\"period\":{\"start\":\"2012\"}}],\"managingOrganization\":{\"reference\":\"Organization/1\"}} To build a form with Angular version 13 and Angular Material, we will need to create a new Angular project and install the necessary dependencies. Afterward, we will create a new component and use the FormBuilder class to create a reactive form that represents the provided JSON object. Here are the steps to follow: Create a new Angular project and navigate to the project directory: ng new angular-fhir-form cd angular-fhir-form Install the required dependencies: npm install @angular/material @angular/cdk @angular/animations npm install --save-dev @angular-devkit/build-angular Import the BrowserAnimationsModule and MatInputModule modules in the app module: import { BrowserAnimationsModule } from '@angular/platform-browser/animations'; import { MatInputModule } from '@angular/material/input'; @NgModule({ imports: [ BrowserAnimationsModule, MatInputModule ], declarations: [AppComponent], bootstrap: [AppComponent] }) export class AppModule {} Create a new component using the Angular CLI: ng generate component patient-form In the patient-form.component.ts file, import the FormBuilder class, the FormGroup and FormArray classes from @angular/forms, and the JSON object: import { Component } from '@angular/core'; import { FormBuilder, FormGroup, FormArray } from '@angular/forms'; import patientData from './patient.json'; Create the form using the FormBuilder class: @Component({ selector: 'app-patient-form', templateUrl: './patient-form.component.html', styleUrls: ['./patient-form.component.css'] }) export class PatientFormComponent { patientForm: FormGroup; constructor(private fb: FormBuilder) { this.patientForm = this.fb.group({ id: [patientData.id], identifier: this.fb.array([]), active: [patientData.active], name: this.fb.array([]), telecom: this.fb.array([]), gender: [patientData.gender], birthDate: [patientData.birthDate], deceasedBoolean: [patientData.deceasedBoolean], address: this.fb.array([]), contact: this.fb.array([]), managingOrganization: [patientData.managingOrganization.reference] }); } } Add the FormArray for the identifier, name, telecom, address, and contact fields and set their initial values: constructor(private fb: FormBuilder) { this.patientForm = this.fb.group({ id: [patientData.id], identifier: this.fb.array([ this.fb.group({ use: [patientData.identifier[0].use], type: this.fb.group({ coding: this.fb.array([ this.fb.group({ system: [patientData.identifier[0].type.coding[0].system], code: [patientData.identifier[0].type.coding[0].code] }) ]) }), system: [patientData.identifier[0].system], value: [patientData.identifier[0].value], period: this.fb.group({ start: [patientData.identifier[0].period.start] }), assigner: this.fb.group({ display: [patientData.identifier[0].assigner.display] }) }) ]), active: [patientData.active], name: this.fb.array([ this.fb.group({ use: [patientData.name[0].use], family: [patientData.name[0].family], given: this.fb.array([ [patientData.name[0].given[ continue Now that we have our Angular project set up and the necessary dependencies installed, we can start building our form using the FormBuilder and Angular Material. First, let's create a new component for our form. Run the following command in your terminal to generate a new component named \"patient-form\": ng generate component patient-form Next, let's open up the patient-form.component.ts file and start by importing the necessary modules and defining our patient form: import { Component, OnInit } from '@angular/core'; import { FormBuilder, FormGroup, FormArray } from '@angular/forms'; import { MatFormFieldModule } from '@angular/material/form-field'; import { MatInputModule } from '@angular/material/input'; import { MatButtonModule } from '@angular/material/button'; @Component({ selector: 'app-patient-form', templateUrl: './patient-form.component.html', styleUrls: ['./patient-form.component.css'] }) export class PatientFormComponent implements OnInit { patientForm: FormGroup; constructor(private fb: FormBuilder) { } ngOnInit() { this.patientForm = this.fb.group({ id: [''], active: [''], name: this.fb.array([]), identifier: this.fb.array([]), telecom: this.fb.array([]), gender: [''], birthDate: [''], deceasedBoolean: [''], address: this.fb.array([]), contact: this.fb.array([]), managingOrganization: [''] }); } } Here we have defined our patientForm as a FormGroup, and included FormArrays for the name, identifier, telecom, address, and contact fields, since each of these fields can have multiple values in the FHIR Patient resource. Next, let's add our HTML template to the patient-form.component.html file: <form [formGroup]=\"patientForm\"> <mat-form-field> <mat-label>ID</mat-label> <input matInput formControlName=\"id\"> </mat-form-field> <mat-form-field> <mat-label>Active</mat-label> <input matInput formControlName=\"active\"> </mat-form-field> <div formArrayName=\"name\"> <div *ngFor=\"let name of patientForm.get('name').controls; let i = index\"> <div [formGroupName]=\"i\"> <mat-form-field> <mat-label>Use</mat-label> <input matInput formControlName=\"use\"> </mat-form-field> <mat-form-field> <mat-label>Family</mat-label> <input matInput formControlName=\"family\"> </mat-form-field> <mat-form-field> <mat-label>Given</mat-label> <div formArrayName=\"given\"> <div *ngFor=\"let given of name.get('given').controls; let j = index\"> <input matInput [formControlName]=\"j\"> <button mat-icon-button color=\"warn\" *ngIf=\"j != 0\" (click)=\"removeGiven(i, j)\"> <mat-icon>remove_circle</mat-icon> </button> <button mat-icon-button color=\"primary\" *ngIf=\"j == name.get('given').length - 1\" (click)=\"addGiven(i)\"> <mat-icon>add_circle</mat-icon> </button> </div> </div> </mat-form-field> <button mat-icon-button color=\"warn\" *ngIf=\"i != 0\" (click)=\"removeName(i)\"> <mat-icon>remove_circle</mat-icon> </button> </div> </div> <button mat-icon-button color=\"primary\" (click whats the contents of the package.json? The contents of the package.json file typically include metadata about a Node.js project, as well as a list of its dependencies and scripts that can be run to perform various tasks. Here is an example of what a package.json file might look like for an Angular project: { \"name\": \"my-app\", \"version\": \"0.0.1\", \"scripts\": { \"ng\": \"ng\", \"start\": \"ng serve\", \"build\": \"ng build\", \"test\": \"ng test\", \"lint\": \"ng lint\", \"e2e\": \"ng e2e\" }, \"dependencies\": { \"@angular/animations\": \"~13.1.0\", \"@angular/common\": \"~13.1.0\", \"@angular/compiler\": \"~13.1.0\", \"@angular/core\": \"~13.1.0\", \"@angular/forms\": \"~13.1.0\", \"@angular/platform-browser\": \"~13.1.0\", \"@angular/platform-browser-dynamic\": \"~13.1.0\", \"@angular/router\": \"~13.1.0\", \"@fortawesome/fontawesome-free\": \"^5.15.4\", \"rxjs\": \"~7.5.3\", \"tslib\": \"^2.3.1\", \"zone.js\": \"~0.11.4\" }, \"devDependencies\": { \"@angular-devkit/build-angular\": \"~13.1.0\", \"@angular/cli\": \"~13.1.0\", \"@angular/compiler-cli\": \"~13.1.0\", \"@types/jasmine\": \"~3.10.0\", \"@types/node\": \"^16.11.15\", \"codelyzer\": \"^6.0.2\", \"jasmine-core\": \"~3.10.0\", \"jasmine-spec-reporter\": \"~7.0.0\", \"karma\": \"~6.3.8\", \"karma-chrome-launcher\": \"~3.1.0\", \"karma-coverage\": \"~2.1.0\", \"karma-jasmine\": \"~4.0.0\", \"karma-jasmine-html-reporter\": \"^1.7.0\", \"protractor\": \"~7.0.0\", \"ts-node\": \"~9.1.1\", \"tslint\": \"~6.1.3\", \"typescript\": \"~4.5.4\" } } This file specifies various dependencies that are required for the project, as well as scripts that can be run with the npm command to start the development server, build the project, run tests, and more.","title":"Angular Anwendung mit GPT erstellen"},{"location":"TeachingMaterials/AI/RAG/","text":"RAG mit Langchain RAG steht f\u00fcr Retrieval-Augmented Generation. Es ist ein Ansatz, der die Leistungsf\u00e4higkeit von gro\u00dfen Sprachmodellen (LLMs) mit der F\u00e4higkeit kombiniert, relevante Informationen aus externen Datenquellen abzurufen. Dies erm\u00f6glicht es dem Modell, pr\u00e4zisere und kontextuell relevantere Antworten zu generieren, indem es auf aktuelle und spezifische Informationen zugreift, anstatt sich nur auf das im Modell gespeicherte Wissen zu verlassen. RAG wird h\u00e4ufig in Anwendungen eingesetzt, bei denen es wichtig ist, aktuelle und genaue Informationen zu liefern, wie z.B. in Frage-Antwort-Systemen, Chatbots und anderen NLP-Anwendungen. RAG Tutorial von IBM Schritt-f\u00fcr-Schritt-Anleitung zur Umsetzung von RAG mit Langchain Schritt 1: Installation der ben\u00f6tigten Bibliotheken Stelle sicher, dass du die erforderlichen Bibliotheken installiert hast. Du kannst dies mit pip tun: pip install langchain pip install ollama pip install faiss-cpu # F\u00fcr die Vektorsuche Schritt 2: Initialisierung des Sprachmodells Importiere das Sprachmodell und initialisiere es: from langchain.llms import Ollama llm = Ollama(model='deepseek-r1') # kann auch ein anderes Modell sein Schritt 3: Einrichten der Datenquelle Bereite eine Datenquelle vor. Dies kann eine Datenbank, eine Dokumentation, Gesetzestexte etc. sein. Schritt 4: Implementierung der Retrieval-Komponente Verwende FAISS oder eine \u00e4hnliche Bibliothek, um eine Vektorsuche zu implementieren: from langchain.vectorstores import FAISS from langchain.embeddings import OllamaEmbeddings # Erstelle Embeddings f\u00fcr deine Daten embeddings = OllamaEmbeddings(model='nomic-embed-text') # hat nur 300MB vector_store = FAISS(embeddings) # F\u00fcge deine Daten zur Vektorsuche hinzu vector_store.add_texts([\"Dein Text hier\", \"Noch ein Text\"]) Schritt 5: Kombination von Retrieval und Generation Erstelle eine Pipeline, die die abgerufenen Informationen mit dem Sprachmodell kombiniert: from langchain.chains import RetrievalQA qa_chain = RetrievalQA(llm=llm, retriever=vector_store.as_retriever()) Schritt 6: Abfrage stellen Stelle eine Abfrage und erhalte eine Antwort: frage = \"Was ist RAG?\" antwort = qa_chain.run(frage) print(antwort) Schritt 7: Integration und Testen Integriere die RAG-Pipeline in deine Chatbot-Anwendung und teste sie gr\u00fcndlich, um sicherzustellen, dass sie wie erwartet funktioniert. Gib auch aus welche Quellen dem Chatbot von der Vektordatenbank zur Verf\u00fcgung gestellt wurden um nachverfolgen zu k\u00f6nnen wie er zu seinen Antworten kam.","title":"RAG mit Langchain"},{"location":"TeachingMaterials/AI/RAG/#rag-mit-langchain","text":"RAG steht f\u00fcr Retrieval-Augmented Generation. Es ist ein Ansatz, der die Leistungsf\u00e4higkeit von gro\u00dfen Sprachmodellen (LLMs) mit der F\u00e4higkeit kombiniert, relevante Informationen aus externen Datenquellen abzurufen. Dies erm\u00f6glicht es dem Modell, pr\u00e4zisere und kontextuell relevantere Antworten zu generieren, indem es auf aktuelle und spezifische Informationen zugreift, anstatt sich nur auf das im Modell gespeicherte Wissen zu verlassen. RAG wird h\u00e4ufig in Anwendungen eingesetzt, bei denen es wichtig ist, aktuelle und genaue Informationen zu liefern, wie z.B. in Frage-Antwort-Systemen, Chatbots und anderen NLP-Anwendungen. RAG Tutorial von IBM","title":"RAG mit Langchain"},{"location":"TeachingMaterials/AI/RAG/#schritt-fur-schritt-anleitung-zur-umsetzung-von-rag-mit-langchain","text":"","title":"Schritt-f\u00fcr-Schritt-Anleitung zur Umsetzung von RAG mit Langchain"},{"location":"TeachingMaterials/AI/RAG/#schritt-1-installation-der-benotigten-bibliotheken","text":"Stelle sicher, dass du die erforderlichen Bibliotheken installiert hast. Du kannst dies mit pip tun: pip install langchain pip install ollama pip install faiss-cpu # F\u00fcr die Vektorsuche","title":"Schritt 1: Installation der ben\u00f6tigten Bibliotheken"},{"location":"TeachingMaterials/AI/RAG/#schritt-2-initialisierung-des-sprachmodells","text":"Importiere das Sprachmodell und initialisiere es: from langchain.llms import Ollama llm = Ollama(model='deepseek-r1') # kann auch ein anderes Modell sein","title":"Schritt 2: Initialisierung des Sprachmodells"},{"location":"TeachingMaterials/AI/RAG/#schritt-3-einrichten-der-datenquelle","text":"Bereite eine Datenquelle vor. Dies kann eine Datenbank, eine Dokumentation, Gesetzestexte etc. sein.","title":"Schritt 3: Einrichten der Datenquelle"},{"location":"TeachingMaterials/AI/RAG/#schritt-4-implementierung-der-retrieval-komponente","text":"Verwende FAISS oder eine \u00e4hnliche Bibliothek, um eine Vektorsuche zu implementieren: from langchain.vectorstores import FAISS from langchain.embeddings import OllamaEmbeddings # Erstelle Embeddings f\u00fcr deine Daten embeddings = OllamaEmbeddings(model='nomic-embed-text') # hat nur 300MB vector_store = FAISS(embeddings) # F\u00fcge deine Daten zur Vektorsuche hinzu vector_store.add_texts([\"Dein Text hier\", \"Noch ein Text\"])","title":"Schritt 4: Implementierung der Retrieval-Komponente"},{"location":"TeachingMaterials/AI/RAG/#schritt-5-kombination-von-retrieval-und-generation","text":"Erstelle eine Pipeline, die die abgerufenen Informationen mit dem Sprachmodell kombiniert: from langchain.chains import RetrievalQA qa_chain = RetrievalQA(llm=llm, retriever=vector_store.as_retriever())","title":"Schritt 5: Kombination von Retrieval und Generation"},{"location":"TeachingMaterials/AI/RAG/#schritt-6-abfrage-stellen","text":"Stelle eine Abfrage und erhalte eine Antwort: frage = \"Was ist RAG?\" antwort = qa_chain.run(frage) print(antwort)","title":"Schritt 6: Abfrage stellen"},{"location":"TeachingMaterials/AI/RAG/#schritt-7-integration-und-testen","text":"Integriere die RAG-Pipeline in deine Chatbot-Anwendung und teste sie gr\u00fcndlich, um sicherzustellen, dass sie wie erwartet funktioniert. Gib auch aus welche Quellen dem Chatbot von der Vektordatenbank zur Verf\u00fcgung gestellt wurden um nachverfolgen zu k\u00f6nnen wie er zu seinen Antworten kam.","title":"Schritt 7: Integration und Testen"},{"location":"TeachingMaterials/AI/SafeapiGPT/spingSafeApi-GPT/","text":"Sichere Spring-Anwendung mit ChatGPT erstellen Im folgenden habe ich versucht die aktuelle \u00dcbung von GPT l\u00f6sen zu lassen. Es sind ein paar gute Codeteile rausgekommen. Jedoch h\u00f6rt die KI bei l\u00e4ngeren Antworten nach einer bestimmten Zeit einfach auf. Mit Folgefragen l\u00e4sst sich das Problem aber l\u00f6sen. Auch merkt man dass der Code nicht auf dem neuesten Stand ist. Create a spring boot application that statifies the following requirements: - Has a controller for user authentication that returns a token - Stores user data utilizing the UserDetails interface - Has an article enity with title and description that each user can own - With an endpoint the user is able to grant the permissions read, write, delete of the article entiy to any other user. this user is then able to read, update or delete the article with appropiate controller endpoints To satisfy the given requirements, you can create a Spring Boot application with the following steps: Set up a new Spring Boot project with the necessary dependencies by using a build tool like Maven or Gradle. For example, you can use Spring Initializr to create a new project with Spring Web, Spring Data JPA, and Spring Security dependencies. Create a User entity that implements the UserDetails interface. The User entity should have fields for username, password, and authorities (i.e., roles). @Entity @Table(name = \"users\") public class User implements UserDetails { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String username; private String password; @ElementCollection(fetch = FetchType.EAGER) private List<String> authorities; // constructors, getters and setters, and UserDetails methods } Create a UserRepository interface that extends the JpaRepository interface to handle CRUD operations for User entities. @Repository public interface UserRepository extends JpaRepository<User, Long> { User findByUsername(String username); } Configure Spring Security to use the UserRepository to authenticate users. @Configuration @EnableWebSecurity public class SecurityConfig extends WebSecurityConfigurerAdapter { @Autowired private UserRepository userRepository; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.userDetailsService(username -> userRepository.findByUsername(username)); } // other security configurations } Create a JWT authentication endpoint that returns a token when a user logs in. You can use the jjwt library to generate and verify JWT tokens. @RestController @RequestMapping(\"/auth\") public class AuthController { @Autowired private AuthenticationManager authenticationManager; @Autowired private UserRepository userRepository; @Autowired private JwtTokenUtil jwtTokenUtil; @PostMapping(\"/login\") public ResponseEntity<?> login(@RequestBody JwtRequest jwtRequest) throws Exception { authenticationManager.authenticate(new UsernamePasswordAuthenticationToken(jwtRequest.getUsername(), jwtRequest.getPassword())); final UserDetails userDetails = userRepository.findByUsername(jwtRequest.getUsername()); final String token = jwtTokenUtil.generateToken(userDetails); return ResponseEntity.ok(new JwtResponse(token)); } } Create an Article entity with fields for title, description, and owner. You can use a ManyToOne relationship to link the Article entity to the User entity. @Entity @Table(name = \"articles\") public class Article { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String title; private String description; @ManyToOne @JoinColumn(name = \"user_id\", referencedColumnName = \"id\") private User owner; // constructors, getters and setters } Create an ArticleRepository interface that extends the JpaRepository interface to handle CRUD operations for Article entities. @Repository public interface ArticleRepository extends JpaRepository<Article, Long> { } Create a Controller for managing articles. This Controller should have endpoints for creating, reading, updating, and deleting articles. The endpoints should be secured with Spring Security. @RestController @RequestMapping(\"/articles\") public class ArticleController { @Autowired private ArticleRepository articleRepository; @Autowired private UserRepository userRepository; @GetMapping(\"/{id}\") public ResponseEntity<Article> getArticle(@PathVariable Long id, Authentication authentication) { Article article = articleRepository.findById(id).orElseThrow(() -> new ResourceNotFoundException(\"Article\", \"id\", id)); User currentUser = userRepository.findByUsername(authentication.getName()); if (article.getOwner().getId().equals(currentUser.getId()) || currentUser.getAuthorities().contains(\"ROLE_ADMIN\")) { return ResponseEntity.ok(article); } else { return","title":"Sichere Spring-Anwendung mit ChatGPT erstellen"},{"location":"TeachingMaterials/AI/SafeapiGPT/spingSafeApi-GPT/#sichere-spring-anwendung-mit-chatgpt-erstellen","text":"Im folgenden habe ich versucht die aktuelle \u00dcbung von GPT l\u00f6sen zu lassen. Es sind ein paar gute Codeteile rausgekommen. Jedoch h\u00f6rt die KI bei l\u00e4ngeren Antworten nach einer bestimmten Zeit einfach auf. Mit Folgefragen l\u00e4sst sich das Problem aber l\u00f6sen. Auch merkt man dass der Code nicht auf dem neuesten Stand ist. Create a spring boot application that statifies the following requirements: - Has a controller for user authentication that returns a token - Stores user data utilizing the UserDetails interface - Has an article enity with title and description that each user can own - With an endpoint the user is able to grant the permissions read, write, delete of the article entiy to any other user. this user is then able to read, update or delete the article with appropiate controller endpoints To satisfy the given requirements, you can create a Spring Boot application with the following steps: Set up a new Spring Boot project with the necessary dependencies by using a build tool like Maven or Gradle. For example, you can use Spring Initializr to create a new project with Spring Web, Spring Data JPA, and Spring Security dependencies. Create a User entity that implements the UserDetails interface. The User entity should have fields for username, password, and authorities (i.e., roles). @Entity @Table(name = \"users\") public class User implements UserDetails { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String username; private String password; @ElementCollection(fetch = FetchType.EAGER) private List<String> authorities; // constructors, getters and setters, and UserDetails methods } Create a UserRepository interface that extends the JpaRepository interface to handle CRUD operations for User entities. @Repository public interface UserRepository extends JpaRepository<User, Long> { User findByUsername(String username); } Configure Spring Security to use the UserRepository to authenticate users. @Configuration @EnableWebSecurity public class SecurityConfig extends WebSecurityConfigurerAdapter { @Autowired private UserRepository userRepository; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.userDetailsService(username -> userRepository.findByUsername(username)); } // other security configurations } Create a JWT authentication endpoint that returns a token when a user logs in. You can use the jjwt library to generate and verify JWT tokens. @RestController @RequestMapping(\"/auth\") public class AuthController { @Autowired private AuthenticationManager authenticationManager; @Autowired private UserRepository userRepository; @Autowired private JwtTokenUtil jwtTokenUtil; @PostMapping(\"/login\") public ResponseEntity<?> login(@RequestBody JwtRequest jwtRequest) throws Exception { authenticationManager.authenticate(new UsernamePasswordAuthenticationToken(jwtRequest.getUsername(), jwtRequest.getPassword())); final UserDetails userDetails = userRepository.findByUsername(jwtRequest.getUsername()); final String token = jwtTokenUtil.generateToken(userDetails); return ResponseEntity.ok(new JwtResponse(token)); } } Create an Article entity with fields for title, description, and owner. You can use a ManyToOne relationship to link the Article entity to the User entity. @Entity @Table(name = \"articles\") public class Article { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String title; private String description; @ManyToOne @JoinColumn(name = \"user_id\", referencedColumnName = \"id\") private User owner; // constructors, getters and setters } Create an ArticleRepository interface that extends the JpaRepository interface to handle CRUD operations for Article entities. @Repository public interface ArticleRepository extends JpaRepository<Article, Long> { } Create a Controller for managing articles. This Controller should have endpoints for creating, reading, updating, and deleting articles. The endpoints should be secured with Spring Security. @RestController @RequestMapping(\"/articles\") public class ArticleController { @Autowired private ArticleRepository articleRepository; @Autowired private UserRepository userRepository; @GetMapping(\"/{id}\") public ResponseEntity<Article> getArticle(@PathVariable Long id, Authentication authentication) { Article article = articleRepository.findById(id).orElseThrow(() -> new ResourceNotFoundException(\"Article\", \"id\", id)); User currentUser = userRepository.findByUsername(authentication.getName()); if (article.getOwner().getId().equals(currentUser.getId()) || currentUser.getAuthorities().contains(\"ROLE_ADMIN\")) { return ResponseEntity.ok(article); } else { return","title":"Sichere Spring-Anwendung mit ChatGPT erstellen"},{"location":"TeachingMaterials/AI/ollama/hbgm/","text":"Einen Chatbot mit Ollama erstellen Ollama-Docker Container herunterladen - [Ollama auf Docker Hub](https://hub.docker.com/r/ollama/ollama) - Oder Ollama direkt installieren: [Ollama Downloads](https://ollama.com/download/windows) Wenn eine Nvidia Grafikkarte verbaut ist: Nvidia Container Toolkit Compose-Datei f\u00fcr Ollama mit GPU-Konfiguration services: ollama: image: ollama/ollama environment: - OLLAMA_ORIGINS=\"*\" deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: - gpu volumes: - ollama:/root/.ollama - ./scripts:/scripts ports: - 11434:11434 volumes: ollama: name: ollama Eine Shell des Containers \u00f6ffnen und Ollama starten C:/> docker compose exec ollama bash root@containerid# ollama ... Ein Modell herunterladen Nimm eine Multimodales Vision Modell aus deren Registry . ollama pull <modellname> Ollama Python-Schnittstelle Verf\u00fcgbar auf PyPi . \u00dcbergabe von Bildern an das Modell: siehe Blogpost Einfacher Chat: siehe Dokumentation Entwicklung einer Chat-Webanwendung Verwende Python f\u00fcr den Serverseitigen Code. Am Client ist JavaScript zu verwenden. Binde die Ollama Python Schnittstelle in ein Django-Projekt ein. Du kannst aber auch ein anderes Webframework verwenden, wenn du willst. Auf der Webseite soll es ein Eingabefeld geben sowie die M\u00f6glichkeit Bilder und PDFs hochzuladen. Wenn ein PDF hochgeladen wird, soll dieses als ein Bild je Seite an das Modell llama3.2-vision:11b \u00fcbergeben werden. Falls dein PC dieses Modell nicht ausf\u00fchren kann, kannst du auch ein kleineres nehmen. Moondream z.B. Die gestreamte Antwort des Models soll dann schrittweise am Client angezeigt werden, siehe Beispielprojekt Django , MDN Docs sowie Ollama Python Beispiel , bzw. Readable Streams . Antwortet der User, soll der gesamte Chatverlauf an das Model geschickt werden. Nur so kennt es den Kontext um bessere Antworten liefern zu k\u00f6nnen. Packe dazu das JSON und den CSRF-Token in ein FormData -Objekt. Die R\u00fcckmeldung des Modells soll dann dynamisch in einem div-Container hinzugef\u00fcgt werden. Vorgehensweise In den views: chat endpoint serve chat ui ensprechende urls f\u00fcr endpunkte templates: Erstelle eine chat ui in html+js+(css) compose siehe Punkt 3 models Model f\u00fcr speicher der Chat-Nachrichten Chat Message Session id message sender bzw. role timestamp manage.py makemigrations migrate uuid import uuid str(uuid.uuid4()) Weitere Informationen Ollama Github Bestehende Modelle in Ollama integrieren Functions as Tools","title":"Chatbot Ollama [HBGM]"},{"location":"TeachingMaterials/AI/ollama/hbgm/#einen-chatbot-mit-ollama-erstellen","text":"","title":"Einen Chatbot mit Ollama erstellen"},{"location":"TeachingMaterials/AI/ollama/hbgm/#ollama-docker-container-herunterladen","text":"- [Ollama auf Docker Hub](https://hub.docker.com/r/ollama/ollama) - Oder Ollama direkt installieren: [Ollama Downloads](https://ollama.com/download/windows)","title":"Ollama-Docker Container herunterladen"},{"location":"TeachingMaterials/AI/ollama/hbgm/#wenn-eine-nvidia-grafikkarte-verbaut-ist","text":"Nvidia Container Toolkit","title":"Wenn eine Nvidia Grafikkarte verbaut ist:"},{"location":"TeachingMaterials/AI/ollama/hbgm/#compose-datei-fur-ollama-mit-gpu-konfiguration","text":"services: ollama: image: ollama/ollama environment: - OLLAMA_ORIGINS=\"*\" deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: - gpu volumes: - ollama:/root/.ollama - ./scripts:/scripts ports: - 11434:11434 volumes: ollama: name: ollama","title":"Compose-Datei f\u00fcr Ollama mit GPU-Konfiguration"},{"location":"TeachingMaterials/AI/ollama/hbgm/#eine-shell-des-containers-offnen-und-ollama-starten","text":"C:/> docker compose exec ollama bash root@containerid# ollama ...","title":"Eine Shell des Containers \u00f6ffnen und Ollama starten"},{"location":"TeachingMaterials/AI/ollama/hbgm/#ein-modell-herunterladen","text":"Nimm eine Multimodales Vision Modell aus deren Registry . ollama pull <modellname>","title":"Ein Modell herunterladen"},{"location":"TeachingMaterials/AI/ollama/hbgm/#ollama-python-schnittstelle","text":"Verf\u00fcgbar auf PyPi . \u00dcbergabe von Bildern an das Modell: siehe Blogpost Einfacher Chat: siehe Dokumentation","title":"Ollama Python-Schnittstelle"},{"location":"TeachingMaterials/AI/ollama/hbgm/#entwicklung-einer-chat-webanwendung","text":"Verwende Python f\u00fcr den Serverseitigen Code. Am Client ist JavaScript zu verwenden. Binde die Ollama Python Schnittstelle in ein Django-Projekt ein. Du kannst aber auch ein anderes Webframework verwenden, wenn du willst. Auf der Webseite soll es ein Eingabefeld geben sowie die M\u00f6glichkeit Bilder und PDFs hochzuladen. Wenn ein PDF hochgeladen wird, soll dieses als ein Bild je Seite an das Modell llama3.2-vision:11b \u00fcbergeben werden. Falls dein PC dieses Modell nicht ausf\u00fchren kann, kannst du auch ein kleineres nehmen. Moondream z.B. Die gestreamte Antwort des Models soll dann schrittweise am Client angezeigt werden, siehe Beispielprojekt Django , MDN Docs sowie Ollama Python Beispiel , bzw. Readable Streams . Antwortet der User, soll der gesamte Chatverlauf an das Model geschickt werden. Nur so kennt es den Kontext um bessere Antworten liefern zu k\u00f6nnen. Packe dazu das JSON und den CSRF-Token in ein FormData -Objekt. Die R\u00fcckmeldung des Modells soll dann dynamisch in einem div-Container hinzugef\u00fcgt werden.","title":"Entwicklung einer Chat-Webanwendung"},{"location":"TeachingMaterials/AI/ollama/hbgm/#vorgehensweise","text":"","title":"Vorgehensweise"},{"location":"TeachingMaterials/AI/ollama/hbgm/#in-den-views","text":"chat endpoint serve chat ui ensprechende urls f\u00fcr endpunkte","title":"In den views:"},{"location":"TeachingMaterials/AI/ollama/hbgm/#templates","text":"Erstelle eine chat ui in html+js+(css)","title":"templates:"},{"location":"TeachingMaterials/AI/ollama/hbgm/#compose","text":"siehe Punkt 3","title":"compose"},{"location":"TeachingMaterials/AI/ollama/hbgm/#models","text":"Model f\u00fcr speicher der Chat-Nachrichten Chat Message Session id message sender bzw. role timestamp","title":"models"},{"location":"TeachingMaterials/AI/ollama/hbgm/#managepy","text":"makemigrations migrate","title":"manage.py"},{"location":"TeachingMaterials/AI/ollama/hbgm/#uuid","text":"import uuid str(uuid.uuid4())","title":"uuid"},{"location":"TeachingMaterials/AI/ollama/hbgm/#weitere-informationen","text":"Ollama Github Bestehende Modelle in Ollama integrieren Functions as Tools","title":"Weitere Informationen"},{"location":"TeachingMaterials/AI/ollama/hif/","text":"Docker Container herunterladen Ollama auf Docker Hub Wenn eine Nvidia Grafikkarte verbaut ist: Nvidia Container Toolkit Compose-Datei f\u00fcr Ollama mit GPU-Konfiguration services: ollama: image: ollama/ollama deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: - gpu volumes: - ollama:/root/.ollama - ./scripts:/scripts ports: - 11434:11434 volumes: ollama: Eine Shell des Containers \u00f6ffnen und Ollama starten C:/> docker compose exec ollama bash root@containerid# ollama ... Ein Modell herunterladen Nimm eine Multimodales Vision Modell aus deren Registry . ollama pull <modellname> Ollama Python-Schnittstelle Verf\u00fcgbar auf PyPi . \u00dcbergabe von Bildern an das Modell: siehe Blogpost Entwicklung einer Chat-Webanwendung Verwende Remix + React mit Material-UI zur Frontendentwicklung. Die Kommunikation mit dem Backend erfolgt \u00fcber die REST-API von Ollama. Folgenderma\u00dfen kannst du dir ein Projekt holen: curl https://codeload.github.com/mui/material-ui/tar.gz/master | tar -xz --strip=2 material-ui-master/examples/material-ui-remix-ts cd material-ui-remix-ts Auf der Webseite soll es ein Eingabefeld geben sowie die M\u00f6glichkeit, Bilder und PDFs hochzuladen. Wenn ein PDF hochgeladen wird, soll dieses als ein Bild je Seite \u00fcbergeben werden siehe blog . Die gestreamte Antwort des Modells siehe docs soll dann schrittweise angezeigt werden. Antwortet der User, soll der gesamte Chatverlauf siehe messages-Array an das Modell geschickt werden, damit es den Kontext kennt und bessere Antworten liefern kann. Weitere Informationen Material UI Installation Material UI Komponenten Ollama Github Bestehende Modelle in Ollama integrieren","title":"Einen Chatbot mit Ollama erstellen"},{"location":"TeachingMaterials/AI/ollama/hif/#weitere-informationen","text":"Material UI Installation Material UI Komponenten Ollama Github Bestehende Modelle in Ollama integrieren","title":"Weitere Informationen"},{"location":"TeachingMaterials/AI/whisper.cpp/docker-setup/","text":"Whisper.cpp Container verbessern Der container ghcr.io/ggerganov/whisper.cpp:main l\u00e4sst einige W\u00fcnsche an einen guten Container offen. Wenn man sich die Struktur des Containers mit docker history ghcr.io/ggerganov/whisper.cpp:main ansieht, bekommt man folgenden output: IMAGE CREATED CREATED BY SIZE COMMENT d10e2be76ea5 4 weeks ago ENTRYPOINT [\"bash\" \"-c\"] 0B buildkit.dockerfile.v0 <missing> 4 weeks ago COPY /app /app # buildkit 187MB buildkit.dockerfile.v0 <missing> 4 weeks ago RUN /bin/sh -c apt-get update && apt-get i\u2026 843MB buildkit.dockerfile.v0 <missing> 4 weeks ago WORKDIR /app 0B buildkit.dockerfile.v0 <missing> 2 months ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B <missing> 2 months ago /bin/sh -c #(nop) ADD file:1b6c8c9518be42fa2\u2026 77.9MB <missing> 2 months ago /bin/sh -c #(nop) LABEL org.opencontainers.\u2026 0B <missing> 2 months ago /bin/sh -c #(nop) LABEL org.opencontainers.\u2026 0B <missing> 2 months ago /bin/sh -c #(nop) ARG LAUNCHPAD_BUILD_ARCH 0B <missing> 2 months ago /bin/sh -c #(nop) ARG RELEASE 0B Dabei ist besonders der ENTRYPOINT bash spannend. Dieser Container kann also nur eine shell \u00f6ffnen. \u00d6ffnet man eine mit docker run -it ghcr.io/ggerganov/whisper.cpp:main bash hat man Zugriff auf den sourcecode von whisper.cpp. Vorbereitung Um jedoch ausf\u00fchrbare Dateien zu bekommen muss man diese SELBST compilieren. Um den Compiler herunterladen zu k\u00f6nnen muss man zun\u00e4chst die Paketquellen aktualisieren. apt update -y Dannach muss man den Compiler und andere Build-Tools installieren: apt install -y build-essential Sobald das erledigt ist kann man mit Cmake den Sourcecode compilieren: cmake -S /app/ -B build Im Ordner /app/build/bin werden dann mehrere ausf\u00fchrbare Dateien erstellt. Z.b whisper-cli f\u00fcr das transkribieren lokaler Dateien, oder whisper-server Diese k\u00f6nnen dann z.b mit dem Befehl ./build/whisper-cli gestart werden. F\u00fchrt man den Befehl /app/build/whisper-cli aus bekommt man folgende Ausgabe: error: no input files specified usage: ./whisper-cli [options] file0 file1 ... supported audio formats: flac, mp3, ogg, wav options: -h, --help [default] show this help message and exit -t N, --threads N [4 ] number of threads to use during computation -p N, --processors N [1 ] number of processors to use during computation -ot N, --offset-t N [0 ] time offset in milliseconds -on N, --offset-n N [0 ] segment index offset -d N, --duration N [0 ] duration of audio to process in milliseconds -mc N, --max-context N [-1 ] maximum number of text context tokens to store -ml N, --max-len N [0 ] maximum segment length in characters -sow, --split-on-word [false ] split on word rather than on token ... damit kann man dann arbeiten. Der Container ist also nicht wirklich f\u00fcr den produktiven Einsatz geeignet. Modelle Die Modelle sind in /app/models gespeichert. Diese werden mit dem Skript /app/model/download-ggml-model.sh heruntergeladen. F\u00fchrt man es direkt im Container aus, bekommt man folgenden Output: Usage: ./download-ggml-model.sh <model> [models_path] Available models: tiny tiny.en tiny-q5_1 tiny.en-q5_1 tiny-q8_0 base base.en base-q5_1 base.en-q5_1 base-q8_0 small small.en small.en-tdrz small-q5_1 small.en-q5_1 small-q8_0 medium medium.en medium-q5_0 medium.en-q5_0 medium-q8_0 large-v1 large-v2 large-v2-q5_0 large-v2-q8_0 large-v3 large-v3-q5_0 large-v3-turbo large-v3-turbo-q5_0 large-v3-turbo-q8_0 ____________________________________ .en = english-only -q5_[01] = quantized -tdrz = tinydiarize Aufgabe Erstelle einen neuen Container, der die Whisper.cpp CLI, Server und andere compilierte Dateien enth\u00e4lt. Erstelle ein Init-Skript welches, basierend auf den Umgebungsvariablen ein passendes Modell herunterl\u00e4dt. Achte in compose darauf dass dieses Modell dann auch in einem Volume gespeichert wird. Ansonsten wird der Container bei jedem Start das Modell neu herunterladen. Der Container soll dann die M\u00f6glichkeit bieten, eine Audio-Datei in Text umzuwandeln. Sobald das Init-Skript ausgef\u00fchrt wird, soll der Container die Audio-Datei in Text umwandeln und das Ergebnis auf der Konsole ausgeben. Beispiel f\u00fcr Compose version: '3.8' services: whisper: image: my-whisper.cpp-improved environment: - MODEL=medium volumes: - ./models:/app/models","title":"Whisper.cpp Container verbessern"},{"location":"TeachingMaterials/AI/whisper.cpp/docker-setup/#whispercpp-container-verbessern","text":"Der container ghcr.io/ggerganov/whisper.cpp:main l\u00e4sst einige W\u00fcnsche an einen guten Container offen. Wenn man sich die Struktur des Containers mit docker history ghcr.io/ggerganov/whisper.cpp:main ansieht, bekommt man folgenden output: IMAGE CREATED CREATED BY SIZE COMMENT d10e2be76ea5 4 weeks ago ENTRYPOINT [\"bash\" \"-c\"] 0B buildkit.dockerfile.v0 <missing> 4 weeks ago COPY /app /app # buildkit 187MB buildkit.dockerfile.v0 <missing> 4 weeks ago RUN /bin/sh -c apt-get update && apt-get i\u2026 843MB buildkit.dockerfile.v0 <missing> 4 weeks ago WORKDIR /app 0B buildkit.dockerfile.v0 <missing> 2 months ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B <missing> 2 months ago /bin/sh -c #(nop) ADD file:1b6c8c9518be42fa2\u2026 77.9MB <missing> 2 months ago /bin/sh -c #(nop) LABEL org.opencontainers.\u2026 0B <missing> 2 months ago /bin/sh -c #(nop) LABEL org.opencontainers.\u2026 0B <missing> 2 months ago /bin/sh -c #(nop) ARG LAUNCHPAD_BUILD_ARCH 0B <missing> 2 months ago /bin/sh -c #(nop) ARG RELEASE 0B Dabei ist besonders der ENTRYPOINT bash spannend. Dieser Container kann also nur eine shell \u00f6ffnen. \u00d6ffnet man eine mit docker run -it ghcr.io/ggerganov/whisper.cpp:main bash hat man Zugriff auf den sourcecode von whisper.cpp.","title":"Whisper.cpp Container verbessern"},{"location":"TeachingMaterials/AI/whisper.cpp/docker-setup/#vorbereitung","text":"Um jedoch ausf\u00fchrbare Dateien zu bekommen muss man diese SELBST compilieren. Um den Compiler herunterladen zu k\u00f6nnen muss man zun\u00e4chst die Paketquellen aktualisieren. apt update -y Dannach muss man den Compiler und andere Build-Tools installieren: apt install -y build-essential Sobald das erledigt ist kann man mit Cmake den Sourcecode compilieren: cmake -S /app/ -B build Im Ordner /app/build/bin werden dann mehrere ausf\u00fchrbare Dateien erstellt. Z.b whisper-cli f\u00fcr das transkribieren lokaler Dateien, oder whisper-server Diese k\u00f6nnen dann z.b mit dem Befehl ./build/whisper-cli gestart werden. F\u00fchrt man den Befehl /app/build/whisper-cli aus bekommt man folgende Ausgabe: error: no input files specified usage: ./whisper-cli [options] file0 file1 ... supported audio formats: flac, mp3, ogg, wav options: -h, --help [default] show this help message and exit -t N, --threads N [4 ] number of threads to use during computation -p N, --processors N [1 ] number of processors to use during computation -ot N, --offset-t N [0 ] time offset in milliseconds -on N, --offset-n N [0 ] segment index offset -d N, --duration N [0 ] duration of audio to process in milliseconds -mc N, --max-context N [-1 ] maximum number of text context tokens to store -ml N, --max-len N [0 ] maximum segment length in characters -sow, --split-on-word [false ] split on word rather than on token ... damit kann man dann arbeiten. Der Container ist also nicht wirklich f\u00fcr den produktiven Einsatz geeignet.","title":"Vorbereitung"},{"location":"TeachingMaterials/AI/whisper.cpp/docker-setup/#modelle","text":"Die Modelle sind in /app/models gespeichert. Diese werden mit dem Skript /app/model/download-ggml-model.sh heruntergeladen. F\u00fchrt man es direkt im Container aus, bekommt man folgenden Output: Usage: ./download-ggml-model.sh <model> [models_path] Available models: tiny tiny.en tiny-q5_1 tiny.en-q5_1 tiny-q8_0 base base.en base-q5_1 base.en-q5_1 base-q8_0 small small.en small.en-tdrz small-q5_1 small.en-q5_1 small-q8_0 medium medium.en medium-q5_0 medium.en-q5_0 medium-q8_0 large-v1 large-v2 large-v2-q5_0 large-v2-q8_0 large-v3 large-v3-q5_0 large-v3-turbo large-v3-turbo-q5_0 large-v3-turbo-q8_0 ____________________________________ .en = english-only -q5_[01] = quantized -tdrz = tinydiarize","title":"Modelle"},{"location":"TeachingMaterials/AI/whisper.cpp/docker-setup/#aufgabe","text":"Erstelle einen neuen Container, der die Whisper.cpp CLI, Server und andere compilierte Dateien enth\u00e4lt. Erstelle ein Init-Skript welches, basierend auf den Umgebungsvariablen ein passendes Modell herunterl\u00e4dt. Achte in compose darauf dass dieses Modell dann auch in einem Volume gespeichert wird. Ansonsten wird der Container bei jedem Start das Modell neu herunterladen. Der Container soll dann die M\u00f6glichkeit bieten, eine Audio-Datei in Text umzuwandeln. Sobald das Init-Skript ausgef\u00fchrt wird, soll der Container die Audio-Datei in Text umwandeln und das Ergebnis auf der Konsole ausgeben.","title":"Aufgabe"},{"location":"TeachingMaterials/AI/whisper.cpp/docker-setup/#beispiel-fur-compose","text":"version: '3.8' services: whisper: image: my-whisper.cpp-improved environment: - MODEL=medium volumes: - ./models:/app/models","title":"Beispiel f\u00fcr Compose"},{"location":"TeachingMaterials/AI/whisper.cpp/recorder/","text":"Anleitung zur Erstellung eines Sprachrekorders mit Whisper.cpp und Docker Schritte Projektstruktur erstellen Erstelle die folgenden Dateien und Verzeichnisse: docker-compose.yml init.sh /record (Endpunkt in Remix) nginx.conf samples/ (Verzeichnis) Docker Compose Datei erstellen Erstelle eine docker-compose.yml Datei mit dem notwendigen Services f\u00fcr Remix und Whisper.cpp. Beispielinhalt: ```yml services: remix: (freiwillig) whispercuda: image: ghcr.io/ggerganov/whisper.cpp:main-cuda stdin_open: true tty: true deploy: resources: reservations: devices: - capabilities: [gpu] runtime: nvidia entrypoint: /app/server command: --host 0.0.0.0 -l auto --convert -m /app/models/ggml-medium-q5_0.bin environment: - NVIDIA_VISIBLE_DEVICES=all - NVIDIA_DRIVER_CAPABILITIES=compute,utility ports: - \"8080:8080\" # Change this to the necessary port mapping volumes: - models:/app/models # Mount the named volume to the desired path - ./samples/:/app/samples volumes: models: ``` Initialisierung gehe in den Container mit docker compose exec whispercuda bash Lade ein Model: sh /app/model/download-ggml-model.sh medium-q5_0 Andere Modellgr\u00f6\u00dfen wie small oder large sind ebenfalls m\u00f6glich Teste ob eine wav-Datei im container zu Text transkribiert werden kann: ```sh # convert mp3 to wav and 16khz ffmpeg -i samples/ . -ar 16000 samples/test16khz.wav run a testfile in german using the medium model and a 16khz sample rate ./main -l de -m /app/models/ggml-medium-q5_0.bin samples/test16khz.wav ``` HTML-Datei f\u00fcr die Weboberfl\u00e4che erstellen Erstelle eine Remix-Route, um eine einfache Weboberfl\u00e4che f\u00fcr die Sprachaufnahme zu bieten. Beispiel mit reinem JS: ```js let mediaRecorder; let audioChunks = []; document.getElementById('recordButton').addEventListener('click', async () => { const stream = await navigator.mediaDevices.getUserMedia({ audio: true }); mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' }); mediaRecorder.start(); mediaRecorder.addEventListener('dataavailable', event => { audioChunks.push(event.data); }); mediaRecorder.addEventListener('stop', () => { const audioBlob = new Blob(audioChunks, { type: 'audio/webm' }); const audioFile = new File([audioBlob], 'recording.wav', { type: 'audio/wav' }); sendAudioToWhisper(audioFile); }); document.getElementById('recordButton').disabled = true; document.getElementById('stopButton').disabled = false; document.getElementById('status').textContent = 'Recording...'; }); document.getElementById('stopButton').addEventListener('click', () => { mediaRecorder.stop(); document.getElementById('recordButton').disabled = false; document.getElementById('stopButton').disabled = true; document.getElementById('status').textContent = 'Stopped recording.'; }); async function sendAudioToWhisper(audioFile) { const formData = new FormData(); formData.append('file', audioFile); formData.append('temperature', '0.0'); formData.append('temperature_inc', '0.2'); formData.append('response_format', 'json'); const response = await fetch('/inference', { method: 'POST', body: formData }); const result = await response.json(); document.getElementById('response').textContent = JSON.stringify(result, null, 2); } ``` Docker Compose starten F\u00fchre den folgenden Befehl aus, um die Docker-Container zu starten: sh docker-compose up -d Weboberfl\u00e4che verwenden \u00d6ffne einen Webbrowser und navigiere zu http://localhost/record , um die Sprachrekorder-Weboberfl\u00e4che zu verwenden. Klicke auf \"Start\", um die Aufnahme zu beginnen, und auf \"Stop\", um die Aufnahme zu beenden und die Datei an den Whisper.cpp Service zu senden. Hinweise Stelle sicher, dass alle Pfade korrekt sind und die notwendigen Dateien vorhanden sind. \u00dcberpr\u00fcfe die Docker-Logs, falls Probleme auftreten: sh docker-compose logs -f","title":"Whisper.cpp und Docker"},{"location":"TeachingMaterials/AI/whisper.cpp/recorder/#anleitung-zur-erstellung-eines-sprachrekorders-mit-whispercpp-und-docker","text":"","title":"Anleitung zur Erstellung eines Sprachrekorders mit Whisper.cpp und Docker"},{"location":"TeachingMaterials/AI/whisper.cpp/recorder/#schritte","text":"Projektstruktur erstellen Erstelle die folgenden Dateien und Verzeichnisse: docker-compose.yml init.sh /record (Endpunkt in Remix) nginx.conf samples/ (Verzeichnis) Docker Compose Datei erstellen Erstelle eine docker-compose.yml Datei mit dem notwendigen Services f\u00fcr Remix und Whisper.cpp. Beispielinhalt: ```yml services: remix: (freiwillig) whispercuda: image: ghcr.io/ggerganov/whisper.cpp:main-cuda stdin_open: true tty: true deploy: resources: reservations: devices: - capabilities: [gpu] runtime: nvidia entrypoint: /app/server command: --host 0.0.0.0 -l auto --convert -m /app/models/ggml-medium-q5_0.bin environment: - NVIDIA_VISIBLE_DEVICES=all - NVIDIA_DRIVER_CAPABILITIES=compute,utility ports: - \"8080:8080\" # Change this to the necessary port mapping volumes: - models:/app/models # Mount the named volume to the desired path - ./samples/:/app/samples volumes: models: ``` Initialisierung gehe in den Container mit docker compose exec whispercuda bash Lade ein Model: sh /app/model/download-ggml-model.sh medium-q5_0 Andere Modellgr\u00f6\u00dfen wie small oder large sind ebenfalls m\u00f6glich Teste ob eine wav-Datei im container zu Text transkribiert werden kann: ```sh # convert mp3 to wav and 16khz ffmpeg -i samples/ . -ar 16000 samples/test16khz.wav","title":"Schritte"},{"location":"TeachingMaterials/AI/whisper.cpp/recorder/#run-a-testfile-in-german-using-the-medium-model-and-a-16khz-sample-rate","text":"./main -l de -m /app/models/ggml-medium-q5_0.bin samples/test16khz.wav ``` HTML-Datei f\u00fcr die Weboberfl\u00e4che erstellen Erstelle eine Remix-Route, um eine einfache Weboberfl\u00e4che f\u00fcr die Sprachaufnahme zu bieten. Beispiel mit reinem JS: ```js let mediaRecorder; let audioChunks = []; document.getElementById('recordButton').addEventListener('click', async () => { const stream = await navigator.mediaDevices.getUserMedia({ audio: true }); mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' }); mediaRecorder.start(); mediaRecorder.addEventListener('dataavailable', event => { audioChunks.push(event.data); }); mediaRecorder.addEventListener('stop', () => { const audioBlob = new Blob(audioChunks, { type: 'audio/webm' }); const audioFile = new File([audioBlob], 'recording.wav', { type: 'audio/wav' }); sendAudioToWhisper(audioFile); }); document.getElementById('recordButton').disabled = true; document.getElementById('stopButton').disabled = false; document.getElementById('status').textContent = 'Recording...'; }); document.getElementById('stopButton').addEventListener('click', () => { mediaRecorder.stop(); document.getElementById('recordButton').disabled = false; document.getElementById('stopButton').disabled = true; document.getElementById('status').textContent = 'Stopped recording.'; }); async function sendAudioToWhisper(audioFile) { const formData = new FormData(); formData.append('file', audioFile); formData.append('temperature', '0.0'); formData.append('temperature_inc', '0.2'); formData.append('response_format', 'json'); const response = await fetch('/inference', { method: 'POST', body: formData }); const result = await response.json(); document.getElementById('response').textContent = JSON.stringify(result, null, 2); } ``` Docker Compose starten F\u00fchre den folgenden Befehl aus, um die Docker-Container zu starten: sh docker-compose up -d Weboberfl\u00e4che verwenden \u00d6ffne einen Webbrowser und navigiere zu http://localhost/record , um die Sprachrekorder-Weboberfl\u00e4che zu verwenden. Klicke auf \"Start\", um die Aufnahme zu beginnen, und auf \"Stop\", um die Aufnahme zu beenden und die Datei an den Whisper.cpp Service zu senden.","title":"run a testfile in german using the medium model and a 16khz sample rate"},{"location":"TeachingMaterials/AI/whisper.cpp/recorder/#hinweise","text":"Stelle sicher, dass alle Pfade korrekt sind und die notwendigen Dateien vorhanden sind. \u00dcberpr\u00fcfe die Docker-Logs, falls Probleme auftreten: sh docker-compose logs -f","title":"Hinweise"},{"location":"TeachingMaterials/DevOps/AzureAnleitung/azure_anleitung/","text":"Azure Deployment VM Anlegen Auf portal.azure.com mit der Spengergasse-Email anmelden WICHTIG: W\u00e4hrend der Anmeldung niemals Kreditkartendaten angeben. Falls Azure das verlangt nochmal pr\u00fcfen ob ihr ein Studentenkonto erstellt habt. Zum Virtuelle Computer Men\u00fc gehen Auf erstellen klicken, dannach auf Azure VM W\u00e4hle eine bestehende Ressourcengruppe aus oder erstelle ggf. eine neue. Leg einen beliebigen Namen fest. Am besten mit deinem K\u00fcrzel oder intialen um doppelte Benennungen zu vermeiden. Erstelle eine VM entweder mit Ubuntu oder alternativ mit einer Distribution mit der du dich gut auskennst. Zum authentifizieren ist ein SSH-Schl\u00fcssel empfehlenswert da man sich kein Passwort merken muss. Bei den Eingangsport brauchen wir alle verf\u00fcgbaren Ports. \u00dcber Port 80 werden wir sp\u00e4ter auf unsere Webanwendung zugreifen. \u00dcber Port 22 k\u00f6nnen wir eine Konsole der VM erreichen. Klicke dann auf \u00dcberpr\u00fcfen + erstellen die Zusammenfassung sollte folgenderma\u00dfen aussehen: WICHTIG beim Klick auf Erstellen erstellt Azure uns einen privaten Schl\u00fcssel in einer pem-Datei. Diesen brauchen wir um uns einloggen zu k\u00f6nnen. Unbedingt herunterladen! In der \u00dcbersichtsseite k\u00f6nnen wir die akteulle IP unserer VM einsehen: Mit folgendem Befehl k\u00f6nnen wir uns jetzt in eine Konsole der VM einloggen: ssh -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> In meinem Fall ( bei dir anders ) sieht der Befehl so aus: ssh -i .\\misVM_key.pem azureuser@20.224.92.12 Erscheint eine Konsole mit gr\u00fcnem Text, hat der Login funktioniert. Daten \u00fcbertragen in die VM Im n\u00e4chsten Schritt m\u00fcssen wir user Projekt in die VM \u00fcbertragen. Dazu eignet sich der Befehl sftp . \u00c4hnlich wie bei SSH ist die Befehlsstruktur hier: sftp -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> Dannach erschreint der Promt sftp> welcher FTP-Befehle entgegen nimmt. Mit put <dateiname>.zip k\u00f6nnen wir eine Datei von einem lokalen Ordner in die VM verschieben. Nachdem die Projektdateien verschoben sind k\u00f6nnen wir uns wieder mit SSH auf der VM einloggen und mit der Installation beginnen. Zun\u00e4chst sollte man wie bei jeder neuen Linux-Installation ein update durchf\u00fchren. Das funktionier unter Ubuntu mit sudo apt-get update -y Ist das Update fertig k\u00f6nnen wir diverse Programme installieren. Wir ben\u00f6tigen docker und compose f\u00fcrs Ausf\u00fchren sowie unzip um die zip-Datei zu entpacken. Als kleines Hilfsprogramm noch dos2unix. sudo apt install docker docker-compose unzip dos2unix -y Nun k\u00f6nnen wir mit unzip <dateiname>.zip unser Projekt auspacken. Um in den darin erstellen Ordner zu wechseln gibt es das Kommando cd <dein-projektordner> . compose ausf\u00fchren Das funktioniert - wie unter Windows auch - mit docker-compose up . Innerhalb des containers wird dann das Dockerfile aufgebaut und ausgef\u00fchrt. Port 8080 in compose ummappen Wie in einem oberen Screenshot ersichtlich werden in Azure nur die Ports 22, 80 und 443 ver\u00f6ffentlicht. Da unsere Spring-Anwendung jedoch auf dem Port 8080 arbeitet m\u00fcssen wir den Netzwerkverkehr umleiten. Durch ver\u00e4ndern der docker-compose.yml-Datei ist das ganz einfach m\u00f6glich. Wie der innere auf den \u00e4u\u00dferen Port umgeleitet wird, wird immer foldenderma\u00dfen angegeben: <aussenport>:<innenport> . Statt 8080:8080 m\u00fcssen wir also 80:8080 schreiben. Webservice aufrufen Rufe die aktuelle IP (oder in meinem Fall habe ich eine Domain hinterlegt) auf und sieh nach ob eine Antwort zur\u00fcckkommt. Bei mir wird folgendes JSON retourniert:","title":"Azure Anleitung"},{"location":"TeachingMaterials/DevOps/AzureAnleitung/azure_anleitung/#azure-deployment","text":"","title":"Azure Deployment"},{"location":"TeachingMaterials/DevOps/AzureAnleitung/azure_anleitung/#vm-anlegen","text":"Auf portal.azure.com mit der Spengergasse-Email anmelden WICHTIG: W\u00e4hrend der Anmeldung niemals Kreditkartendaten angeben. Falls Azure das verlangt nochmal pr\u00fcfen ob ihr ein Studentenkonto erstellt habt. Zum Virtuelle Computer Men\u00fc gehen Auf erstellen klicken, dannach auf Azure VM W\u00e4hle eine bestehende Ressourcengruppe aus oder erstelle ggf. eine neue. Leg einen beliebigen Namen fest. Am besten mit deinem K\u00fcrzel oder intialen um doppelte Benennungen zu vermeiden. Erstelle eine VM entweder mit Ubuntu oder alternativ mit einer Distribution mit der du dich gut auskennst. Zum authentifizieren ist ein SSH-Schl\u00fcssel empfehlenswert da man sich kein Passwort merken muss. Bei den Eingangsport brauchen wir alle verf\u00fcgbaren Ports. \u00dcber Port 80 werden wir sp\u00e4ter auf unsere Webanwendung zugreifen. \u00dcber Port 22 k\u00f6nnen wir eine Konsole der VM erreichen. Klicke dann auf \u00dcberpr\u00fcfen + erstellen die Zusammenfassung sollte folgenderma\u00dfen aussehen: WICHTIG beim Klick auf Erstellen erstellt Azure uns einen privaten Schl\u00fcssel in einer pem-Datei. Diesen brauchen wir um uns einloggen zu k\u00f6nnen. Unbedingt herunterladen! In der \u00dcbersichtsseite k\u00f6nnen wir die akteulle IP unserer VM einsehen: Mit folgendem Befehl k\u00f6nnen wir uns jetzt in eine Konsole der VM einloggen: ssh -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> In meinem Fall ( bei dir anders ) sieht der Befehl so aus: ssh -i .\\misVM_key.pem azureuser@20.224.92.12 Erscheint eine Konsole mit gr\u00fcnem Text, hat der Login funktioniert.","title":"VM Anlegen"},{"location":"TeachingMaterials/DevOps/AzureAnleitung/azure_anleitung/#daten-ubertragen-in-die-vm","text":"Im n\u00e4chsten Schritt m\u00fcssen wir user Projekt in die VM \u00fcbertragen. Dazu eignet sich der Befehl sftp . \u00c4hnlich wie bei SSH ist die Befehlsstruktur hier: sftp -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> Dannach erschreint der Promt sftp> welcher FTP-Befehle entgegen nimmt. Mit put <dateiname>.zip k\u00f6nnen wir eine Datei von einem lokalen Ordner in die VM verschieben. Nachdem die Projektdateien verschoben sind k\u00f6nnen wir uns wieder mit SSH auf der VM einloggen und mit der Installation beginnen. Zun\u00e4chst sollte man wie bei jeder neuen Linux-Installation ein update durchf\u00fchren. Das funktionier unter Ubuntu mit sudo apt-get update -y Ist das Update fertig k\u00f6nnen wir diverse Programme installieren. Wir ben\u00f6tigen docker und compose f\u00fcrs Ausf\u00fchren sowie unzip um die zip-Datei zu entpacken. Als kleines Hilfsprogramm noch dos2unix. sudo apt install docker docker-compose unzip dos2unix -y Nun k\u00f6nnen wir mit unzip <dateiname>.zip unser Projekt auspacken. Um in den darin erstellen Ordner zu wechseln gibt es das Kommando cd <dein-projektordner> .","title":"Daten \u00fcbertragen in die VM"},{"location":"TeachingMaterials/DevOps/AzureAnleitung/azure_anleitung/#compose-ausfuhren","text":"Das funktioniert - wie unter Windows auch - mit docker-compose up . Innerhalb des containers wird dann das Dockerfile aufgebaut und ausgef\u00fchrt.","title":"compose ausf\u00fchren"},{"location":"TeachingMaterials/DevOps/AzureAnleitung/azure_anleitung/#port-8080-in-compose-ummappen","text":"Wie in einem oberen Screenshot ersichtlich werden in Azure nur die Ports 22, 80 und 443 ver\u00f6ffentlicht. Da unsere Spring-Anwendung jedoch auf dem Port 8080 arbeitet m\u00fcssen wir den Netzwerkverkehr umleiten. Durch ver\u00e4ndern der docker-compose.yml-Datei ist das ganz einfach m\u00f6glich. Wie der innere auf den \u00e4u\u00dferen Port umgeleitet wird, wird immer foldenderma\u00dfen angegeben: <aussenport>:<innenport> . Statt 8080:8080 m\u00fcssen wir also 80:8080 schreiben.","title":"Port 8080 in compose ummappen"},{"location":"TeachingMaterials/DevOps/AzureAnleitung/azure_anleitung/#webservice-aufrufen","text":"Rufe die aktuelle IP (oder in meinem Fall habe ich eine Domain hinterlegt) auf und sieh nach ob eine Antwort zur\u00fcckkommt. Bei mir wird folgendes JSON retourniert:","title":"Webservice aufrufen"},{"location":"TeachingMaterials/DevOps/AzureAnleitungASP/azure_anleitungasp/","text":"Virtuellen Computer anlegen Auf portal.azure.com mit der Spengergasse-Email anmelden WICHTIG: W\u00e4hrend der Anmeldung niemals Kreditkartendaten angeben. Falls Azure das verlangt nochmal pr\u00fcfen ob ihr ein Studentenkonto erstellt habt. Zum Virtuelle Computer Men\u00fc gehen Auf erstellen klicken, dannach auf Azure VM W\u00e4hle eine bestehende Ressourcengruppe aus oder erstelle ggf. eine neue. Leg einen beliebigen Namen fest. Am besten mit deinem K\u00fcrzel oder intialen um doppelte Benennungen zu vermeiden. Erstelle eine VM entweder mit Ubuntu oder alternativ mit einer Distribution mit der du dich gut auskennst. Zum authentifizieren ist ein SSH-Schl\u00fcssel empfehlenswert da man sich kein Passwort merken muss. Bei den Eingangsport brauchen wir SSH (22) und HTTP (80). \u00dcber Port 80 werden wir sp\u00e4ter auf unsere Webanwendung zugreifen. \u00dcber Port 22 k\u00f6nnen wir eine Konsole der VM erreichen. Wenn du willst kannst du auch versuchen HTTPS einzurichten, ist aber f\u00fcr diese \u00dcbung nicht notwendig. Klicke dann auf \u00dcberpr\u00fcfen + erstellen die Zusammenfassung sollte folgenderma\u00dfen aussehen: Nachdem die Ressource erstellt wurde bekommt man folgende Best\u00e4tigung: WICHTIG beim Klick auf Erstellen erstellt Azure uns einen privaten Schl\u00fcssel in einer pem-Datei. Diesen brauchen wir um uns einloggen zu k\u00f6nnen. Unbedingt herunterladen! In der \u00dcbersichtsseite k\u00f6nnen wir zum einen die aktuelle IP unserer VM einsehen, als auch einen DNS-Namen festlegen: Mit folgendem Befehl k\u00f6nnen wir uns jetzt in eine Konsole der VM einloggen: ssh -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> In meinem Fall ( bei dir anders ) sieht der Befehl so aus: ssh -i asphost_key.pem azureuser@108.143.154.216 Bevor man sich zum ersten mal einloggt fragt einen SSH ob man der Gegenseite vertraut. The authenticity of host '108.143.154.216 (108.143.154.216)' can't be established. ED25519 key fingerprint is SHA256:gspN+LirQ+6X4C3m9EX1Fc6fbMxv/GQ66FPvyFnXYUU. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? Hier muss man yes eingeben bevor eine Verbindung aufgebaut wird. F\u00fcr den am Anfang h\u00e4ufigen Fehler Unprotected Key file gibt es hier: https://superuser.com/questions/1296024/windows-ssh-permissions-for-private-key-are-too-open L\u00f6sungsans\u00e4tze. Erscheint dann eine Konsole mit gr\u00fcnem Text, hat der Login funktioniert. F\u00fcr den n\u00e4chsten Schritt m\u00fcssen wir kurz zur\u00fcck auf den eigenen Rechner wechseln. Das geht mit dem Befehl exit . Daten \u00fcbertragen in die VM Im n\u00e4chsten Schritt m\u00fcssen wir user Projekt in die VM \u00fcbertragen. Dazu eignet sich der Befehl sftp . Der Name steht f\u00fcr SSH File Transfer Protocol. Da FTP unverschl\u00fcsselt ist sollte man davon absehen es in seiner normalen Form zu verwenden. \u00c4hnlich wie bei SSH ist die Befehlsstruktur hier: sftp -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> Dannach erschreint der Prompt sftp> welcher FTP-Befehle entgegen nimmt. Mit put <dateiname>.zip k\u00f6nnen wir eine Datei von einem lokalen Ordner in die VM verschieben. Sobald alles verschoben wurde kann man auch hier mit exit wieder das Programm verlassen. Nachdem die Projektdateien verschoben sind k\u00f6nnen wir uns wieder mit SSH auf der VM einloggen ( ssh -i ... ) und mit der Installation beginnen. Zun\u00e4chst sollte man wie bei jeder neuen Linux-Installation ein update durchf\u00fchren. Das funktioniert unter Ubuntu mit sudo apt-get update -y Ist das Update fertig k\u00f6nnen wir diverse Programme installieren. Wir ben\u00f6tigen Docker f\u00fcrs Ausf\u00fchren sowie unzip um die zip-Datei zu entpacken. Als kleines Hilfsprogramm noch dos2unix. sudo apt install unzip dos2unix -y sudo snap install docker Nun k\u00f6nnen wir mit unzip <dateiname>.zip unser Projekt auspacken. Um in den darin erstellen Ordner zu wechseln gibt es das Kommando cd <dein-projektordner> . Installation mit Docker Nachdem wir mit Docker noch nicht gearbeitet haben ist dieser Schritt freiwillig. Du kannst auch nur mit dotnet die Installation vornehmen. Es ist jedoch in der IT eine moderne und \u00fcbliche Vorgehensweise Docker zu verwenden. F\u00fcr ein Praktikum im Sommer o.\u00e4 w\u00e4re das ein gro\u00dfer Pluspunkt hier schon erste Erfahrungen zu haben. compose ausf\u00fchren Das funktioniert - wie unter Windows auch - mit docker-compose up . Innerhalb des Containers wird dann das Dockerfile aufgebaut und ausgef\u00fchrt. Port 8080 in compose ummappen Wie in einem oberen Screenshot ersichtlich werden in Azure nur die Ports 22, 80 und 443 ver\u00f6ffentlicht. Da unsere Anwendung jedoch auf dem Port 8080 arbeitet m\u00fcssen wir den Netzwerkverkehr umleiten. Durch ver\u00e4ndern der docker-compose.yml-Datei ist das ganz einfach m\u00f6glich. Wie der innere auf den \u00e4u\u00dferen Port umgeleitet wird, wird immer foldenderma\u00dfen angegeben: <aussenport>:<innenport> . Statt 8080:8080 m\u00fcssen wir also 80:8080 schreiben. So k\u00f6nnte in deinem Projektordner die Datei docker-compose.yml aussehen: version: \"3\" services: dotnet: build: . ports: - \"80:8080\" Installation mit dotnet So wie man unter Windows mit dem Kommando dotnet eine ASP-Anwendung ohne IDE ausf\u00fchren kann, geht das auch unter Linux. Wenn man nur einen Konsolen-Zugriff hat geht es auch garnicht anders. Das Programm dotnet muss aber erst mit dem Paketmanager installiert werden. Dazu gibt es das Kommando: sudo apt-get install -y dotnet-sdk-6.0 Quelle mit weiterf\u00fchrenden Anleitungen z.B f\u00fcr andere OS-Versionen bzw. f\u00fcr .NET 8 Mit dem Kommando: sudo dotnet run --urls=http://*:80/ kann man dann den Webserver starten. Dabei ist wichtig dass in dem Ordner in dem der Befehl ausgef\u00fchrt wird sich die .csproj -Datei befindet. Was genau macht dieser Befehl? Mit sudo (superuser do) wird der nachfolgende Befehl als Administrator ausgef\u00fchrt. dotnet run startet das ASP-Projekt. Genau der Befehl wird ausgef\u00fchrt wenn man in einer IDE auf den Play-Button klickt. Mit dem Parameter --urls wird festgelegt wie der Server erreichbar sein soll. http://*:80/ ist ein etwas komplizierterer Ausdruck. Mit http wird das \u00dcbertragungsprotokoll festgelegt. Der Stern * bedeutet dass der Name der Seite egal ist. Standardm\u00e4\u00dfig werden nur Anfragen beantwortet die an localhost gerichtet sind. Der Port 80 ist der Standardport von HTTP. Man k\u00f6nnte jede Seite mit :80 dahinter aufrufen. Standardm\u00e4\u00dfig kann man ihn aber auch weglassen. Webservice aufrufen Rufe die aktuelle IP (oder in meinem Fall habe ich eine Domain hinterlegt) auf und sieh nach ob eine Antwort zur\u00fcckkommt. Bei mir wird folgendes JSON retourniert:","title":"Azure Anleitung ASP"},{"location":"TeachingMaterials/DevOps/AzureAnleitungASP/azure_anleitungasp/#virtuellen-computer-anlegen","text":"Auf portal.azure.com mit der Spengergasse-Email anmelden WICHTIG: W\u00e4hrend der Anmeldung niemals Kreditkartendaten angeben. Falls Azure das verlangt nochmal pr\u00fcfen ob ihr ein Studentenkonto erstellt habt. Zum Virtuelle Computer Men\u00fc gehen Auf erstellen klicken, dannach auf Azure VM W\u00e4hle eine bestehende Ressourcengruppe aus oder erstelle ggf. eine neue. Leg einen beliebigen Namen fest. Am besten mit deinem K\u00fcrzel oder intialen um doppelte Benennungen zu vermeiden. Erstelle eine VM entweder mit Ubuntu oder alternativ mit einer Distribution mit der du dich gut auskennst. Zum authentifizieren ist ein SSH-Schl\u00fcssel empfehlenswert da man sich kein Passwort merken muss. Bei den Eingangsport brauchen wir SSH (22) und HTTP (80). \u00dcber Port 80 werden wir sp\u00e4ter auf unsere Webanwendung zugreifen. \u00dcber Port 22 k\u00f6nnen wir eine Konsole der VM erreichen. Wenn du willst kannst du auch versuchen HTTPS einzurichten, ist aber f\u00fcr diese \u00dcbung nicht notwendig. Klicke dann auf \u00dcberpr\u00fcfen + erstellen die Zusammenfassung sollte folgenderma\u00dfen aussehen: Nachdem die Ressource erstellt wurde bekommt man folgende Best\u00e4tigung: WICHTIG beim Klick auf Erstellen erstellt Azure uns einen privaten Schl\u00fcssel in einer pem-Datei. Diesen brauchen wir um uns einloggen zu k\u00f6nnen. Unbedingt herunterladen! In der \u00dcbersichtsseite k\u00f6nnen wir zum einen die aktuelle IP unserer VM einsehen, als auch einen DNS-Namen festlegen: Mit folgendem Befehl k\u00f6nnen wir uns jetzt in eine Konsole der VM einloggen: ssh -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> In meinem Fall ( bei dir anders ) sieht der Befehl so aus: ssh -i asphost_key.pem azureuser@108.143.154.216 Bevor man sich zum ersten mal einloggt fragt einen SSH ob man der Gegenseite vertraut. The authenticity of host '108.143.154.216 (108.143.154.216)' can't be established. ED25519 key fingerprint is SHA256:gspN+LirQ+6X4C3m9EX1Fc6fbMxv/GQ66FPvyFnXYUU. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? Hier muss man yes eingeben bevor eine Verbindung aufgebaut wird. F\u00fcr den am Anfang h\u00e4ufigen Fehler Unprotected Key file gibt es hier: https://superuser.com/questions/1296024/windows-ssh-permissions-for-private-key-are-too-open L\u00f6sungsans\u00e4tze. Erscheint dann eine Konsole mit gr\u00fcnem Text, hat der Login funktioniert. F\u00fcr den n\u00e4chsten Schritt m\u00fcssen wir kurz zur\u00fcck auf den eigenen Rechner wechseln. Das geht mit dem Befehl exit .","title":"Virtuellen Computer anlegen"},{"location":"TeachingMaterials/DevOps/AzureAnleitungASP/azure_anleitungasp/#daten-ubertragen-in-die-vm","text":"Im n\u00e4chsten Schritt m\u00fcssen wir user Projekt in die VM \u00fcbertragen. Dazu eignet sich der Befehl sftp . Der Name steht f\u00fcr SSH File Transfer Protocol. Da FTP unverschl\u00fcsselt ist sollte man davon absehen es in seiner normalen Form zu verwenden. \u00c4hnlich wie bei SSH ist die Befehlsstruktur hier: sftp -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> Dannach erschreint der Prompt sftp> welcher FTP-Befehle entgegen nimmt. Mit put <dateiname>.zip k\u00f6nnen wir eine Datei von einem lokalen Ordner in die VM verschieben. Sobald alles verschoben wurde kann man auch hier mit exit wieder das Programm verlassen. Nachdem die Projektdateien verschoben sind k\u00f6nnen wir uns wieder mit SSH auf der VM einloggen ( ssh -i ... ) und mit der Installation beginnen. Zun\u00e4chst sollte man wie bei jeder neuen Linux-Installation ein update durchf\u00fchren. Das funktioniert unter Ubuntu mit sudo apt-get update -y Ist das Update fertig k\u00f6nnen wir diverse Programme installieren. Wir ben\u00f6tigen Docker f\u00fcrs Ausf\u00fchren sowie unzip um die zip-Datei zu entpacken. Als kleines Hilfsprogramm noch dos2unix. sudo apt install unzip dos2unix -y sudo snap install docker Nun k\u00f6nnen wir mit unzip <dateiname>.zip unser Projekt auspacken. Um in den darin erstellen Ordner zu wechseln gibt es das Kommando cd <dein-projektordner> .","title":"Daten \u00fcbertragen in die VM"},{"location":"TeachingMaterials/DevOps/AzureAnleitungASP/azure_anleitungasp/#installation-mit-docker","text":"Nachdem wir mit Docker noch nicht gearbeitet haben ist dieser Schritt freiwillig. Du kannst auch nur mit dotnet die Installation vornehmen. Es ist jedoch in der IT eine moderne und \u00fcbliche Vorgehensweise Docker zu verwenden. F\u00fcr ein Praktikum im Sommer o.\u00e4 w\u00e4re das ein gro\u00dfer Pluspunkt hier schon erste Erfahrungen zu haben.","title":"Installation mit Docker"},{"location":"TeachingMaterials/DevOps/AzureAnleitungASP/azure_anleitungasp/#compose-ausfuhren","text":"Das funktioniert - wie unter Windows auch - mit docker-compose up . Innerhalb des Containers wird dann das Dockerfile aufgebaut und ausgef\u00fchrt.","title":"compose ausf\u00fchren"},{"location":"TeachingMaterials/DevOps/AzureAnleitungASP/azure_anleitungasp/#port-8080-in-compose-ummappen","text":"Wie in einem oberen Screenshot ersichtlich werden in Azure nur die Ports 22, 80 und 443 ver\u00f6ffentlicht. Da unsere Anwendung jedoch auf dem Port 8080 arbeitet m\u00fcssen wir den Netzwerkverkehr umleiten. Durch ver\u00e4ndern der docker-compose.yml-Datei ist das ganz einfach m\u00f6glich. Wie der innere auf den \u00e4u\u00dferen Port umgeleitet wird, wird immer foldenderma\u00dfen angegeben: <aussenport>:<innenport> . Statt 8080:8080 m\u00fcssen wir also 80:8080 schreiben. So k\u00f6nnte in deinem Projektordner die Datei docker-compose.yml aussehen: version: \"3\" services: dotnet: build: . ports: - \"80:8080\"","title":"Port 8080 in compose ummappen"},{"location":"TeachingMaterials/DevOps/AzureAnleitungASP/azure_anleitungasp/#installation-mit-dotnet","text":"So wie man unter Windows mit dem Kommando dotnet eine ASP-Anwendung ohne IDE ausf\u00fchren kann, geht das auch unter Linux. Wenn man nur einen Konsolen-Zugriff hat geht es auch garnicht anders. Das Programm dotnet muss aber erst mit dem Paketmanager installiert werden. Dazu gibt es das Kommando: sudo apt-get install -y dotnet-sdk-6.0 Quelle mit weiterf\u00fchrenden Anleitungen z.B f\u00fcr andere OS-Versionen bzw. f\u00fcr .NET 8 Mit dem Kommando: sudo dotnet run --urls=http://*:80/ kann man dann den Webserver starten. Dabei ist wichtig dass in dem Ordner in dem der Befehl ausgef\u00fchrt wird sich die .csproj -Datei befindet. Was genau macht dieser Befehl? Mit sudo (superuser do) wird der nachfolgende Befehl als Administrator ausgef\u00fchrt. dotnet run startet das ASP-Projekt. Genau der Befehl wird ausgef\u00fchrt wenn man in einer IDE auf den Play-Button klickt. Mit dem Parameter --urls wird festgelegt wie der Server erreichbar sein soll. http://*:80/ ist ein etwas komplizierterer Ausdruck. Mit http wird das \u00dcbertragungsprotokoll festgelegt. Der Stern * bedeutet dass der Name der Seite egal ist. Standardm\u00e4\u00dfig werden nur Anfragen beantwortet die an localhost gerichtet sind. Der Port 80 ist der Standardport von HTTP. Man k\u00f6nnte jede Seite mit :80 dahinter aufrufen. Standardm\u00e4\u00dfig kann man ihn aber auch weglassen.","title":"Installation mit dotnet"},{"location":"TeachingMaterials/DevOps/AzureAnleitungASP/azure_anleitungasp/#webservice-aufrufen","text":"Rufe die aktuelle IP (oder in meinem Fall habe ich eine Domain hinterlegt) auf und sieh nach ob eine Antwort zur\u00fcckkommt. Bei mir wird folgendes JSON retourniert:","title":"Webservice aufrufen"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/","text":"Virtuellen Computer anlegen Auf portal.azure.com mit der Spengergasse-Email anmelden WICHTIG: W\u00e4hrend der Anmeldung niemals Kreditkartendaten angeben. Falls Azure das verlangt nochmal pr\u00fcfen ob ihr ein Studentenkonto erstellt habt. Zum Virtuelle Computer Men\u00fc gehen Auf erstellen klicken, dannach auf Azure VM W\u00e4hle eine bestehende Ressourcengruppe aus oder erstelle ggf. eine neue. Leg einen beliebigen Namen fest. Am besten mit deinem K\u00fcrzel oder intialen um doppelte Benennungen zu vermeiden. Erstelle eine VM entweder mit Ubuntu oder alternativ mit einer Distribution mit der du dich gut auskennst. Zum authentifizieren ist ein SSH-Schl\u00fcssel empfehlenswert da man sich kein Passwort merken muss. Bei den Eingangsport brauchen wir SSH (22) und HTTP (80). \u00dcber Port 80 werden wir sp\u00e4ter auf unsere Webanwendung zugreifen. \u00dcber Port 22 k\u00f6nnen wir eine Konsole der VM erreichen. Wenn du willst kannst du auch versuchen HTTPS einzurichten, ist aber f\u00fcr diese \u00dcbung nicht notwendig. Klicke dann auf \u00dcberpr\u00fcfen + erstellen die Zusammenfassung sollte folgenderma\u00dfen aussehen: Nachdem die Ressource erstellt wurde bekommt man folgende Best\u00e4tigung: WICHTIG beim Klick auf Erstellen erstellt Azure uns einen privaten Schl\u00fcssel in einer pem-Datei. Diesen brauchen wir um uns einloggen zu k\u00f6nnen. Unbedingt herunterladen! In der \u00dcbersichtsseite k\u00f6nnen wir zum einen die aktuelle IP unserer VM einsehen, als auch einen DNS-Namen festlegen: Mit folgendem Befehl k\u00f6nnen wir uns jetzt in eine Konsole der VM einloggen: ssh -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> In meinem Fall ( bei dir anders ) sieht der Befehl so aus: ssh -i asphost_key.pem azureuser@108.143.154.216 Bevor man sich zum ersten mal einloggt fragt einen SSH ob man der Gegenseite vertraut. The authenticity of host '108.143.154.216 (108.143.154.216)' can't be established. ED25519 key fingerprint is SHA256:gspN+LirQ+6X4C3m9EX1Fc6fbMxv/GQ66FPvyFnXYUU. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? Hier muss man yes eingeben bevor eine Verbindung aufgebaut wird. F\u00fcr den am Anfang h\u00e4ufigen Fehler Unprotected Key file gibt es hier: https://superuser.com/questions/1296024/windows-ssh-permissions-for-private-key-are-too-open L\u00f6sungsans\u00e4tze. Erscheint dann eine Konsole mit gr\u00fcnem Text, hat der Login funktioniert. F\u00fcr den n\u00e4chsten Schritt m\u00fcssen wir kurz zur\u00fcck auf den eigenen Rechner wechseln. Das geht mit dem Befehl exit . Daten \u00fcbertragen in die VM Im n\u00e4chsten Schritt m\u00fcssen wir user Projekt in die VM \u00fcbertragen. Dazu eignet sich der Befehl sftp . Der Name steht f\u00fcr SSH File Transfer Protocol. Da FTP unverschl\u00fcsselt ist sollte man davon absehen es in seiner normalen Form zu verwenden. \u00c4hnlich wie bei SSH ist die Befehlsstruktur hier: sftp -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> Dannach erschreint der Prompt sftp> welcher FTP-Befehle entgegen nimmt. Mit put <dateiname>.zip k\u00f6nnen wir eine Datei von einem lokalen Ordner in die VM verschieben. Sobald alles verschoben wurde kann man auch hier mit exit wieder das Programm verlassen. Nachdem die Projektdateien verschoben sind k\u00f6nnen wir uns wieder mit SSH auf der VM einloggen ( ssh -i ... ) und mit der Installation beginnen. Zun\u00e4chst sollte man wie bei jeder neuen Linux-Installation ein update durchf\u00fchren. Das funktioniert unter Ubuntu mit sudo apt update -y Ist das Update fertig k\u00f6nnen wir diverse Programme installieren. Wir ben\u00f6tigen Docker f\u00fcrs Ausf\u00fchren sowie unzip um die zip-Datei zu entpacken. Als kleines Hilfsprogramm noch dos2unix. sudo apt install unzip dos2unix -y # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update sudo apt install docker-ce -y Quelle Nun k\u00f6nnen wir mit unzip <dateiname>.zip unser Projekt auspacken. Um in den darin erstellen Ordner zu wechseln gibt es das Kommando cd <dein-projektordner> . Installation Docker und Deployment mit Caddy Nachdem wir die Projektdateien auf die VM \u00fcbertragen haben, erstellen wir zun\u00e4chst die notwendigen Docker-Konfigurationsdateien f\u00fcr einen mehrstufigen Build-Prozess. Erstelle in deinem Projektordner ein Dockerfile : # Build stage # use latest lts node version FROM node:22-alpine as builder RUN apk add openssl WORKDIR /app COPY package*.json ./ RUN npm install COPY . . RUN npm run build # use latest lts node version FROM node:22-alpine RUN apk add openssl WORKDIR /app COPY package*.json ./ RUN npm ci COPY --from=builder /app/build ./build COPY --from=builder /app/public ./public COPY --from=builder /app/prisma ./prisma RUN npm run prisma generate EXPOSE 3000 CMD [\"npm\", \"run\", \"start\"] Dass prisma generate funktioniert musst du darauf achten, dass im package.json in den scripts folgender eintrag ist: \"scripts\": {\"prisma\": \"prisma\"} Um beim Befehl COPY . . nicht zu viele Daten zu \u00fcbertragen kannst du die Datei .dockerignore hinzuf\u00fcgen. Alle darin aufgelisteten Dateien und Ordner werden nicht reinkopiert. Syntax ist \u00e4hnlich wie bei der .gitignore . node_modules Erstelle eine Caddyfile im Projektordner: yourdomain.example.com { # Reverse proxy to Remix server reverse_proxy remix-app:3000 { header_up Host {host} header_up X-Real-IP {remote} header_up X-Forwarded-For {remote} header_up X-Forwarded-Proto {scheme} } # Enable compression encode gzip } Docker Compose f\u00fcr einfacheres Management: Erstelle ein docker-compose.yml : services: remix-app: build: . restart: unless-stopped networks: - remix_network environment: - NODE_ENV=production volumes: - ./prisma:/app/prisma env_file: .env caddy: image: caddy:2-alpine restart: unless-stopped ports: - \"80:80\" - \"443:443\" volumes: - ./Caddyfile:/etc/caddy/Caddyfile - caddy_data:/data - caddy_config:/config networks: - remix_network depends_on: - remix-app networks: remix_network: driver: bridge volumes: caddy_data: caddy_config: Deployment durchf\u00fchren: # Docker Compose starten sudo docker compose up -d --build Wartung und Monitoring Logs \u00fcberpr\u00fcfen: # Container Logs anzeigen sudo docker compose logs -f Container neustarten: sudo docker compose restart Updates durchf\u00fchren: Funktioniert nur wenn man statt sftp die Daten zwischen privatem Rechner und Azure mit git \u00fcbertr\u00e4gt. # Neue Version pullen und Container neustarten git pull sudo docker compose down sudo docker compose up --build -d Troubleshooting H\u00e4ufige Probleme und L\u00f6sungen: Container startet nicht: Logs pr\u00fcfen: sudo docker compose logs Build-Logs pr\u00fcfen: sudo docker compose build --no-cache Ports pr\u00fcfen: sudo netstat -tulpn Statische Assets werden nicht geladen: Pr\u00fcfen Sie die Pfade in der Caddyfile Verifizieren Sie die Build-Ausgabe: sudo docker exec -it <container-id> ls /srv Performance-Probleme: Container Ressourcen \u00fcberpr\u00fcfen: sudo docker stats System-Ressourcen monitoren: htop \u00c4nderungen werden nicht \u00fcbernommen Container muss bei jeder \u00c4nderung neu gebaut werden Start mit docker compose up -d --build HTTPS einrichten F\u00fcr eine produktive Umgebung mit eigener Domain, nutze folgendes Caddyfile: yourdomain.example.com { # Reverse proxy to Remix server reverse_proxy remix-app:3000 { header_up Host {host} header_up X-Real-IP {remote} header_up X-Forwarded-For {remote} header_up X-Forwarded-Proto {scheme} } # Enable compression encode gzip } Nach dem \u00c4ndern der Konfiguration: sudo docker compose restart Sicherheitshinweise Regelm\u00e4\u00dfige System-Updates: sudo apt update && sudo apt upgrade -y Docker-Images aktualisieren: sudo docker compose pull sudo docker compose up -d Backup der Volumes: docker run --rm \\ -v caddy_data:/data \\ -v $(pwd):/backup \\ alpine tar czf /backup/caddy_data_backup.tar.gz /data","title":"Azure Anleitung nodejs"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/#virtuellen-computer-anlegen","text":"Auf portal.azure.com mit der Spengergasse-Email anmelden WICHTIG: W\u00e4hrend der Anmeldung niemals Kreditkartendaten angeben. Falls Azure das verlangt nochmal pr\u00fcfen ob ihr ein Studentenkonto erstellt habt. Zum Virtuelle Computer Men\u00fc gehen Auf erstellen klicken, dannach auf Azure VM W\u00e4hle eine bestehende Ressourcengruppe aus oder erstelle ggf. eine neue. Leg einen beliebigen Namen fest. Am besten mit deinem K\u00fcrzel oder intialen um doppelte Benennungen zu vermeiden. Erstelle eine VM entweder mit Ubuntu oder alternativ mit einer Distribution mit der du dich gut auskennst. Zum authentifizieren ist ein SSH-Schl\u00fcssel empfehlenswert da man sich kein Passwort merken muss. Bei den Eingangsport brauchen wir SSH (22) und HTTP (80). \u00dcber Port 80 werden wir sp\u00e4ter auf unsere Webanwendung zugreifen. \u00dcber Port 22 k\u00f6nnen wir eine Konsole der VM erreichen. Wenn du willst kannst du auch versuchen HTTPS einzurichten, ist aber f\u00fcr diese \u00dcbung nicht notwendig. Klicke dann auf \u00dcberpr\u00fcfen + erstellen die Zusammenfassung sollte folgenderma\u00dfen aussehen: Nachdem die Ressource erstellt wurde bekommt man folgende Best\u00e4tigung: WICHTIG beim Klick auf Erstellen erstellt Azure uns einen privaten Schl\u00fcssel in einer pem-Datei. Diesen brauchen wir um uns einloggen zu k\u00f6nnen. Unbedingt herunterladen! In der \u00dcbersichtsseite k\u00f6nnen wir zum einen die aktuelle IP unserer VM einsehen, als auch einen DNS-Namen festlegen: Mit folgendem Befehl k\u00f6nnen wir uns jetzt in eine Konsole der VM einloggen: ssh -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> In meinem Fall ( bei dir anders ) sieht der Befehl so aus: ssh -i asphost_key.pem azureuser@108.143.154.216 Bevor man sich zum ersten mal einloggt fragt einen SSH ob man der Gegenseite vertraut. The authenticity of host '108.143.154.216 (108.143.154.216)' can't be established. ED25519 key fingerprint is SHA256:gspN+LirQ+6X4C3m9EX1Fc6fbMxv/GQ66FPvyFnXYUU. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? Hier muss man yes eingeben bevor eine Verbindung aufgebaut wird. F\u00fcr den am Anfang h\u00e4ufigen Fehler Unprotected Key file gibt es hier: https://superuser.com/questions/1296024/windows-ssh-permissions-for-private-key-are-too-open L\u00f6sungsans\u00e4tze. Erscheint dann eine Konsole mit gr\u00fcnem Text, hat der Login funktioniert. F\u00fcr den n\u00e4chsten Schritt m\u00fcssen wir kurz zur\u00fcck auf den eigenen Rechner wechseln. Das geht mit dem Befehl exit .","title":"Virtuellen Computer anlegen"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/#daten-ubertragen-in-die-vm","text":"Im n\u00e4chsten Schritt m\u00fcssen wir user Projekt in die VM \u00fcbertragen. Dazu eignet sich der Befehl sftp . Der Name steht f\u00fcr SSH File Transfer Protocol. Da FTP unverschl\u00fcsselt ist sollte man davon absehen es in seiner normalen Form zu verwenden. \u00c4hnlich wie bei SSH ist die Befehlsstruktur hier: sftp -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> Dannach erschreint der Prompt sftp> welcher FTP-Befehle entgegen nimmt. Mit put <dateiname>.zip k\u00f6nnen wir eine Datei von einem lokalen Ordner in die VM verschieben. Sobald alles verschoben wurde kann man auch hier mit exit wieder das Programm verlassen. Nachdem die Projektdateien verschoben sind k\u00f6nnen wir uns wieder mit SSH auf der VM einloggen ( ssh -i ... ) und mit der Installation beginnen. Zun\u00e4chst sollte man wie bei jeder neuen Linux-Installation ein update durchf\u00fchren. Das funktioniert unter Ubuntu mit sudo apt update -y Ist das Update fertig k\u00f6nnen wir diverse Programme installieren. Wir ben\u00f6tigen Docker f\u00fcrs Ausf\u00fchren sowie unzip um die zip-Datei zu entpacken. Als kleines Hilfsprogramm noch dos2unix. sudo apt install unzip dos2unix -y # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update sudo apt install docker-ce -y Quelle Nun k\u00f6nnen wir mit unzip <dateiname>.zip unser Projekt auspacken. Um in den darin erstellen Ordner zu wechseln gibt es das Kommando cd <dein-projektordner> .","title":"Daten \u00fcbertragen in die VM"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/#installation-docker-und-deployment-mit-caddy","text":"Nachdem wir die Projektdateien auf die VM \u00fcbertragen haben, erstellen wir zun\u00e4chst die notwendigen Docker-Konfigurationsdateien f\u00fcr einen mehrstufigen Build-Prozess. Erstelle in deinem Projektordner ein Dockerfile : # Build stage # use latest lts node version FROM node:22-alpine as builder RUN apk add openssl WORKDIR /app COPY package*.json ./ RUN npm install COPY . . RUN npm run build # use latest lts node version FROM node:22-alpine RUN apk add openssl WORKDIR /app COPY package*.json ./ RUN npm ci COPY --from=builder /app/build ./build COPY --from=builder /app/public ./public COPY --from=builder /app/prisma ./prisma RUN npm run prisma generate EXPOSE 3000 CMD [\"npm\", \"run\", \"start\"] Dass prisma generate funktioniert musst du darauf achten, dass im package.json in den scripts folgender eintrag ist: \"scripts\": {\"prisma\": \"prisma\"} Um beim Befehl COPY . . nicht zu viele Daten zu \u00fcbertragen kannst du die Datei .dockerignore hinzuf\u00fcgen. Alle darin aufgelisteten Dateien und Ordner werden nicht reinkopiert. Syntax ist \u00e4hnlich wie bei der .gitignore . node_modules Erstelle eine Caddyfile im Projektordner: yourdomain.example.com { # Reverse proxy to Remix server reverse_proxy remix-app:3000 { header_up Host {host} header_up X-Real-IP {remote} header_up X-Forwarded-For {remote} header_up X-Forwarded-Proto {scheme} } # Enable compression encode gzip } Docker Compose f\u00fcr einfacheres Management: Erstelle ein docker-compose.yml : services: remix-app: build: . restart: unless-stopped networks: - remix_network environment: - NODE_ENV=production volumes: - ./prisma:/app/prisma env_file: .env caddy: image: caddy:2-alpine restart: unless-stopped ports: - \"80:80\" - \"443:443\" volumes: - ./Caddyfile:/etc/caddy/Caddyfile - caddy_data:/data - caddy_config:/config networks: - remix_network depends_on: - remix-app networks: remix_network: driver: bridge volumes: caddy_data: caddy_config: Deployment durchf\u00fchren: # Docker Compose starten sudo docker compose up -d --build","title":"Installation Docker und Deployment mit Caddy"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/#wartung-und-monitoring","text":"Logs \u00fcberpr\u00fcfen: # Container Logs anzeigen sudo docker compose logs -f Container neustarten: sudo docker compose restart Updates durchf\u00fchren: Funktioniert nur wenn man statt sftp die Daten zwischen privatem Rechner und Azure mit git \u00fcbertr\u00e4gt. # Neue Version pullen und Container neustarten git pull sudo docker compose down sudo docker compose up --build -d","title":"Wartung und Monitoring"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/#haufige-probleme-und-losungen","text":"Container startet nicht: Logs pr\u00fcfen: sudo docker compose logs Build-Logs pr\u00fcfen: sudo docker compose build --no-cache Ports pr\u00fcfen: sudo netstat -tulpn Statische Assets werden nicht geladen: Pr\u00fcfen Sie die Pfade in der Caddyfile Verifizieren Sie die Build-Ausgabe: sudo docker exec -it <container-id> ls /srv Performance-Probleme: Container Ressourcen \u00fcberpr\u00fcfen: sudo docker stats System-Ressourcen monitoren: htop \u00c4nderungen werden nicht \u00fcbernommen Container muss bei jeder \u00c4nderung neu gebaut werden Start mit docker compose up -d --build","title":"H\u00e4ufige Probleme und L\u00f6sungen:"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/#https-einrichten","text":"F\u00fcr eine produktive Umgebung mit eigener Domain, nutze folgendes Caddyfile: yourdomain.example.com { # Reverse proxy to Remix server reverse_proxy remix-app:3000 { header_up Host {host} header_up X-Real-IP {remote} header_up X-Forwarded-For {remote} header_up X-Forwarded-Proto {scheme} } # Enable compression encode gzip } Nach dem \u00c4ndern der Konfiguration: sudo docker compose restart","title":"HTTPS einrichten"},{"location":"TeachingMaterials/DevOps/AzureAnleitungCaddy/azure_anleitungcaddy/#sicherheitshinweise","text":"Regelm\u00e4\u00dfige System-Updates: sudo apt update && sudo apt upgrade -y Docker-Images aktualisieren: sudo docker compose pull sudo docker compose up -d Backup der Volumes: docker run --rm \\ -v caddy_data:/data \\ -v $(pwd):/backup \\ alpine tar czf /backup/caddy_data_backup.tar.gz /data","title":"Sicherheitshinweise"},{"location":"TeachingMaterials/DevOps/AzureAnleitungNode/azure_anleitungnode/","text":"Virtuellen Computer anlegen Auf portal.azure.com mit der Spengergasse-Email anmelden WICHTIG: W\u00e4hrend der Anmeldung niemals Kreditkartendaten angeben. Falls Azure das verlangt nochmal pr\u00fcfen ob ihr ein Studentenkonto erstellt habt. Zum Virtuelle Computer Men\u00fc gehen Auf erstellen klicken, dannach auf Azure VM W\u00e4hle eine bestehende Ressourcengruppe aus oder erstelle ggf. eine neue. Leg einen beliebigen Namen fest. Am besten mit deinem K\u00fcrzel oder intialen um doppelte Benennungen zu vermeiden. Erstelle eine VM entweder mit Ubuntu oder alternativ mit einer Distribution mit der du dich gut auskennst. Zum authentifizieren ist ein SSH-Schl\u00fcssel empfehlenswert da man sich kein Passwort merken muss. Bei den Eingangsport brauchen wir SSH (22) und HTTP (80). \u00dcber Port 80 werden wir sp\u00e4ter auf unsere Webanwendung zugreifen. \u00dcber Port 22 k\u00f6nnen wir eine Konsole der VM erreichen. Wenn du willst kannst du auch versuchen HTTPS einzurichten, ist aber f\u00fcr diese \u00dcbung nicht notwendig. Klicke dann auf \u00dcberpr\u00fcfen + erstellen die Zusammenfassung sollte folgenderma\u00dfen aussehen: Nachdem die Ressource erstellt wurde bekommt man folgende Best\u00e4tigung: WICHTIG beim Klick auf Erstellen erstellt Azure uns einen privaten Schl\u00fcssel in einer pem-Datei. Diesen brauchen wir um uns einloggen zu k\u00f6nnen. Unbedingt herunterladen! In der \u00dcbersichtsseite k\u00f6nnen wir zum einen die aktuelle IP unserer VM einsehen, als auch einen DNS-Namen festlegen: Mit folgendem Befehl k\u00f6nnen wir uns jetzt in eine Konsole der VM einloggen: ssh -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> In meinem Fall ( bei dir anders ) sieht der Befehl so aus: ssh -i asphost_key.pem azureuser@108.143.154.216 Bevor man sich zum ersten mal einloggt fragt einen SSH ob man der Gegenseite vertraut. The authenticity of host '108.143.154.216 (108.143.154.216)' can't be established. ED25519 key fingerprint is SHA256:gspN+LirQ+6X4C3m9EX1Fc6fbMxv/GQ66FPvyFnXYUU. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? Hier muss man yes eingeben bevor eine Verbindung aufgebaut wird. F\u00fcr den am Anfang h\u00e4ufigen Fehler Unprotected Key file gibt es hier: https://superuser.com/questions/1296024/windows-ssh-permissions-for-private-key-are-too-open L\u00f6sungsans\u00e4tze. Erscheint dann eine Konsole mit gr\u00fcnem Text, hat der Login funktioniert. F\u00fcr den n\u00e4chsten Schritt m\u00fcssen wir kurz zur\u00fcck auf den eigenen Rechner wechseln. Das geht mit dem Befehl exit . Daten \u00fcbertragen in die VM Im n\u00e4chsten Schritt m\u00fcssen wir user Projekt in die VM \u00fcbertragen. Dazu eignet sich der Befehl sftp . Der Name steht f\u00fcr SSH File Transfer Protocol. Da FTP unverschl\u00fcsselt ist sollte man davon absehen es in seiner normalen Form zu verwenden. \u00c4hnlich wie bei SSH ist die Befehlsstruktur hier: sftp -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> Dannach erschreint der Prompt sftp> welcher FTP-Befehle entgegen nimmt. Mit put <dateiname>.zip k\u00f6nnen wir eine Datei von einem lokalen Ordner in die VM verschieben. Sobald alles verschoben wurde kann man auch hier mit exit wieder das Programm verlassen. Nachdem die Projektdateien verschoben sind k\u00f6nnen wir uns wieder mit SSH auf der VM einloggen ( ssh -i ... ) und mit der Installation beginnen. Zun\u00e4chst sollte man wie bei jeder neuen Linux-Installation ein update durchf\u00fchren. Das funktioniert unter Ubuntu mit sudo apt update -y Ist das Update fertig k\u00f6nnen wir diverse Programme installieren. Wir ben\u00f6tigen Docker f\u00fcrs Ausf\u00fchren sowie unzip um die zip-Datei zu entpacken. Als kleines Hilfsprogramm noch dos2unix. sudo apt install unzip dos2unix -y sudo snap install docker Nun k\u00f6nnen wir mit unzip <dateiname>.zip unser Projekt auspacken. Um in den darin erstellen Ordner zu wechseln gibt es das Kommando cd <dein-projektordner> . Installation mit nodejs So wie man unter Windows mit dem Kommando npm run ... bzw. npm start eine nodejs-Anwendung ohne IDE ausf\u00fchren kann, geht das auch unter Linux. Wenn man nur einen Konsolen-Zugriff hat geht es auch garnicht anders. Das Programm npm bzw. node muss aber erst installiert werden. Weil nodejs etwas langsam darin ist deren Pakete in den Paketmanager zu aktualisieren ist es besser man schaut sich an wie man von nodejs bereitgestellte Paketrepositories verwendet in denen immer die aktuelle Version enthalten ist. Eine Anleitung dazu gibt es hier (github.com/nodesource/distributions) . Mit dem Kommando: npm run dev kann man dann den Webserver starten. Dabei ist wichtig dass in dem Ordner in dem der Befehl ausgef\u00fchrt wird sich die package.json Datei befindet. Was genau macht dieser Befehl? Mit sudo (superuser do) wird der nachfolgende Befehl als Administrator ausgef\u00fchrt. npm run dev startet das Remix-Projekt. Genau der Befehl wird ausgef\u00fchrt wenn man in einer IDE auf den Play-Button klickt. Wichtig ist noch die Zone an zul\u00e4ssigen IPs und den Port zu konfigurieren. Diese Einstellungen m\u00fcssen in Vite vorgenommen werden. Ein Anleitung dazu gibt es hier (vitejs.dev/config) . Wichtig ist die Postion des server -Objekts: export default defineConfig({ // ... other config server: { host: \"0.0.0.0\", port: 80 } }); Der Port 80 ist der Standardport von HTTP. Man k\u00f6nnte jede Seite mit :80 dahinter aufrufen. Standardm\u00e4\u00dfig kann man ihn aber auch weglassen. Mit 0.0.0.0 erlaubt man jeder IP Adresse auf den Webserver zugreifen zu d\u00fcrfen. H\u00e4tte man 10.0.0.0 d\u00fcrften z.B nur IPs aus dem internen Netz Daten abfragen. Um zu pr\u00fcfen ob eine eingehende Verbindung erlaubt ist, wird der AND Operator auf die Maske ( 0.0.0.0 ) und die eingehende IP z.B 80.167.22.58 angewandt. Wenn das Ergebnis mit der Maske \u00fcbereinstimmt ist die Verbindung erlaubt. Nachdem jede Zahl kombiniert mit dem AND-Operator und einer 0 wieder 0 ergibt sind alle Verbindungen erlaubt. Manchmal sieht man auch die Schreibweise 0.0.0.0/0 . Die wird hier erkl\u00e4rt.","title":"Azure Anleitung caddy"},{"location":"TeachingMaterials/DevOps/AzureAnleitungNode/azure_anleitungnode/#virtuellen-computer-anlegen","text":"Auf portal.azure.com mit der Spengergasse-Email anmelden WICHTIG: W\u00e4hrend der Anmeldung niemals Kreditkartendaten angeben. Falls Azure das verlangt nochmal pr\u00fcfen ob ihr ein Studentenkonto erstellt habt. Zum Virtuelle Computer Men\u00fc gehen Auf erstellen klicken, dannach auf Azure VM W\u00e4hle eine bestehende Ressourcengruppe aus oder erstelle ggf. eine neue. Leg einen beliebigen Namen fest. Am besten mit deinem K\u00fcrzel oder intialen um doppelte Benennungen zu vermeiden. Erstelle eine VM entweder mit Ubuntu oder alternativ mit einer Distribution mit der du dich gut auskennst. Zum authentifizieren ist ein SSH-Schl\u00fcssel empfehlenswert da man sich kein Passwort merken muss. Bei den Eingangsport brauchen wir SSH (22) und HTTP (80). \u00dcber Port 80 werden wir sp\u00e4ter auf unsere Webanwendung zugreifen. \u00dcber Port 22 k\u00f6nnen wir eine Konsole der VM erreichen. Wenn du willst kannst du auch versuchen HTTPS einzurichten, ist aber f\u00fcr diese \u00dcbung nicht notwendig. Klicke dann auf \u00dcberpr\u00fcfen + erstellen die Zusammenfassung sollte folgenderma\u00dfen aussehen: Nachdem die Ressource erstellt wurde bekommt man folgende Best\u00e4tigung: WICHTIG beim Klick auf Erstellen erstellt Azure uns einen privaten Schl\u00fcssel in einer pem-Datei. Diesen brauchen wir um uns einloggen zu k\u00f6nnen. Unbedingt herunterladen! In der \u00dcbersichtsseite k\u00f6nnen wir zum einen die aktuelle IP unserer VM einsehen, als auch einen DNS-Namen festlegen: Mit folgendem Befehl k\u00f6nnen wir uns jetzt in eine Konsole der VM einloggen: ssh -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> In meinem Fall ( bei dir anders ) sieht der Befehl so aus: ssh -i asphost_key.pem azureuser@108.143.154.216 Bevor man sich zum ersten mal einloggt fragt einen SSH ob man der Gegenseite vertraut. The authenticity of host '108.143.154.216 (108.143.154.216)' can't be established. ED25519 key fingerprint is SHA256:gspN+LirQ+6X4C3m9EX1Fc6fbMxv/GQ66FPvyFnXYUU. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? Hier muss man yes eingeben bevor eine Verbindung aufgebaut wird. F\u00fcr den am Anfang h\u00e4ufigen Fehler Unprotected Key file gibt es hier: https://superuser.com/questions/1296024/windows-ssh-permissions-for-private-key-are-too-open L\u00f6sungsans\u00e4tze. Erscheint dann eine Konsole mit gr\u00fcnem Text, hat der Login funktioniert. F\u00fcr den n\u00e4chsten Schritt m\u00fcssen wir kurz zur\u00fcck auf den eigenen Rechner wechseln. Das geht mit dem Befehl exit .","title":"Virtuellen Computer anlegen"},{"location":"TeachingMaterials/DevOps/AzureAnleitungNode/azure_anleitungnode/#daten-ubertragen-in-die-vm","text":"Im n\u00e4chsten Schritt m\u00fcssen wir user Projekt in die VM \u00fcbertragen. Dazu eignet sich der Befehl sftp . Der Name steht f\u00fcr SSH File Transfer Protocol. Da FTP unverschl\u00fcsselt ist sollte man davon absehen es in seiner normalen Form zu verwenden. \u00c4hnlich wie bei SSH ist die Befehlsstruktur hier: sftp -i <dein-schl\u00fcsselname>.pem azureuser@<ip-deiner-vm> Dannach erschreint der Prompt sftp> welcher FTP-Befehle entgegen nimmt. Mit put <dateiname>.zip k\u00f6nnen wir eine Datei von einem lokalen Ordner in die VM verschieben. Sobald alles verschoben wurde kann man auch hier mit exit wieder das Programm verlassen. Nachdem die Projektdateien verschoben sind k\u00f6nnen wir uns wieder mit SSH auf der VM einloggen ( ssh -i ... ) und mit der Installation beginnen. Zun\u00e4chst sollte man wie bei jeder neuen Linux-Installation ein update durchf\u00fchren. Das funktioniert unter Ubuntu mit sudo apt update -y Ist das Update fertig k\u00f6nnen wir diverse Programme installieren. Wir ben\u00f6tigen Docker f\u00fcrs Ausf\u00fchren sowie unzip um die zip-Datei zu entpacken. Als kleines Hilfsprogramm noch dos2unix. sudo apt install unzip dos2unix -y sudo snap install docker Nun k\u00f6nnen wir mit unzip <dateiname>.zip unser Projekt auspacken. Um in den darin erstellen Ordner zu wechseln gibt es das Kommando cd <dein-projektordner> .","title":"Daten \u00fcbertragen in die VM"},{"location":"TeachingMaterials/DevOps/AzureAnleitungNode/azure_anleitungnode/#installation-mit-nodejs","text":"So wie man unter Windows mit dem Kommando npm run ... bzw. npm start eine nodejs-Anwendung ohne IDE ausf\u00fchren kann, geht das auch unter Linux. Wenn man nur einen Konsolen-Zugriff hat geht es auch garnicht anders. Das Programm npm bzw. node muss aber erst installiert werden. Weil nodejs etwas langsam darin ist deren Pakete in den Paketmanager zu aktualisieren ist es besser man schaut sich an wie man von nodejs bereitgestellte Paketrepositories verwendet in denen immer die aktuelle Version enthalten ist. Eine Anleitung dazu gibt es hier (github.com/nodesource/distributions) . Mit dem Kommando: npm run dev kann man dann den Webserver starten. Dabei ist wichtig dass in dem Ordner in dem der Befehl ausgef\u00fchrt wird sich die package.json Datei befindet. Was genau macht dieser Befehl? Mit sudo (superuser do) wird der nachfolgende Befehl als Administrator ausgef\u00fchrt. npm run dev startet das Remix-Projekt. Genau der Befehl wird ausgef\u00fchrt wenn man in einer IDE auf den Play-Button klickt. Wichtig ist noch die Zone an zul\u00e4ssigen IPs und den Port zu konfigurieren. Diese Einstellungen m\u00fcssen in Vite vorgenommen werden. Ein Anleitung dazu gibt es hier (vitejs.dev/config) . Wichtig ist die Postion des server -Objekts: export default defineConfig({ // ... other config server: { host: \"0.0.0.0\", port: 80 } }); Der Port 80 ist der Standardport von HTTP. Man k\u00f6nnte jede Seite mit :80 dahinter aufrufen. Standardm\u00e4\u00dfig kann man ihn aber auch weglassen. Mit 0.0.0.0 erlaubt man jeder IP Adresse auf den Webserver zugreifen zu d\u00fcrfen. H\u00e4tte man 10.0.0.0 d\u00fcrften z.B nur IPs aus dem internen Netz Daten abfragen. Um zu pr\u00fcfen ob eine eingehende Verbindung erlaubt ist, wird der AND Operator auf die Maske ( 0.0.0.0 ) und die eingehende IP z.B 80.167.22.58 angewandt. Wenn das Ergebnis mit der Maske \u00fcbereinstimmt ist die Verbindung erlaubt. Nachdem jede Zahl kombiniert mit dem AND-Operator und einer 0 wieder 0 ergibt sind alle Verbindungen erlaubt. Manchmal sieht man auch die Schreibweise 0.0.0.0/0 . Die wird hier erkl\u00e4rt.","title":"Installation mit nodejs"},{"location":"TeachingMaterials/DevOps/BegleitendeAufgaben/","text":"layout: page title: React Remix und GitHub OAuth menubar: false nav_exclude: true exclude: true nav: false","title":"Index"},{"location":"TeachingMaterials/DevOps/BegleitendeAufgaben/container/","text":"Erstelle ein Docker-Image f\u00fcr deine Anwendung, um sicherzustellen, dass sie in einer isolierten Umgebung l\u00e4uft. Gehe dabei wie folgt vor: Erstelle eine Dockerfile im Hauptverzeichnis deines Projekts. Beispiel f\u00fcr eine Node.js-Anwendung: dockerfile FROM node:16 WORKDIR /app COPY package*.json ./ RUN npm install COPY . . EXPOSE 3000 CMD [\"npm\", \"start\"] Baue das Docker-Image: sh docker build -t <dein-image-name> . Starte einen Container basierend auf deinem Image: sh docker run -p 3000:3000 <dein-image-name> Teste, ob deine Anwendung im Container wie erwartet funktioniert. Optional: Lade dein Docker-Image in eine Container-Registry wie Docker Hub oder Azure Container Registry hoch, um es sp\u00e4ter f\u00fcr das Deployment zu verwenden.","title":"Begleitende Aufgabe - Containerisierung"},{"location":"TeachingMaterials/DevOps/BegleitendeAufgaben/deployment/","text":"Nachdem deine Webanwendung nun lokal l\u00e4uft, sorge daf\u00fcr dass diese im Internet aufrufbar ist. Nutze daf\u00fcr die Azure App Services. Gehe dabei wie folgt vor: Melde dich im Azure Portal (portal.azure.com) mit deinem Schulaccount an. Erstelle eine neue Ressourcengruppe f\u00fcr dein Projekt: W\u00e4hle im Portal \"Ressourcengruppe\" und klicke auf \"Erstellen\" Benenne sie nach deinem Projekt (z.B. \"meine-webapp-rg\") Erstelle einen neuen App Service: Suche nach \"App Services\" und klicke auf \"Erstellen\" W\u00e4hle deine Ressourcengruppe aus Gib einen Namen f\u00fcr deine Web-App ein (dieser wird Teil der URL: deinname.azurewebsites.net) W\u00e4hle den passenden Runtime Stack f\u00fcr deine Anwendung (z.B. Node.js, .NET, Python) W\u00e4hle den kostenfreien F1-Tarif f\u00fcr Testzwecke Konfiguriere das Deployment: Unter \"Deployment\" kannst du verschiedene Optionen w\u00e4hlen: GitHub-Integration (falls dein Code auf GitHub liegt) Azure DevOps (f\u00fcr CI/CD-Pipelines) Lokales Git-Repository FTP-Upload W\u00e4hle die f\u00fcr dich passendste Methode und konfiguriere sie nach den Anweisungen Dokumentiere in deinem Projektbericht: Die URL deiner Webanwendung Die gew\u00e4hlte Deployment-Methode Herausforderungen und L\u00f6sungen beim Deployment-Prozess Screenshot deiner laufenden Anwendung im Internet Optional: Konfiguriere eine benutzerdefinierte Domain, wenn du eine besitzt. Hinweis: Stell sicher, dass alle Umgebungsvariablen und Konfigurationen, die deine Anwendung ben\u00f6tigt, im App Service richtig eingestellt sind. Diese k\u00f6nnen unter \"Konfiguration\" im App Service angepasst werden.","title":"Begleitende Aufgabe - Deployment"},{"location":"TeachingMaterials/DevOps/BegleitendeAufgaben/git/","text":"Erstelle von deiner Programmieraufgabe regelm\u00e4\u00dfig commits mit git . Es ist besser etwas zu viele Commits zu machen als zu wenige. Zumindest nach jeder Unterrichtsstunde sollte ein Commit gemacht werden. Gerne mehr. Gehe dabei wie folgt vor in der Konsole: cd <dein Projektpfad> git init # zum anlegen eine neuen Repositories git add . # zum hinzuf\u00fcgen neuer Dateien in git git stage . # beim n\u00e4chsten commit werden alle Dateien im Ordner angef\u00fcgt git commit -m \"<beschreibung was du implementiert hast>\" # optional git push # hochladen der \u00c4nderungen auf Github, Gitlab etc.","title":"Begleitende Aufgabe - Versionsverwaltung"},{"location":"TeachingMaterials/DevOps/BegleitendeAufgaben/logging/","text":"Implementiere ein professionelles Logging-System f\u00fcr deine Anwendung, um Fehler und Nutzerverhalten zu \u00fcberwachen. Folge daf\u00fcr dieser Schritt-f\u00fcr-Schritt-Anleitung: Erstelle eine Application Insights Ressource in Azure: Gehe im Azure Portal zu deiner Ressourcengruppe Klicke auf \"Hinzuf\u00fcgen\" und suche nach \"Application Insights\" W\u00e4hle einen Namen (z.B. \"meine-app-insights\") Weise sie derselben Ressourcengruppe zu, in der dein App Service l\u00e4uft Klicke auf \"\u00dcberpr\u00fcfen + erstellen\" und dann auf \"Erstellen\" Verbinde deinen App Service mit Application Insights: \u00d6ffne deinen App Service im Azure Portal W\u00e4hle im linken Men\u00fc unter \"Einstellungen\" die Option \"Application Insights\" Klicke auf \"Application Insights aktivieren\" W\u00e4hle die zuvor erstellte Application Insights-Ressource aus Klicke auf \"\u00dcbernehmen\" Installiere die notwendigen Pakete in deinem Projekt: bash # F\u00fcr Node.js Anwendungen npm install @opentelemetry/api @opentelemetry/sdk-node @opentelemetry/auto-instrumentations-node @opentelemetry/exporter-trace-otlp-http Erstelle eine Datei instrumentation.js im Hauptverzeichnis deines Projekts: ```javascript const { NodeSDK } = require('@opentelemetry/sdk-node'); const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node'); const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http'); const sdk = new NodeSDK({ instrumentations: [getNodeAutoInstrumentations()], traceExporter: new OTLPTraceExporter({ url: process.env.APPLICATIONINSIGHTS_CONNECTION_STRING, }), }); sdk.start(); ``` Implementiere manuelles Logging in deinem Code: ```javascript const { trace } = require('@opentelemetry/api'); // Irgendwo in deinem Code, z.B. in einer Route-Handler-Funktion function handleUserRequest(req, res) { const tracer = trace.getTracer('user-operations'); // Erstelle einen Span (Logging-Block) f\u00fcr diese Operation const span = tracer.startSpan('handle-user-request'); // F\u00fcge wichtige Informationen hinzu span.setAttribute('user.id', req.user?.id || 'anonymous'); span.setAttribute('request.path', req.path); try { // Deine normale Gesch\u00e4ftslogik const result = processUserData(req.body); // Logge erfolgreiche Ergebnisse span.addEvent('data_processed_successfully', { items_processed: result.length }); res.json(result); } catch (error) { // Logge Fehler mit Details span.recordException(error); span.setStatus({ code: SpanStatusCode.ERROR }); res.status(500).send('Internal Server Error'); } finally { // Schlie\u00dfe den Span span.end(); } } ``` F\u00fcge die Instrumentierung zu deiner App hinzu: F\u00fcr Node.js: \u00c4ndere deinen Startbefehl so, dass er zuerst die Instrumentierung l\u00e4dt: node -r ./instrumentation.js app.js Oder in package.json: json \"scripts\": { \"start\": \"node -r ./instrumentation.js app.js\" } Teste dein Logging-System: Starte deine Anwendung lokal oder deploye sie in Azure F\u00fchre einige Operationen aus, die geloggt werden sollten \u00d6ffne im Azure Portal deine Application Insights Ressource \u00dcberpr\u00fcfe unter \"\u00dcberwachung\" \u2192 \"Anwendungsverfolgung\" deine Logs Erweitere dein Logging f\u00fcr mindestens drei wichtige Bereiche deiner Anwendung: Benutzerauthentifizierung (Login/Logout-Versuche) Kritische Gesch\u00e4ftsoperationen (z.B. Datenbank\u00e4nderungen) Fehlerbehandlung (mit detaillierten Informationen zur einfacheren Fehlersuche) Erstelle ein benutzerdefiniertes Dashboard: Gehe in Application Insights zu \"Dashboards\" \u2192 \"Neu\" F\u00fcge relevante Metriken f\u00fcr deine Anwendung hinzu (z.B. Anfragerate, Fehlerrate) Speichere dieses Dashboard f\u00fcr die zuk\u00fcnftige \u00dcberwachung Dokumentiere in deinem Projektbericht: Wie du Application Insights eingerichtet hast Welche Arten von Ereignissen du loggst und warum Ein Screenshot deines Dashboards mit realen Monitoring-Daten Wie Logging bei der Fehlerbehebung helfen kann (mit einem konkreten Beispiel) Hinweise: - Achte darauf, keine sensiblen Daten (Passw\u00f6rter, pers\u00f6nliche Informationen) zu loggen! - Die Connection String f\u00fcr Application Insights findest du im Azure Portal unter deiner Application Insights Ressource \u2192 \"\u00dcbersicht\" \u2192 \"Verbindungszeichenfolge\" - In Produktionsumgebungen sollten Log-Levels beachtet werden (DEBUG, INFO, WARN, ERROR)","title":"Begleitende Aufgabe - Logging"},{"location":"TeachingMaterials/DevOps/Bind9/","text":"","title":"Index"},{"location":"TeachingMaterials/DevOps/DevContainers/DevContainers/","text":"Mit Dev-Containern kann man eine einheitliche Umgebung herstellen in der alle Programme, Umgebungsvariablen, Bibliotheken etc. installiert sind die man braucht um eine Software zu entwickeln. Damit mein PC nicht mit zu vielen Programmen zugem\u00fcllt wird, entwickle ich gerne in Dev-Containern . Eine Vorlage f\u00fcr Javascript und Node gibt es hier , Infos aus der Registry hier . Mein devcontainer.json f\u00fcr nodejs: // For format details, see https://aka.ms/devcontainer.json. For config options, see the // README at: https://github.com/devcontainers/templates/tree/main/src/javascript-node { \"name\": \"Node.js\", // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile \"image\": \"mcr.microsoft.com/devcontainers/javascript-node:1-22-bookworm\", \"forwardPorts\": [ 5173, // vite 5555 // prisma studio ], \"privileged\": true, \"customizations\": { \"vscode\": { \"extensions\": [ \"bradlc.vscode-tailwindcss\", // tailwind suggestions \"Prisma.prisma\", // prisma schema support \"dbaeumer.vscode-eslint\", // esling \"esbenp.prettier-vscode\", // pretty print \"formulahendry.auto-rename-tag\", // rename start and closing tag at the same time \"graphql.vscode-graphql\", // graphql support \"miguelsolorio.symbols\", // fancy symbols ] } }, \"remoteUser\": \"root\" } Eine Allgemeine Anleitung gibt es hier (code.visualstudio.com/docs) .","title":"Dev Container"},{"location":"TeachingMaterials/DevOps/DockerBasics/DockerBasics/","text":"Containers are separated sections of the Linux kernel that logically act as different machines to each other. Communication between containers, for example, is only possible in the same way two machines would communicate with each other. Namely over a network interface. Using control groups for separating resources Control groups are one pillar for creating isolated environments in Linux. With cgroups, one can set limits on how many resources a process or a group of processes is allowed to consume. There are cgroup subsystems for each type of resource that can be limited, such as: Memory CPU time Block I/O Process count CPU & memory pinning Freezer Devices Network priority etc. Each subsystem is independent of each other. A cgroup has hierarchies, inheriting its limits to child processes. There is also the option to have dedicated limits for each hierarchy tier. Practically, the available cgroups can be listed on a Linux system as seen in Listing 1. $ ls /sys/fs/cgroup/ blkio cpuacct devices hugetlb misc net_prio pids unified cpu cpuset freezer memory net_cls perf_event rdma Listing 1: List of available cgroups in a virtual filesystem of the Linux kernel. Can vary based on distribution In each directory, there is a tasks file that assigns PIDs to that cgroup. Depending on the subsystem, there are additional files where various limits can be set. To get all the cgroups from a given PID, a lookup to the /proc virtual filesystem is necessary, as seen in Listing 2. By writing the current PID into a cgroup file, the cgroup gets applied to that process. max@jetengine:/sys/fs/cgroup$ echo $$ 370 max@jetengine:/sys/fs/cgroup$ cat /proc/370/cgroup 29:name=systemd:/ 28:misc:/ 27:rdma:/ 26:pids:/ 25:hugetlb:/ 24:net_prio:/ 23:perf_event:/ 22:net_cls:/ 21:freezer:/ 20:devices:/ 19:memory:/ 18:blkio:/ 17:cpuacct:/ 16:cpu:/ 15:cpuset:/ 0::/ Listing 2: Assigned cgroups of the current bash process with the PID 370 Partitioning kernel resources using Linux namespaces Cgroups handle what a given process can access . Namespaces manage what the process can see . They are available for: Network Filesystem mounts Processes Inter-process communication Hostname and Domain name User and group IDs cgroups For other areas, there is active development to integrate them into namespaces as well. Processes can share different namespaces in different categories. For example, they can run on the same network but different filesystem mounts. For containers , the mount namespace is of particular importance. Since containers come with their own software, this gets mounted in that namespace to allow for a separate and controlled environment. In containers, they are called volumes. Namespaces can be managed with the clone and unshare syscalls . With clone , a new process can create a new namespace, while unshare lets existing processes create new namespaces. With nsenter and ip-netns , existing namespaces can be entered. To run a single command in a new network namespace, the ip link command can be used, as shown in Listing 3. To create a container , the command systemd-nspawn can be used. Although in practice, tools like docker or podman are easier to use. THey use these and other commands/syscalls internally. max@jetengine:/$ ip link 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:15:5d:be:06:00 brd ff:ff:ff:ff:ff:ff max@jetengine:/sys/fs/cgroup$ sudo unshare --net ip link 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 Listing 3: Assigned network interfaces on the main machine and in a namespace This immediately leaves the namespace again since there is no process running in it when the ip link command finishes. When opening a bash session, the namespace persists as long as the bash session is active. The shell commands for opening a bash session in a new namespace are shown in Listing 4. max@jetengine:/sys/fs/cgroup$ sudo unshare --net bash root@jetengine:/sys/fs/cgroup# Listing 4: Entering a new namespace as long as the initialized bash session is active When entering the namespace, as seen in Listing 4, a # appears in the prompt, indicating the user is inside the namespace. When running docker exec bash , the same # will be displayed. Using Docker as a container engine to create Linux namespaces Docker and namespaces are closely related. Docker uses namespaces to create isolated environments (containers). For example, a Ubuntu container can be created using the command shown in Listing 5. To check if the container is running, the docker ps command can be used, as seen in Listing 6. max@jetengine:~$ docker run -it ubuntu bash root@a74b66bd26b2:/# Listing 5: Create a new Ubuntu container and run bash within it via Docker max@jetengine:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a74b66bd26b2 ubuntu \"bash\" 17 seconds ago Up 16 seconds zen_neumann Listing 6: Show all currently running containers Using docker compose to run and configure multiple containers THe docker run command is useful for creating a new container that needs to be used only once. If multiple containers need to be used, all depending on each other a compose file is more suitable. To transition from docker run to compose, the website composerize is a great tool. It shows the relation between the two ways of describing containers. The specification of docker compose files can be seen here . Creating a container using basic Linux commands only First, install debootstrap . This will download all the necessary files for a basic debian installation. This can be done on Fedora using sudo dnf install debootstrap . Select a directory on your host system in which the necessary files for the namespace (container) will be stored: mkdir /your/path . Install Debian into the the selected directory: debootstrap --arch amd64 buster /your/path http://deb.debian.org/debian Run the namespace with systemd-nspawn. After the basic installation, you can start a namespace using systemd-nspawn. systemd-nspawn -D /your/path The -D flag specifies the directory to use as the root filesystem. This creates an environment similar to a container, with better resource isolation than chroot . For further isolation the flag --network-veth can be appended to systemd-nspawn . To see which interfaces are active on the host and in the namespace, execute ip link show . Within the container run ip link show 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: host0@if95: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether b6:31:71:1d:6d:26 brd ff:ff:ff:ff:ff:ff link-netnsid 0 ip addr add 192.168.100.2/24 dev host0 ip link set host0 up On the host run ip link show wlp0s20f3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DORMANT group default qlen 1000 ... < many interfaces> ve-debian@if2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 ... sudo ip addr add 192.168.100.1/24 dev ve-debian ip link set ve-debian up When everything is set up, mutual pinging is possible: In the namespace ping 192.168.100.1 On the host: ping 192.168.100.2 Creating new container images with Dockerfiles See specification here Answer the Questions here","title":"Deployment methods and virtualization in Linux systems"},{"location":"TeachingMaterials/DevOps/DockerBasics/DockerBasics/#using-control-groups-for-separating-resources","text":"Control groups are one pillar for creating isolated environments in Linux. With cgroups, one can set limits on how many resources a process or a group of processes is allowed to consume. There are cgroup subsystems for each type of resource that can be limited, such as: Memory CPU time Block I/O Process count CPU & memory pinning Freezer Devices Network priority etc. Each subsystem is independent of each other. A cgroup has hierarchies, inheriting its limits to child processes. There is also the option to have dedicated limits for each hierarchy tier. Practically, the available cgroups can be listed on a Linux system as seen in Listing 1. $ ls /sys/fs/cgroup/ blkio cpuacct devices hugetlb misc net_prio pids unified cpu cpuset freezer memory net_cls perf_event rdma Listing 1: List of available cgroups in a virtual filesystem of the Linux kernel. Can vary based on distribution In each directory, there is a tasks file that assigns PIDs to that cgroup. Depending on the subsystem, there are additional files where various limits can be set. To get all the cgroups from a given PID, a lookup to the /proc virtual filesystem is necessary, as seen in Listing 2. By writing the current PID into a cgroup file, the cgroup gets applied to that process. max@jetengine:/sys/fs/cgroup$ echo $$ 370 max@jetengine:/sys/fs/cgroup$ cat /proc/370/cgroup 29:name=systemd:/ 28:misc:/ 27:rdma:/ 26:pids:/ 25:hugetlb:/ 24:net_prio:/ 23:perf_event:/ 22:net_cls:/ 21:freezer:/ 20:devices:/ 19:memory:/ 18:blkio:/ 17:cpuacct:/ 16:cpu:/ 15:cpuset:/ 0::/ Listing 2: Assigned cgroups of the current bash process with the PID 370","title":"Using control groups for separating resources"},{"location":"TeachingMaterials/DevOps/DockerBasics/DockerBasics/#partitioning-kernel-resources-using-linux-namespaces","text":"Cgroups handle what a given process can access . Namespaces manage what the process can see . They are available for: Network Filesystem mounts Processes Inter-process communication Hostname and Domain name User and group IDs cgroups For other areas, there is active development to integrate them into namespaces as well. Processes can share different namespaces in different categories. For example, they can run on the same network but different filesystem mounts. For containers , the mount namespace is of particular importance. Since containers come with their own software, this gets mounted in that namespace to allow for a separate and controlled environment. In containers, they are called volumes. Namespaces can be managed with the clone and unshare syscalls . With clone , a new process can create a new namespace, while unshare lets existing processes create new namespaces. With nsenter and ip-netns , existing namespaces can be entered. To run a single command in a new network namespace, the ip link command can be used, as shown in Listing 3. To create a container , the command systemd-nspawn can be used. Although in practice, tools like docker or podman are easier to use. THey use these and other commands/syscalls internally. max@jetengine:/$ ip link 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:15:5d:be:06:00 brd ff:ff:ff:ff:ff:ff max@jetengine:/sys/fs/cgroup$ sudo unshare --net ip link 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 Listing 3: Assigned network interfaces on the main machine and in a namespace This immediately leaves the namespace again since there is no process running in it when the ip link command finishes. When opening a bash session, the namespace persists as long as the bash session is active. The shell commands for opening a bash session in a new namespace are shown in Listing 4. max@jetengine:/sys/fs/cgroup$ sudo unshare --net bash root@jetengine:/sys/fs/cgroup# Listing 4: Entering a new namespace as long as the initialized bash session is active When entering the namespace, as seen in Listing 4, a # appears in the prompt, indicating the user is inside the namespace. When running docker exec bash , the same # will be displayed.","title":"Partitioning kernel resources using Linux namespaces"},{"location":"TeachingMaterials/DevOps/DockerBasics/DockerBasics/#using-docker-as-a-container-engine-to-create-linux-namespaces","text":"Docker and namespaces are closely related. Docker uses namespaces to create isolated environments (containers). For example, a Ubuntu container can be created using the command shown in Listing 5. To check if the container is running, the docker ps command can be used, as seen in Listing 6. max@jetengine:~$ docker run -it ubuntu bash root@a74b66bd26b2:/# Listing 5: Create a new Ubuntu container and run bash within it via Docker max@jetengine:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a74b66bd26b2 ubuntu \"bash\" 17 seconds ago Up 16 seconds zen_neumann Listing 6: Show all currently running containers","title":"Using Docker as a container engine to create Linux namespaces"},{"location":"TeachingMaterials/DevOps/DockerBasics/DockerBasics/#using-docker-compose-to-run-and-configure-multiple-containers","text":"THe docker run command is useful for creating a new container that needs to be used only once. If multiple containers need to be used, all depending on each other a compose file is more suitable. To transition from docker run to compose, the website composerize is a great tool. It shows the relation between the two ways of describing containers. The specification of docker compose files can be seen here .","title":"Using docker compose to run and configure multiple containers"},{"location":"TeachingMaterials/DevOps/DockerBasics/DockerBasics/#creating-a-container-using-basic-linux-commands-only","text":"First, install debootstrap . This will download all the necessary files for a basic debian installation. This can be done on Fedora using sudo dnf install debootstrap . Select a directory on your host system in which the necessary files for the namespace (container) will be stored: mkdir /your/path . Install Debian into the the selected directory: debootstrap --arch amd64 buster /your/path http://deb.debian.org/debian Run the namespace with systemd-nspawn. After the basic installation, you can start a namespace using systemd-nspawn. systemd-nspawn -D /your/path The -D flag specifies the directory to use as the root filesystem. This creates an environment similar to a container, with better resource isolation than chroot . For further isolation the flag --network-veth can be appended to systemd-nspawn . To see which interfaces are active on the host and in the namespace, execute ip link show . Within the container run ip link show 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: host0@if95: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether b6:31:71:1d:6d:26 brd ff:ff:ff:ff:ff:ff link-netnsid 0 ip addr add 192.168.100.2/24 dev host0 ip link set host0 up On the host run ip link show wlp0s20f3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DORMANT group default qlen 1000 ... < many interfaces> ve-debian@if2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 ... sudo ip addr add 192.168.100.1/24 dev ve-debian ip link set ve-debian up When everything is set up, mutual pinging is possible: In the namespace ping 192.168.100.1 On the host: ping 192.168.100.2","title":"Creating a container using basic Linux commands only"},{"location":"TeachingMaterials/DevOps/DockerBasics/DockerBasics/#creating-new-container-images-with-dockerfiles","text":"See specification here Answer the Questions here","title":"Creating new container images with Dockerfiles"},{"location":"TeachingMaterials/DevOps/DockerBasics/theoriefragen/","text":"Wann ist es deiner Meinung nach sinnvoll Docker einzusetzen? Begr\u00fcnde deine Antworten. beim Entwicklen (developement) im aktiven Einsatz (production) beim manuellen Testen neuer Features (beta-tests) f\u00fcr automatisches Testen (unit & integration tests) beim installieren des Programms auf (einer vielzahl von) Maschinen (deployment) Gegeben ist folgendes Dockerfile: ```Docker FROM openjdk:17 WORKDIR /b-app COPY target/spengermed-0.0.1-SNAPSHOT.jar spengermed.jar EXPOSE 8080 CMD [\"java\", \"-jar\", \"spengermed.jar\" ] ``` Welche Funktion erf\u00fcllen die folgenden Befehle? Wie ist ihre allgemeine Syntax? Beispiel-Notation zur Syntax: kommando parameter1 parameter2 parameter3:parameter4 etc. FROM : WORKDIR : COPY : EXPOSE : CMD : Fragen zu docker-compose Wie unterscheiden sich die Aufgabenbereiche der Befehle docker und docker-compose ? Gegeben ist folgende docker compose konfiguration: version: '3.3' services: springapp: build: . ports: - '8080:8080' depends_on: - springdb springdb: image: mysql:latest restart: always environment: MYSQL_DATABASE: 'spengermed' MYSQL_ROOT_PASSWORD: 'root' cap_add: - ALL ports: - '3306:3306' volumes: - springdbvolume:/var/lib/mysql volumes: springdbvolume: Welche Aufgaben haben folgende Schl\u00fcsselw\u00f6rter: version services volumes build image restart environment ports volume depends_on cap_add Was sind die Standard-Dateinamen f\u00fcr die Konfigurationsdateien folgender Programme: Docker docker-compose","title":"Fragen zu Docker"},{"location":"TeachingMaterials/DevOps/DockerBasics/theoriefragen/#fragen-zu-docker-compose","text":"Wie unterscheiden sich die Aufgabenbereiche der Befehle docker und docker-compose ? Gegeben ist folgende docker compose konfiguration: version: '3.3' services: springapp: build: . ports: - '8080:8080' depends_on: - springdb springdb: image: mysql:latest restart: always environment: MYSQL_DATABASE: 'spengermed' MYSQL_ROOT_PASSWORD: 'root' cap_add: - ALL ports: - '3306:3306' volumes: - springdbvolume:/var/lib/mysql volumes: springdbvolume: Welche Aufgaben haben folgende Schl\u00fcsselw\u00f6rter: version services volumes build image restart environment ports volume depends_on cap_add Was sind die Standard-Dateinamen f\u00fcr die Konfigurationsdateien folgender Programme: Docker docker-compose","title":"Fragen zu docker-compose"},{"location":"TeachingMaterials/DevOps/DockerRegistry/DockerRegistry/","text":"In Registries (de: Register) kann man Images hochladen um sie \u00f6ffenlich mit anderen Personen zu teilen oder privat mit verschiedenen Ger\u00e4ten. Urspr\u00fcnglich wurden die Registries von Docker erfunden . Mittlerweile wurde deren API und Implementierung aber als Open-Source Projekt an die CNCF \u00fcbergeben und hat nun den Namen distribution , Man kann also nun aus mehreren Quellen Container herunterladen. Ein Beispiel f\u00fcr ein bereits bekanntes Image aus einem Register w\u00e4re mysql . Dabei f\u00fcgt Docker jedoch implizit noch einiges an Zusatzinformation hinzu. Eine l\u00e4ngere Schreibweise w\u00e4re: docker.io/library/mysql:latest . docker.io ist das Standardregister welches verwendet wird wenn kein anderes angegeben ist. H\u00e4ufige Alternativen w\u00e4ren registry.gitlab.com oder ghcr.io von Github. library ist der Username welcher das Image ver\u00f6ffentlicht hat. Auch dieser ist der Standardname wenn kein anderer angegeben wurde. Alle Container von diesem Usernamen wurden von der Docker Foundation ver\u00f6ffentlicht und gepr\u00fcft. Auch tragen sie immer das Abzeichen Docker official image . Am Schluss wenn kein anderer tag spezifiziert wurde, nimmt Docker immer den tag latest . Dieser ist per Definition immer der neueste Tag und wird bei jedem build neu herunterlgeladen, unabh\u00e4ngig davon ob er sich ge\u00e4ndert hat oder nicht seit dem letzten Build. Selbst Images ver\u00f6ffentlichen Docker Hub bietet einem die M\u00f6glichkeit dass man auf deren Platform Images hochladen kann. Github und Gitlab verfolgen hier einen etwas anderen Ansatz. Neben der Bereitstellung von Git-Repositories bieten sie auch Container-Registries sowie eine Build-Infrastruktur an. Docker Hub bietet ausschlie\u00dflich an bereits fertige Images zu ver\u00f6ffentlichen. Erstelle dir f\u00fcr unsere einen User auf hub.docker.com . Dannach hast du die M\u00f6glichkeit lokale Images dort hochzuladen. Auf deinem Rechner musst du dich dann noch mit dem Kommando docker login in der Konsole einloggen. Username uns Passwort sind die gleichen wie auf Docker Hub. Zu dem bereits bekannten Kommando docker pull zum Herunterladen von Images gibt es auch ein Gegenst\u00fcck. Mit docker push ist es m\u00f6glich Images in ein Registry hochzuladen. Um ein Image zu bauen muss man docker build ausf\u00fchren. Dabei bekommt man ein Image mit einem Hashwert als Namen. M\u00f6chte man es auch benennen ist das mit der Option -t localname m\u00f6glich. t steht f\u00fcr tag. Hat man ein benanntes lokales Image kann man dieses mit docker push localname registry/username/imagename:tag hochladen. Ver\u00f6ffentlichtes Image in Compose verwenden Um dein eigenes Image nicht immer dort bauen zu m\u00fcssen wo du es brauchst kannst du in compose auf ein Register verweisen. F\u00fcr unsere Datenbank haben wir das mit image: mysql bereits verwendet. F\u00fcr deine Spring-Anwendung kannst du build: ... ersetzen mit image: <dein-dockerhub-name>/<dein-imagename> . Ist alles fertig aufgesetzt kannst du mit dieser Datei alleine deine Anwendung auf jedem Host mit Docker verwenden. Vorausgesetzt dein Image ist \u00f6ffentlich zug\u00e4nglich. Ansonsten m\u00fcsstest du dich auch auf dem ensprechenden Rechner zuerst wieder mit docker login einloggen. Quellen und weitere Infos Anleitung auf Stackoverflow Offizielle Docker Images Gitlabs Container Registry erkl\u00e4rt GitHubs Container Registry erkl\u00e4rt","title":"Verteilung von Containern"},{"location":"TeachingMaterials/DevOps/DockerRegistry/DockerRegistry/#selbst-images-veroffentlichen","text":"Docker Hub bietet einem die M\u00f6glichkeit dass man auf deren Platform Images hochladen kann. Github und Gitlab verfolgen hier einen etwas anderen Ansatz. Neben der Bereitstellung von Git-Repositories bieten sie auch Container-Registries sowie eine Build-Infrastruktur an. Docker Hub bietet ausschlie\u00dflich an bereits fertige Images zu ver\u00f6ffentlichen. Erstelle dir f\u00fcr unsere einen User auf hub.docker.com . Dannach hast du die M\u00f6glichkeit lokale Images dort hochzuladen. Auf deinem Rechner musst du dich dann noch mit dem Kommando docker login in der Konsole einloggen. Username uns Passwort sind die gleichen wie auf Docker Hub. Zu dem bereits bekannten Kommando docker pull zum Herunterladen von Images gibt es auch ein Gegenst\u00fcck. Mit docker push ist es m\u00f6glich Images in ein Registry hochzuladen. Um ein Image zu bauen muss man docker build ausf\u00fchren. Dabei bekommt man ein Image mit einem Hashwert als Namen. M\u00f6chte man es auch benennen ist das mit der Option -t localname m\u00f6glich. t steht f\u00fcr tag. Hat man ein benanntes lokales Image kann man dieses mit docker push localname registry/username/imagename:tag hochladen.","title":"Selbst Images ver\u00f6ffentlichen"},{"location":"TeachingMaterials/DevOps/DockerRegistry/DockerRegistry/#veroffentlichtes-image-in-compose-verwenden","text":"Um dein eigenes Image nicht immer dort bauen zu m\u00fcssen wo du es brauchst kannst du in compose auf ein Register verweisen. F\u00fcr unsere Datenbank haben wir das mit image: mysql bereits verwendet. F\u00fcr deine Spring-Anwendung kannst du build: ... ersetzen mit image: <dein-dockerhub-name>/<dein-imagename> . Ist alles fertig aufgesetzt kannst du mit dieser Datei alleine deine Anwendung auf jedem Host mit Docker verwenden. Vorausgesetzt dein Image ist \u00f6ffentlich zug\u00e4nglich. Ansonsten m\u00fcsstest du dich auch auf dem ensprechenden Rechner zuerst wieder mit docker login einloggen.","title":"Ver\u00f6ffentlichtes Image in Compose verwenden"},{"location":"TeachingMaterials/DevOps/DockerRegistry/DockerRegistry/#quellen-und-weitere-infos","text":"Anleitung auf Stackoverflow Offizielle Docker Images Gitlabs Container Registry erkl\u00e4rt GitHubs Container Registry erkl\u00e4rt","title":"Quellen und weitere Infos"},{"location":"TeachingMaterials/DevOps/KanikoCI/KanikoCI/","text":"Kaniko is a tool that is able to build containers within a CI-Pipeline, a Kubernetes cluster, a compose deployment, or any other kind of distributed containerized setup . Gitlab suggests in their documentation to use Kaniko to build Docker images. If the containers shall be built automatically in a CI-Pipeline, it is important to enable pipelines in the project settings, as shown in Figure 1 . Figure 1 Project settings, on the very bottom, CI/CD has to be enabled To use the CI/CD pipeline, a GitLab Runner has to be set up. It is a daemon that waits for build jobs to be sent to it. Whenever a CI pipeline job gets triggered on the GitLab project, the runner gets the command to first clone the repository in its current state. Within the .gitlab-ci.yml file, which has to be in the root of the cloned repository, all further instructions are specified. At first, in the .gitlab-ci.yml file (see Listing 1 ), an image is specified. Within that container, all further commands, such as building, testing, linting, etc., get executed. The image has to provide all the commands necessary to perform these steps. Listing 1 A sample gitlab-ci configuration file that uploads the generated container to the GitLab container registry, based on the official documentation from GitLab** build: stage: build variables: http_proxy: <your-proxy> https_proxy: <your-proxy> no_proxy: <your-no-proxy> image: name: gcr.io/kaniko-project/executor:v1.23.2-debug entrypoint: [\"\"] script: - /kaniko/executor --context \"${CI_PROJECT_DIR}\" --build-arg http_proxy=$http_proxy --build-arg https_proxy=$https_proxy --build-arg no_proxy=$no_proxy --dockerfile \"${CI_PROJECT_DIR}/Dockerfile\" --destination \"${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}\" rules: - if: $CI_COMMIT_TAG Dockerfile for MIS Project FROM alpine:3.20 as build # gefunden auf: https://pkgs.alpinelinux.org RUN apk add openjdk21-jdk RUN apk add maven RUN apk add nodejs-current WORKDIR /build COPY . . RUN mvn package -DskipTests # ENTRYPOINT [ \"sh\" ] FROM openjdk:21 WORKDIR /app COPY --from=build /build/target/*.jar /app/app.jar ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=docker\", \"/app/app.jar\"] In this GitLab documentation , detailed instructions are shown how to install a Gitlab runner via Docker. Listing 2 Gitlab Runner service-definition in Docker Compose runner: image: gitlab/gitlab-runner stdin_open: true tty: true volumes: - /var/run/docker.sock:/var/run/docker.sock - ./config:/etc/gitlab-runner Figure 2 Click \"New Project runner\" to add a new runner, under Project Settings > CI/CD Settings > Runners When the runner is registered, a command with the generated token is displayed. The command has to be executed in the gitlab-runner container. See Listing 2 for reference. Listing 4 The commands required to register the runner on gitlab # start the runner-container docker compose up runner -d # get a shell into the container docker compose exec runner bash # register then runner (has to be executed inside the container) gitlab-runner register --url https://gitlab.com --token <TOKEN> When setting up a runner during the setup process, the program asks which executor to choose source . The most suitable executor for this thesis would be the Docker executor. It runs all the jobs within Docker containers. This ensures that on every new job, the build environment is cleanly set up. Due to compartmentalization, any issues from outside or inside the container do not have a big effect on each other. To prepare the runner, first, the secrets have to be set up as CI keys, as shown in Figure 3. Afterward, when a job starts, its status can be observed in the web view, as shown in Figure 4 . Figure 4 First job running on the configured runner Figure 5 After the job finished, Kaniko uploaded the image to the GitLab Container registry It can then be used on any compose file or kubernetes deployment-definition as long as one has access to the registry: version: \"3\" services: <name>: image: registry.gitlab.com/<username>/<imagename>:latest","title":"Building and providing container images automatically using Kaniko"},{"location":"TeachingMaterials/DevOps/KanikoCI/KanikoCI/#figure-1","text":"Project settings, on the very bottom, CI/CD has to be enabled To use the CI/CD pipeline, a GitLab Runner has to be set up. It is a daemon that waits for build jobs to be sent to it. Whenever a CI pipeline job gets triggered on the GitLab project, the runner gets the command to first clone the repository in its current state. Within the .gitlab-ci.yml file, which has to be in the root of the cloned repository, all further instructions are specified. At first, in the .gitlab-ci.yml file (see Listing 1 ), an image is specified. Within that container, all further commands, such as building, testing, linting, etc., get executed. The image has to provide all the commands necessary to perform these steps.","title":"Figure 1"},{"location":"TeachingMaterials/DevOps/KanikoCI/KanikoCI/#listing-1","text":"A sample gitlab-ci configuration file that uploads the generated container to the GitLab container registry, based on the official documentation from GitLab** build: stage: build variables: http_proxy: <your-proxy> https_proxy: <your-proxy> no_proxy: <your-no-proxy> image: name: gcr.io/kaniko-project/executor:v1.23.2-debug entrypoint: [\"\"] script: - /kaniko/executor --context \"${CI_PROJECT_DIR}\" --build-arg http_proxy=$http_proxy --build-arg https_proxy=$https_proxy --build-arg no_proxy=$no_proxy --dockerfile \"${CI_PROJECT_DIR}/Dockerfile\" --destination \"${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}\" rules: - if: $CI_COMMIT_TAG Dockerfile for MIS Project FROM alpine:3.20 as build # gefunden auf: https://pkgs.alpinelinux.org RUN apk add openjdk21-jdk RUN apk add maven RUN apk add nodejs-current WORKDIR /build COPY . . RUN mvn package -DskipTests # ENTRYPOINT [ \"sh\" ] FROM openjdk:21 WORKDIR /app COPY --from=build /build/target/*.jar /app/app.jar ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=docker\", \"/app/app.jar\"] In this GitLab documentation , detailed instructions are shown how to install a Gitlab runner via Docker.","title":"Listing 1"},{"location":"TeachingMaterials/DevOps/KanikoCI/KanikoCI/#listing-2","text":"Gitlab Runner service-definition in Docker Compose runner: image: gitlab/gitlab-runner stdin_open: true tty: true volumes: - /var/run/docker.sock:/var/run/docker.sock - ./config:/etc/gitlab-runner","title":"Listing 2"},{"location":"TeachingMaterials/DevOps/KanikoCI/KanikoCI/#figure-2","text":"Click \"New Project runner\" to add a new runner, under Project Settings > CI/CD Settings > Runners When the runner is registered, a command with the generated token is displayed. The command has to be executed in the gitlab-runner container. See Listing 2 for reference.","title":"Figure 2"},{"location":"TeachingMaterials/DevOps/KanikoCI/KanikoCI/#listing-4","text":"The commands required to register the runner on gitlab # start the runner-container docker compose up runner -d # get a shell into the container docker compose exec runner bash # register then runner (has to be executed inside the container) gitlab-runner register --url https://gitlab.com --token <TOKEN> When setting up a runner during the setup process, the program asks which executor to choose source . The most suitable executor for this thesis would be the Docker executor. It runs all the jobs within Docker containers. This ensures that on every new job, the build environment is cleanly set up. Due to compartmentalization, any issues from outside or inside the container do not have a big effect on each other. To prepare the runner, first, the secrets have to be set up as CI keys, as shown in Figure 3. Afterward, when a job starts, its status can be observed in the web view, as shown in Figure 4 .","title":"Listing 4"},{"location":"TeachingMaterials/DevOps/KanikoCI/KanikoCI/#figure-4","text":"First job running on the configured runner","title":"Figure 4"},{"location":"TeachingMaterials/DevOps/KanikoCI/KanikoCI/#figure-5","text":"After the job finished, Kaniko uploaded the image to the GitLab Container registry It can then be used on any compose file or kubernetes deployment-definition as long as one has access to the registry: version: \"3\" services: <name>: image: registry.gitlab.com/<username>/<imagename>:latest","title":"Figure 5"},{"location":"TeachingMaterials/DevOps/OracleCloudAnleitung/OracleCloudSetup/","text":"Oracle Cloud Deployment Setup Auf cloud.oracle.com einen Account anlegen. Um Fake-Accounts zu verhindern verlangt Oracle eine Kreditkarte zur Identit\u00e4tsfeststellung. Wir werden nur Services verwenden die nichts kosten. Als zus\u00e4tzliche Absicherung bekommt man in den ersten 30 Tagen 250\u20ac welche man f\u00fcr alle m\u00f6glichen Ressorucen aufbrauchen kann. Sollte dieser Wert nach unten gehen ist das ein Indikator das nicht freie Services verwendet wurden. Bleibt der Wert bei 250 kann man bedenkenlos \u00fcber den Testzeitraum hinaus die Free-Eligibel Services verwenden. Jedoch nur diese, alles andere kostet Geld. Deployment vorbereitungen Um unserer Software m\u00f6glichst einfach installierbar zu machen werden wir sie in einen Container packen. Dazu verwenden wir Docker. Das fertige Dockerfile sollte so wie hier in der Sektion Multistage build aussehen. Allerdings muss man mvnw gegen mvn austauschen und beim ersten FROM ein maven image nehmen. Z.b so FROM maven:3-openjdk-17-slim as build . Um unsere Docker-Konfiguration noch leichter starbar zu machen k\u00f6nnen wir mir docker-compose mehrere Services auf einmal Konfigurieren, ihnen Volumes zuweisen, Port-Forwarding einrichten uvm. Unser Dockerfile sieht zun\u00e4chst so aus: version: \"3\" services: api: build: \".\" ports: - \"8080:8080\" Instanz anlegen In cloud.oracle.com einloggen und eine neue instanz erstellen Zun\u00e4chst muss man einen Namen f\u00fcr die Instanz festlegen. Als Basisbetriebssystem ist standardm\u00e4\u00dfig Oricle Linux ausgew\u00e4hlt. Wir werden mit Ubuntu arbeiten, mit einem klick auf den Edit Button kann man das \u00e4ndern. Andere Betriebssysteme wie CentOS w\u00e4ren auch m\u00f6glich. Wichtig ist nur dass die Bezeichnung Always free eligible f\u00fcr das OS zutrifft. Ansonsten wird sp\u00e4testens nach einem 30 Tage zeitraum die Nutzung verrechnet. Als n\u00e4chstes m\u00fcssen wir festlegen wie wir uns beim Verbindungsaufbau authentifizieren. Dazu f\u00fcgen wir unseren \u00f6ffentlichen SSH-Schl\u00fcssel ein. Das wars auch schon mit dem Anlegen der Instanz. Wir k\u00f6nnen mit einem Klick auf Create die Provisionierung beginnen. SSH Verbindung \u00f6ffnen Zum Verbinden brauchen wir einen Usernamen, IP und ein Passwort. Das Passwort ist unser privater SSH-Schl\u00fcssel. Username und IP werden von der Oracle Cloud bereitgestellt. In dem Fall kann ich mich jetzt mit dem Befehl ssh ubuntu@193.122.1.244 -i id_rsa verbinden. Das generelle Schema ist ssh <username>@<host> -i <pfad-zu-ssh-private-key> Der private key liegt im Userverzeichnis im Ordner .ssh . Falls die Datei nicht gefunden wird kann es sein dass man den Befehl im Ordner .ssh ausf\u00fchren muss oder den ganzen Pfad -i C:\\Users\\<dein-username>\\.ssh\\id_rsa angeben muss. Auch muss es nicht sein das die Schl\u00fcsseldatei id_rsa heisst. Es ist aber der Standardname. Nach einem erfolgreichen Login \u00fcber das SSH-Kommando sollte man eine Linux-Bash sehen: Nun m\u00fcssen wir noch unsere Programmdateien auf den Server bringen. Daf\u00fcr gibt es das Programm sftp . Es routet das FTP-Protokoll \u00fcber einen verschl\u00fcsselten SSH-Tunnel. Wir loggen uns also mit SSH ein und starten dann das FTP-Protokoll. Der login sieht daher mit sftp ubuntu@193.122.1.244 sehr \u00e4hnlich aus. Da der Host bereits gespeichert wurde (Datei known_hosts im .ssh Ordner) m\u00fcssen wir den Schl\u00fcssel nicht noch einmal angeben. W\u00e4re aber mit -i weiterhin m\u00f6glich. Unser Spring Projekt k\u00f6nnen wir in eine zip-Datei einpacken und dann via SFTP hochladen. Das k\u00f6nnen wir entweder in Windows machen (Rechte Maustaste -> Senden an -> zip komprimmierter Ordner). Um die Dateigr\u00f6\u00dfe aber m\u00f6glichst klein zu halten ist es besser das Kommando git archive zu verwenden. Dieses wird keinen Dateien welche im .gitignore Ordner zur zip-Datei hinzuf\u00fcgen. Alle m\u00f6glichen Buildartefakte aus dem target-Ordner, IntelliJ konfigurationen usw. kommen also garnicht erst ins zip. Mit git archive HEAD --format=zip -o springproject.zip bekommen wir vom aktuellsten commit weg (HEAD) eine Zip Datei (--format=zip) welche also springproject.zip (-o) gespeochert wird. Wenn wir das obige sftp-kommado nun starten kommt eine neue shell mit > als Prompt. Mit dem ftp-Kommando put <pfad/dateiname> wird die datei in unser Homeverzeichnis auf der Oracel instanz hochgeladen. Geht man jetzt wieder mit ssh in die Oracle Instanz und l\u00e4sst sich mit ls alle Dateien anzeigen wird man die Datei vorfinden: \u00dcber das Kommando unzip kann man unter Ubuntu zip-Dateien entpachen. Dieses Programm (und docker) ist jedoch noch nicht installiert. Programme bzw. Pakete kann man in Ubuntu mit dem Paket-Verwaltungstool apt installieren. Beim ersten Start der Instanz muss man zun\u00e4chst die Paketquellen aktualisieren. Das funktioniert mit: sudo apt update -y Sudo f\u00fchrt dazu das der nachfolgende Befehl mit Administratorrechten ausgef\u00fcrt wird. Normalwerweise wird man dann gefragt ob man es wirklich installiern will, mit -y kann man diesen Schritt \u00fcberspringen. Ist alles geupdated k\u00f6nnen wir die f\u00fcr uns notwendigen Programme auf einmal alle mit sudo apt install unzip docker docker-compose -y installieren. Ist alles installiert kann man nicht sofort unzip verwenden. Das Programm w\u00fcrde alles in den aktuellen Ordner entpacken. Mit mkdir springproject k\u00f6nnen wir einen neuen Ordner anlegen. Durch mv <deine-zip>.zip springproject/<deine-zip>.zip k\u00f6nnen wir die Datei in den neu erstellen unterordner schieben. Nun k\u00f6nnen wir mit cd springproject in den Ordner reingehen und mit unzip <deine-zip>.zip die Zipdatei entpacken. \u00dcber docker-compose wird nun das Dockerfile aufgebaut. Im Docker-compose.yml ist einiges bereits vordefiniert. Wir m\u00fcssen nur mit sudo docker-compose up -d den Container starten. Sobald das Programm gestartet ist, ist es theoretisch von \u00fcberall aus dem Internet erreichbar. Praktisch hat Oracle aber zu unserer Sicherheit noch eine Firewall eingebaut die wir zun\u00e4chst locken m\u00fcssen. Firewall konfigurieren Unsere Firewall k\u00f6nnen wir via unserer virtuellen Netzwerkkarte (VNIC) konfigurieren. Dazu m\u00fcssen wir unser Subnet editieren. Bei mir heisst es subnetspring . In besagten subnet m\u00fcssen wir bei den Security Lists einstellen welche Port von wo nach wo in welcher Range durchd\u00fcrfen Dazu m\u00fcssen wir eine neue Regel f\u00fcr eingehende Verbindungen (ingress rule) erstellen. Durch die Quell-IP 0.0.0.0 mit dem Prefix /0 geben wir an das die IP von \u00fcberall kommen darf. In einer Perfekten Welt g\u00e4be es eine IP-Gruppe f\u00fcr jedes Land. Dann k\u00f6nnten wir mit 43.0.0.0/0 z.B sagen das nur verbindungen aus \u00d6sterreich erlaubt sind. In der Praxis m\u00fcsste man das f\u00fcr jeden Provider machen wenn man das einstellen wollen w\u00fcrde. Das Feld f\u00fcr den Quellport lassen wir frei, damit sind alle erlaubt. Als Destination Port w\u00e4hlen wir Port 80. Den Port des HTTP-Protokolls. Spring operiert zwar auf port 8080, jedoch wurde in unserem Docker-compose alles von 8080 auf 80 umgeleitet. Nun da die Firewall eingestellt ist, der Server im Hintergrund l\u00e4uft und auch sonst alles Reibungslos funktioniert hat k\u00f6nnen wir unsere API via HTTP aufrufen. \u00dcber Firefox bekommen wir auf unseren GET-Request z.B folgende Antwort (habe bei mir dann doch 8080 statt 80 verwendet): Um die Oracle-Server nicht unn\u00f6tig zu belasten sollten wir unseren Server wenn wir ihn nicht brauchen auch wieder Herunterfahren. In der Instanzverwaltung dazu einfach auf den Stop-Button klicken und ein force-shutdown starten: \u00dcber den Start-Button kann man die Instanz jederzeit wieder starten, sich mit SSH einloggen und weitermachen.","title":"Azure Anleitung ASP"},{"location":"TeachingMaterials/DevOps/OracleCloudAnleitung/OracleCloudSetup/#oracle-cloud-deployment","text":"","title":"Oracle Cloud Deployment"},{"location":"TeachingMaterials/DevOps/OracleCloudAnleitung/OracleCloudSetup/#setup","text":"Auf cloud.oracle.com einen Account anlegen. Um Fake-Accounts zu verhindern verlangt Oracle eine Kreditkarte zur Identit\u00e4tsfeststellung. Wir werden nur Services verwenden die nichts kosten. Als zus\u00e4tzliche Absicherung bekommt man in den ersten 30 Tagen 250\u20ac welche man f\u00fcr alle m\u00f6glichen Ressorucen aufbrauchen kann. Sollte dieser Wert nach unten gehen ist das ein Indikator das nicht freie Services verwendet wurden. Bleibt der Wert bei 250 kann man bedenkenlos \u00fcber den Testzeitraum hinaus die Free-Eligibel Services verwenden. Jedoch nur diese, alles andere kostet Geld.","title":"Setup"},{"location":"TeachingMaterials/DevOps/OracleCloudAnleitung/OracleCloudSetup/#deployment-vorbereitungen","text":"Um unserer Software m\u00f6glichst einfach installierbar zu machen werden wir sie in einen Container packen. Dazu verwenden wir Docker. Das fertige Dockerfile sollte so wie hier in der Sektion Multistage build aussehen. Allerdings muss man mvnw gegen mvn austauschen und beim ersten FROM ein maven image nehmen. Z.b so FROM maven:3-openjdk-17-slim as build . Um unsere Docker-Konfiguration noch leichter starbar zu machen k\u00f6nnen wir mir docker-compose mehrere Services auf einmal Konfigurieren, ihnen Volumes zuweisen, Port-Forwarding einrichten uvm. Unser Dockerfile sieht zun\u00e4chst so aus: version: \"3\" services: api: build: \".\" ports: - \"8080:8080\"","title":"Deployment vorbereitungen"},{"location":"TeachingMaterials/DevOps/OracleCloudAnleitung/OracleCloudSetup/#instanz-anlegen","text":"In cloud.oracle.com einloggen und eine neue instanz erstellen Zun\u00e4chst muss man einen Namen f\u00fcr die Instanz festlegen. Als Basisbetriebssystem ist standardm\u00e4\u00dfig Oricle Linux ausgew\u00e4hlt. Wir werden mit Ubuntu arbeiten, mit einem klick auf den Edit Button kann man das \u00e4ndern. Andere Betriebssysteme wie CentOS w\u00e4ren auch m\u00f6glich. Wichtig ist nur dass die Bezeichnung Always free eligible f\u00fcr das OS zutrifft. Ansonsten wird sp\u00e4testens nach einem 30 Tage zeitraum die Nutzung verrechnet. Als n\u00e4chstes m\u00fcssen wir festlegen wie wir uns beim Verbindungsaufbau authentifizieren. Dazu f\u00fcgen wir unseren \u00f6ffentlichen SSH-Schl\u00fcssel ein. Das wars auch schon mit dem Anlegen der Instanz. Wir k\u00f6nnen mit einem Klick auf Create die Provisionierung beginnen.","title":"Instanz anlegen"},{"location":"TeachingMaterials/DevOps/OracleCloudAnleitung/OracleCloudSetup/#ssh-verbindung-offnen","text":"Zum Verbinden brauchen wir einen Usernamen, IP und ein Passwort. Das Passwort ist unser privater SSH-Schl\u00fcssel. Username und IP werden von der Oracle Cloud bereitgestellt. In dem Fall kann ich mich jetzt mit dem Befehl ssh ubuntu@193.122.1.244 -i id_rsa verbinden. Das generelle Schema ist ssh <username>@<host> -i <pfad-zu-ssh-private-key> Der private key liegt im Userverzeichnis im Ordner .ssh . Falls die Datei nicht gefunden wird kann es sein dass man den Befehl im Ordner .ssh ausf\u00fchren muss oder den ganzen Pfad -i C:\\Users\\<dein-username>\\.ssh\\id_rsa angeben muss. Auch muss es nicht sein das die Schl\u00fcsseldatei id_rsa heisst. Es ist aber der Standardname. Nach einem erfolgreichen Login \u00fcber das SSH-Kommando sollte man eine Linux-Bash sehen: Nun m\u00fcssen wir noch unsere Programmdateien auf den Server bringen. Daf\u00fcr gibt es das Programm sftp . Es routet das FTP-Protokoll \u00fcber einen verschl\u00fcsselten SSH-Tunnel. Wir loggen uns also mit SSH ein und starten dann das FTP-Protokoll. Der login sieht daher mit sftp ubuntu@193.122.1.244 sehr \u00e4hnlich aus. Da der Host bereits gespeichert wurde (Datei known_hosts im .ssh Ordner) m\u00fcssen wir den Schl\u00fcssel nicht noch einmal angeben. W\u00e4re aber mit -i weiterhin m\u00f6glich. Unser Spring Projekt k\u00f6nnen wir in eine zip-Datei einpacken und dann via SFTP hochladen. Das k\u00f6nnen wir entweder in Windows machen (Rechte Maustaste -> Senden an -> zip komprimmierter Ordner). Um die Dateigr\u00f6\u00dfe aber m\u00f6glichst klein zu halten ist es besser das Kommando git archive zu verwenden. Dieses wird keinen Dateien welche im .gitignore Ordner zur zip-Datei hinzuf\u00fcgen. Alle m\u00f6glichen Buildartefakte aus dem target-Ordner, IntelliJ konfigurationen usw. kommen also garnicht erst ins zip. Mit git archive HEAD --format=zip -o springproject.zip bekommen wir vom aktuellsten commit weg (HEAD) eine Zip Datei (--format=zip) welche also springproject.zip (-o) gespeochert wird. Wenn wir das obige sftp-kommado nun starten kommt eine neue shell mit > als Prompt. Mit dem ftp-Kommando put <pfad/dateiname> wird die datei in unser Homeverzeichnis auf der Oracel instanz hochgeladen. Geht man jetzt wieder mit ssh in die Oracle Instanz und l\u00e4sst sich mit ls alle Dateien anzeigen wird man die Datei vorfinden: \u00dcber das Kommando unzip kann man unter Ubuntu zip-Dateien entpachen. Dieses Programm (und docker) ist jedoch noch nicht installiert. Programme bzw. Pakete kann man in Ubuntu mit dem Paket-Verwaltungstool apt installieren. Beim ersten Start der Instanz muss man zun\u00e4chst die Paketquellen aktualisieren. Das funktioniert mit: sudo apt update -y Sudo f\u00fchrt dazu das der nachfolgende Befehl mit Administratorrechten ausgef\u00fcrt wird. Normalwerweise wird man dann gefragt ob man es wirklich installiern will, mit -y kann man diesen Schritt \u00fcberspringen. Ist alles geupdated k\u00f6nnen wir die f\u00fcr uns notwendigen Programme auf einmal alle mit sudo apt install unzip docker docker-compose -y installieren. Ist alles installiert kann man nicht sofort unzip verwenden. Das Programm w\u00fcrde alles in den aktuellen Ordner entpacken. Mit mkdir springproject k\u00f6nnen wir einen neuen Ordner anlegen. Durch mv <deine-zip>.zip springproject/<deine-zip>.zip k\u00f6nnen wir die Datei in den neu erstellen unterordner schieben. Nun k\u00f6nnen wir mit cd springproject in den Ordner reingehen und mit unzip <deine-zip>.zip die Zipdatei entpacken. \u00dcber docker-compose wird nun das Dockerfile aufgebaut. Im Docker-compose.yml ist einiges bereits vordefiniert. Wir m\u00fcssen nur mit sudo docker-compose up -d den Container starten. Sobald das Programm gestartet ist, ist es theoretisch von \u00fcberall aus dem Internet erreichbar. Praktisch hat Oracle aber zu unserer Sicherheit noch eine Firewall eingebaut die wir zun\u00e4chst locken m\u00fcssen.","title":"SSH Verbindung \u00f6ffnen"},{"location":"TeachingMaterials/DevOps/OracleCloudAnleitung/OracleCloudSetup/#firewall-konfigurieren","text":"Unsere Firewall k\u00f6nnen wir via unserer virtuellen Netzwerkkarte (VNIC) konfigurieren. Dazu m\u00fcssen wir unser Subnet editieren. Bei mir heisst es subnetspring . In besagten subnet m\u00fcssen wir bei den Security Lists einstellen welche Port von wo nach wo in welcher Range durchd\u00fcrfen Dazu m\u00fcssen wir eine neue Regel f\u00fcr eingehende Verbindungen (ingress rule) erstellen. Durch die Quell-IP 0.0.0.0 mit dem Prefix /0 geben wir an das die IP von \u00fcberall kommen darf. In einer Perfekten Welt g\u00e4be es eine IP-Gruppe f\u00fcr jedes Land. Dann k\u00f6nnten wir mit 43.0.0.0/0 z.B sagen das nur verbindungen aus \u00d6sterreich erlaubt sind. In der Praxis m\u00fcsste man das f\u00fcr jeden Provider machen wenn man das einstellen wollen w\u00fcrde. Das Feld f\u00fcr den Quellport lassen wir frei, damit sind alle erlaubt. Als Destination Port w\u00e4hlen wir Port 80. Den Port des HTTP-Protokolls. Spring operiert zwar auf port 8080, jedoch wurde in unserem Docker-compose alles von 8080 auf 80 umgeleitet. Nun da die Firewall eingestellt ist, der Server im Hintergrund l\u00e4uft und auch sonst alles Reibungslos funktioniert hat k\u00f6nnen wir unsere API via HTTP aufrufen. \u00dcber Firefox bekommen wir auf unseren GET-Request z.B folgende Antwort (habe bei mir dann doch 8080 statt 80 verwendet): Um die Oracle-Server nicht unn\u00f6tig zu belasten sollten wir unseren Server wenn wir ihn nicht brauchen auch wieder Herunterfahren. In der Instanzverwaltung dazu einfach auf den Stop-Button klicken und ein force-shutdown starten: \u00dcber den Start-Button kann man die Instanz jederzeit wieder starten, sich mit SSH einloggen und weitermachen.","title":"Firewall konfigurieren"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/","text":"Reverse ssh tunnels allow you to use any publicly available IP-Address to relay the traffic to a (more powerful) server where the requests get handled. Aside from that, SSH is mainly used for remotely operating computers. It is available virtually everywhere . For demonstration, I will use a freely available Oracle Cloud Instance. It is not very powerful but the limited features suffice for this task. Using similarly small systems on Azure or any privately hosted systems with access to a public IP will yield similar results. Creating a Compute-Instance on Oracle Cloud The created Always-Free eligible instance would cost just about 8\u20ac per month. If it was a paid account. Since an Always-Free account is used, there won't be any charges. It just has one CPU and 6 GB of RAM. The used CPU VM.Standard.A1.Flex is so small that the VM is barely useable. You cannot perform any updates, or run any resource-intensive command. The system would just crash. But for using pure SSH tunneling this free setup is sufficient. Setup of the Raspberry Pi Install the latest operating system, using rpi imager . Using a Raspberry Pi is not mandatory. Any system behind any (standard) firewall can be used. On your host, (e.g a raspberry pi or your notebook) run ssh-keygen . On the VPS (Oracle, Azure etc.) add the generated puslic key to the authorized_keys file. When your system attempts to connect to the VPS, it will verify it's identity via a key exchange. The public key (which the VPS has access to) gets used to encrypt a message. On the host it will be decrypted using its private key. It then gets encrypted with the public key of the VPS. On the VPS arriving message of the roundtrip gets decrypted. If it is the same random message as the initial message, access is granted. On your host, install Docker. Instructions for the Raspberry Pi Firewall setup Open Ports on your (Oracle) VPS The Virtual Machine created, need to be able to accept the Ports 80 and 443. For this, https://stackoverflow.com/a/54835902/17996814 open firewall port 80: sudo firewall-cmd --zone=public --permanent --add-port=80/tcp sudo firewall-cmd --reload Allow non-root users to use ports below 1024 on vps https://superuser.com/questions/710253/allow-non-root-process-to-bind-to-port-80-and-443 /etc/sysctl.d/50-unprivileged-ports.conf is just a link to ../sysctl.conf, just add the line net.ipv4.ip_unprivileged_port_start=0 there Testing the connection On the raspberry start a basic nginx container or your desired webservice. Use a docker-compose.yml that looks like this: services: nginx: image: nginx ports: - \"80:80\" Start it using docker compose up -d Setting up a reverse ssh tunnel https://www.jeffgeerling.com/blog/2022/ssh-and-http-raspberry-pi-behind-cg-nat If it works with ssh, you can also utilize it for HTTP(s) using ssh -nNTv -R 0.0.0.0:80:0.0.0.0:80 opc@your.server.ip to open multiple tunnels, use ssh -nNTv -R 443:0.0.0.0:443 -R 80:0.0.0.0:80 opc@your.server.ip Setting up the dynamic DNS service from afraid.org The VPS offers a free way to relay traffic but no free domain. This can be achieved using for example freedns.afraid.org . Persist the docker compose configuration using cronab With crontab commands can be executed periodically. Using @reboot it can be run on every startup. Therefore, in crontab the line: @reboot docker compose -f /path/to/docker-compose.yml up -d will ensure the containers are started as soon as the Raspberry Pi is operational.","title":"Using an Oracle Cloud instance as reverse SSH tunnel"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#creating-a-compute-instance-on-oracle-cloud","text":"The created Always-Free eligible instance would cost just about 8\u20ac per month. If it was a paid account. Since an Always-Free account is used, there won't be any charges. It just has one CPU and 6 GB of RAM. The used CPU VM.Standard.A1.Flex is so small that the VM is barely useable. You cannot perform any updates, or run any resource-intensive command. The system would just crash. But for using pure SSH tunneling this free setup is sufficient.","title":"Creating a Compute-Instance on Oracle Cloud"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#setup-of-the-raspberry-pi","text":"Install the latest operating system, using rpi imager . Using a Raspberry Pi is not mandatory. Any system behind any (standard) firewall can be used. On your host, (e.g a raspberry pi or your notebook) run ssh-keygen . On the VPS (Oracle, Azure etc.) add the generated puslic key to the authorized_keys file. When your system attempts to connect to the VPS, it will verify it's identity via a key exchange. The public key (which the VPS has access to) gets used to encrypt a message. On the host it will be decrypted using its private key. It then gets encrypted with the public key of the VPS. On the VPS arriving message of the roundtrip gets decrypted. If it is the same random message as the initial message, access is granted. On your host, install Docker. Instructions for the Raspberry Pi","title":"Setup of the Raspberry Pi"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#firewall-setup","text":"","title":"Firewall setup"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#open-ports-on-your-oracle-vps","text":"The Virtual Machine created, need to be able to accept the Ports 80 and 443. For this, https://stackoverflow.com/a/54835902/17996814 open firewall port 80: sudo firewall-cmd --zone=public --permanent --add-port=80/tcp sudo firewall-cmd --reload","title":"Open Ports on your (Oracle) VPS"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#allow-non-root-users-to-use-ports-below-1024-on-vps","text":"https://superuser.com/questions/710253/allow-non-root-process-to-bind-to-port-80-and-443 /etc/sysctl.d/50-unprivileged-ports.conf is just a link to ../sysctl.conf, just add the line net.ipv4.ip_unprivileged_port_start=0 there","title":"Allow non-root users to use ports below 1024 on vps"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#testing-the-connection","text":"On the raspberry start a basic nginx container or your desired webservice. Use a docker-compose.yml that looks like this: services: nginx: image: nginx ports: - \"80:80\" Start it using docker compose up -d","title":"Testing the connection"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#setting-up-a-reverse-ssh-tunnel","text":"https://www.jeffgeerling.com/blog/2022/ssh-and-http-raspberry-pi-behind-cg-nat If it works with ssh, you can also utilize it for HTTP(s) using ssh -nNTv -R 0.0.0.0:80:0.0.0.0:80 opc@your.server.ip to open multiple tunnels, use ssh -nNTv -R 443:0.0.0.0:443 -R 80:0.0.0.0:80 opc@your.server.ip","title":"Setting up a reverse ssh tunnel"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#setting-up-the-dynamic-dns-service-from-afraidorg","text":"The VPS offers a free way to relay traffic but no free domain. This can be achieved using for example freedns.afraid.org .","title":"Setting up the dynamic DNS service from afraid.org"},{"location":"TeachingMaterials/DevOps/OracleSSHTunnel/#persist-the-docker-compose-configuration-using-cronab","text":"With crontab commands can be executed periodically. Using @reboot it can be run on every startup. Therefore, in crontab the line: @reboot docker compose -f /path/to/docker-compose.yml up -d will ensure the containers are started as soon as the Raspberry Pi is operational.","title":"Persist the docker compose configuration using cronab"},{"location":"TeachingMaterials/DevOps/Prometheus/","text":"Beispielhafte Implementierung hier (github.com) Wenn eine Anwendung fertig ist und diese auf einem Server deployed wurde ist es weiterhin wichtig den \u00dcberblick zu behalten. Hacker k\u00f6nnten versuchen sich Zutritt zu verschaffen. Benutzer einen Fehler ausl\u00f6sen etc. Bedient man sich des Begriffs DevOps geht es jetzt haupts\u00e4chlich um die Operations (Ops). Die Entwicklung (dev) ist so gut wie abgeschlossen. Um den Status der Server \u00fcberwachen zu k\u00f6nnen kann man Prometheus verwenden. Prometheus verwendet einerseits exporter um Daten zu sammeln. Exporter sind Container, welche andere Container \u00fcberwachen. Dabei muss man f\u00fcr jeden Service einen eigenen Exporter nehmen. F\u00fcr weit verbreitete Services wie MySQL, nginx, redis etc. gibt es bereits vordefinierte Exporter ( siehe Liste ). Den Host kann man \u00fcber den node exporter \u00fcberwachen. Man kann aber auch f\u00fcr seinen eigenen Service einen schreiben. Die Leistungsdaten der Exporter werden dann von einem Prometheus Container zentral gesammelt. Mittels service discovery erkennt prometheus welche Exporter sich im aktuellen virtuellen Netzwerk befinden. Durch Abfragen kann man dann ein Diagramm der Werte sich zeichnen lassen. Auch kann man Alerts anlegen wenn bestimmte Bedingungen eintreten. Z.b wenn eine Fehlermeldung auftritt oder die CPU Last best\u00e4ndig \u00fcber 90% liegt. In dem Fall sollte man sich die Logs ansehen. Daf\u00fcr kann man einen ELK-Stack verwenden. Nachdem das aber auch viel Aufwand zum Einrichten ist kann man auch als abgespeckte Variante Dozzle nehmen. In einem Praktischen Beispiel habe ich Prometheus mit Spring verk\u00fcpft. Nachdem die Anwendung nicht direkt scanbar ist, scannen die Exporter MySQL und nginx. Ist die Last auf die Datenbank oder den Webproxy also sehr hoch, kann man daraus ableiten dass es ein Problem gibt. Zun\u00e4chst muss man daher mal den gesamten an Spring gerichteten Traffic via nginx routen. Und in der Production-Umgebung muss man MySQL als Datenbank einstellen (Zum entwickeln habe ich H2 verwendet). Das geht \u00fcber folgende Config-Datei: events { } http { server { listen 80; listen [::]:80; server_name _; # logging access_log /var/log/nginx/access.log combined buffer=512k flush=1m; error_log /var/log/nginx/error.log warn; # reverse proxy location / { proxy_pass http://spring:8080; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_redirect off; } location /stub_status { stub_status on; } } } Der nginx Container sieht so aus: FROM nginx:latest COPY . /etc/nginx/ Mit dem COPY -Kommando wird die nginx.conf-Datei dann an den richtigne Ort im Container kopiert. K\u00f6nnte man auch \u00fcber einen mount in compose erreichen. Auch braucht man die Exporter-Container. Die muss man nur herunterladen und im Compose einbinden: ... mysql-exporter: image: prom/mysqld-exporter depends_on: - mysql command: - --config.my-cnf=/cfg/.my.cnf - --mysqld.address=localhost:3306 volumes: - \"./observability/mysql/.my.cnf:/cfg/.my.cnf\" ports: - 9104:9104 nginx-exporter: image: nginx/nginx-prometheus-exporter:0.11.0 depends_on: - nginx command: - -nginx.scrape-uri=http://nginx:80/stub_status ports: - \"9113:9113\" node_exporter: image: quay.io/prometheus/node-exporter:latest container_name: node_exporter command: - '--path.rootfs=/host' network_mode: host pid: host restart: unless-stopped volumes: - '/:/host:ro,rslave' ports: - \"9113:9113\" # forwarding not required Nachdem das erledigt ist kann man auch schon Prometheus \u00f6ffen und sich die Daten ansehen: Auch ist es m\u00f6glich \u00fcber Grafana auf die Daten zuzugreifen, sieht dann etwas sch\u00f6ner aus und man kann mehrere Diagrammarten nebeneinander zusammenstellen: Alerts Diese sind in der Datei rules.yml zu definieren. In compose kann man diese Datei \u00fcber einen mount hinzuf\u00fcgen: - \"./rules.yml:/etc/prometheus/rules.yml\" Der Aufbau innerhalb der YML-Datei wird in den docs beschrieben. Auch wird hier darauf hingewiesen dass die Alarme alleine nur auf dem Prometheus-Dashboard angezeigt werden, es kein Rate-Limiting gibt etc. Wenn man ein Werkzeug braucht dass einem den richtigen Alarm sofort anzeigt muss man sich einen AlertManager ( docs , container ) einrichten. Schnelle Logging-L\u00f6sung Und sollte mal ein alert kommen, kann man mit Dozzle schnell nachsehen worans denn liegt. Professioneller w\u00e4re es nat\u00fcrlich die Logs dann auch zu speichern und das nicht der Docker-Engine zu \u00fcberlassen. Daf\u00fcr g\u00e4be es L\u00f6sungen wie flutend, logstash etc. Dozzle hat daf\u00fcr den Vorteil dass es schnell in compose eingerichtet werden kann. Folgende Servicedefinition muss man hinzuf\u00fcgen: services: dozzle: container_name: dozzle image: amir20/dozzle:latest volumes: - /var/run/docker.sock:/var/run/docker.sock:ro ports: - 8888:8080 Weiterf\u00fchrende Literatur IT Ops Times - Log Management: A guide for buyers","title":"Prometheus konfigurieren f\u00fcr eine Spring-Anwendung"},{"location":"TeachingMaterials/DevOps/Prometheus/#alerts","text":"Diese sind in der Datei rules.yml zu definieren. In compose kann man diese Datei \u00fcber einen mount hinzuf\u00fcgen: - \"./rules.yml:/etc/prometheus/rules.yml\" Der Aufbau innerhalb der YML-Datei wird in den docs beschrieben. Auch wird hier darauf hingewiesen dass die Alarme alleine nur auf dem Prometheus-Dashboard angezeigt werden, es kein Rate-Limiting gibt etc. Wenn man ein Werkzeug braucht dass einem den richtigen Alarm sofort anzeigt muss man sich einen AlertManager ( docs , container ) einrichten.","title":"Alerts"},{"location":"TeachingMaterials/DevOps/Prometheus/#schnelle-logging-losung","text":"Und sollte mal ein alert kommen, kann man mit Dozzle schnell nachsehen worans denn liegt. Professioneller w\u00e4re es nat\u00fcrlich die Logs dann auch zu speichern und das nicht der Docker-Engine zu \u00fcberlassen. Daf\u00fcr g\u00e4be es L\u00f6sungen wie flutend, logstash etc. Dozzle hat daf\u00fcr den Vorteil dass es schnell in compose eingerichtet werden kann. Folgende Servicedefinition muss man hinzuf\u00fcgen: services: dozzle: container_name: dozzle image: amir20/dozzle:latest volumes: - /var/run/docker.sock:/var/run/docker.sock:ro ports: - 8888:8080","title":"Schnelle Logging-L\u00f6sung"},{"location":"TeachingMaterials/DevOps/Prometheus/#weiterfuhrende-literatur","text":"IT Ops Times - Log Management: A guide for buyers","title":"Weiterf\u00fchrende Literatur"},{"location":"TeachingMaterials/DevOps/RCloneBackups/","text":"Backups mit rclone erstellen rclone ist ein Programm zur Syncronisation von diversen Cloudspeicher-Anbietern. Es gibt einige Cloud-Syncronisationsprogramme wie z.B Cyberduck o.\u00e4, aber keines welches mir bekannt ist, ist komatibel zu 70! anbietern. Interessant hierbei sind neben normalen Anbietern wie OneDrive, Google Drive etc. auch die virtual provider . Diese k\u00f6nnen die storage provider weiterverarbeiten. Mit dem virtual provider Crypt kann man z.B alle seine Dateien verschl\u00fcsselt auf einen storage provider hochladen. Erstellt man Backups von fremden personenbezogenen Daten welche der DSGVO unterliegen kann das Hilfreich sein um die darin enthaltenen Bestimmungen einzuhalten. Dann sind Serverstandorte au\u00dferhalb der EU weniger problematisch. Ben\u00f6tigen bestimmte Services Datei-\u00dcbertragunsprotokolle wie FTP oder Webdav, kann man mit dem virtual provider union die Daten umleiten. Wobei es in den meisten F\u00e4llen ausreichend sein wird die Dateien lokal zu speichern. Dazu gibt es den Local File System provider. rclone einrichten Auch wenn rclone vorwiegend ein Kommandozeilen basiertes Programm ist, bietet es seit einiger Zeit eine GUI an. Mit dem Kommando: rclone rcd --rc-web-gui kann man die GUI starten. Im Bereich Configs in der Seitenleiste: Dort dann auf Create a New Config klicken: Anleitungen f\u00fcr jeden storage provider gibt es hier . Eine Anleitung zum verschl\u00fcsselten hochladen hier Aufgabe 1: Erstelle eine Konfiguration welche einen beliebigen Ordner verschl\u00fcsselt auf einen storage provider deiner Wahl hochl\u00e4dt. Protokollliere deine Vorgehensweise. Aufgabe 2: Erstelle ein verschl\u00fcsseltes Backup von einem MySQL Container. Dazu kannst du das rclone Docker image verwenden. Mounte das selbe Volume in dem sich auch die MySQL Daten befinden in einem Container von diesem Volume und f\u00fchre beim hochfahren des Containers einen Upload der Daten durch.","title":"Backups mit rclone erstellen"},{"location":"TeachingMaterials/DevOps/RCloneBackups/#backups-mit-rclone-erstellen","text":"rclone ist ein Programm zur Syncronisation von diversen Cloudspeicher-Anbietern. Es gibt einige Cloud-Syncronisationsprogramme wie z.B Cyberduck o.\u00e4, aber keines welches mir bekannt ist, ist komatibel zu 70! anbietern. Interessant hierbei sind neben normalen Anbietern wie OneDrive, Google Drive etc. auch die virtual provider . Diese k\u00f6nnen die storage provider weiterverarbeiten. Mit dem virtual provider Crypt kann man z.B alle seine Dateien verschl\u00fcsselt auf einen storage provider hochladen. Erstellt man Backups von fremden personenbezogenen Daten welche der DSGVO unterliegen kann das Hilfreich sein um die darin enthaltenen Bestimmungen einzuhalten. Dann sind Serverstandorte au\u00dferhalb der EU weniger problematisch. Ben\u00f6tigen bestimmte Services Datei-\u00dcbertragunsprotokolle wie FTP oder Webdav, kann man mit dem virtual provider union die Daten umleiten. Wobei es in den meisten F\u00e4llen ausreichend sein wird die Dateien lokal zu speichern. Dazu gibt es den Local File System provider.","title":"Backups mit rclone erstellen"},{"location":"TeachingMaterials/DevOps/RCloneBackups/#rclone-einrichten","text":"Auch wenn rclone vorwiegend ein Kommandozeilen basiertes Programm ist, bietet es seit einiger Zeit eine GUI an. Mit dem Kommando: rclone rcd --rc-web-gui kann man die GUI starten. Im Bereich Configs in der Seitenleiste: Dort dann auf Create a New Config klicken: Anleitungen f\u00fcr jeden storage provider gibt es hier . Eine Anleitung zum verschl\u00fcsselten hochladen hier","title":"rclone einrichten"},{"location":"TeachingMaterials/DevOps/RCloneBackups/#aufgabe-1","text":"Erstelle eine Konfiguration welche einen beliebigen Ordner verschl\u00fcsselt auf einen storage provider deiner Wahl hochl\u00e4dt. Protokollliere deine Vorgehensweise.","title":"Aufgabe 1:"},{"location":"TeachingMaterials/DevOps/RCloneBackups/#aufgabe-2","text":"Erstelle ein verschl\u00fcsseltes Backup von einem MySQL Container. Dazu kannst du das rclone Docker image verwenden. Mounte das selbe Volume in dem sich auch die MySQL Daten befinden in einem Container von diesem Volume und f\u00fchre beim hochfahren des Containers einen Upload der Daten durch.","title":"Aufgabe 2:"},{"location":"TeachingMaterials/DevOps/Terraform/","text":"Dateibasierte Provisionierung mit Terraform Vorbereitung How to install the Azure CLI - learn.microsoft.com Terrform installation Vorwort In Docker Compose kann man eine Umgebung f\u00fcr seine Software definieren, d.h. welche Ports und anderen Services verf\u00fcgbar sind. Terraform macht dasselbe auf der Ebene der Infrastruktur. Man legt hier seine Umgebung fest - welche VM erzeugt werden soll, wie viel Arbeits- und Festplattenspeicher diese hat. Auch \u00f6ffentliche IP-Adressen und Hostnamen kann man festlegen. All das sind sogenannte Resources in einer .tf Datei. Diese Resources kann man sp\u00e4ter verwenden, um seine Services darauf auszuf\u00fchren. Aufbau einer .tf Datei Ganz oben hat man meistens einen Provider-Block. In diesem wird festgelegt, wo die erstellten Ressourcen angelegt werden, z.B. auf Azure, AWS, Oracle Cloud oder Google Cloud. Dazu muss man sich zun\u00e4chst die CLIs dieser Provider installieren. Terraform wird dann die eingegebenen Ressourcen in Kommandos f\u00fcr die CLIs \u00fcbersetzen, damit diese auf den Providern angelegt werden. Beispiel f\u00fcr einen Azure Provider Block: terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"~> 3.0\" } } } provider \"azurerm\" { features {} } Hat man den Provider festgelegt, kann man verschiedene Ressourcen anlegen, zum Beispiel \u00f6ffentliche IP-Adressen, virtuelle Maschinen, App Services oder Kubernetes Cluster. Je nach Provider muss man die Ressourcen allerdings unterschiedlich anlegen. Man kann nicht eine VM erstellen und diese dann 1:1 auf Oracle Cloud und Google Cloud ausf\u00fchren lassen. Es gibt f\u00fcr jeden Cloud-Provider genaue Vorgaben, wie die Ressourcen zu definieren sind. Jedoch ist es damit immer noch einfacher, schnell von einem Provider zum anderen zu wechseln, wenn man f\u00fcr jeden Provider eine eigene .tf-Datei anlegt. Im Vergleich zu einer manuellen Migration \u00fcber die Web-Interfaces oder CLIs der Anbieter. Beispiel f\u00fcr eine Azure VM Resource: resource \"azurerm_virtual_machine\" \"example\" { name = \"example-vm\" location = \"westeurope\" resource_group_name = azurerm_resource_group.example.name network_interface_ids = [azurerm_network_interface.example.id] vm_size = \"Standard_B1s\" storage_image_reference { publisher = \"Canonical\" offer = \"UbuntuServer\" sku = \"18.04-LTS\" version = \"latest\" } os_disk { name = \"example-osdisk\" caching = \"ReadWrite\" storage_account_type = \"Standard_LRS\" } } Hat man alle Ressourcen angelegt und diese wurden erstellt, wird auf der lokalen Festplatte eine terraform.tfstate Datei erstellt, in der der aktuelle Zustand der Cloud-Ressourcen festgehalten wird. Sollte w\u00e4hrend dem Erstellen der Ressourcen ein Fehler passieren, kann man immer von diesem aktuellen Zustand ausgehen und muss bestimmte Ressourcen nicht doppelt erstellen bzw. verhindert so, dass sie doppelt erstellt werden. Wenn alle Ressourcen erstellt wurden, kann man zur leichteren Weiterverarbeitung Outputs definieren. Darin werden bestimmte w\u00e4hrend der Erstellung der Ressourcen angelegte Variablen und Parameter auf die Konsole geschrieben. Man kann auch sp\u00e4ter mit terraform output <output-name> einzelne Outputs abfragen, z.B. die IP-Adresse der aktuell angelegten virtuellen Maschine. Beispiel f\u00fcr Outputs: output \"public_ip\" { value = azurerm_public_ip.example.ip_address } output \"vm_username\" { value = \"azureuser\" } Aufgabe Nimm folgende .tf-Datei als Grunlage um dir eine VM zu erstellen: # Configure the Azure provider terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"~> 3.0\" } tls = { source = \"hashicorp/tls\" version = \"~> 4.0\" } } } provider \"azurerm\" { features {} } # Generate SSH key pair resource \"tls_private_key\" \"ssh_key\" { algorithm = \"RSA\" rsa_bits = 4096 } # Create resource group resource \"azurerm_resource_group\" \"rg\" { name = \"tf_rg\" location = \"westeurope\" } # Create virtual network resource \"azurerm_virtual_network\" \"vnet\" { name = \"5xhbgm-vnet\" address_space = [\"10.1.0.0/16\"] location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name } # Create subnet resource \"azurerm_subnet\" \"subnet\" { name = \"default\" resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = [\"10.1.1.0/24\"] } # Create network security group and rules resource \"azurerm_network_security_group\" \"nsg\" { name = \"5xhbgm-nsg\" location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name security_rule { name = \"SSH\" priority = 300 direction = \"Inbound\" access = \"Allow\" protocol = \"Tcp\" source_port_range = \"*\" destination_port_range = \"22\" source_address_prefix = \"*\" destination_address_prefix = \"*\" } security_rule { name = \"HTTP\" priority = 320 direction = \"Inbound\" access = \"Allow\" protocol = \"Tcp\" source_port_range = \"*\" destination_port_range = \"80\" source_address_prefix = \"*\" destination_address_prefix = \"*\" } security_rule { name = \"HTTPS\" priority = 340 direction = \"Inbound\" access = \"Allow\" protocol = \"Tcp\" source_port_range = \"*\" destination_port_range = \"443\" source_address_prefix = \"*\" destination_address_prefix = \"*\" } } # Create public IP resource \"azurerm_public_ip\" \"pip\" { name = \"5xhbgm-ip\" location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name allocation_method = \"Dynamic\" } # Create network interface resource \"azurerm_network_interface\" \"nic\" { name = \"5xhbgm-nic\" location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name ip_configuration { name = \"ipconfig1\" subnet_id = azurerm_subnet.subnet.id private_ip_address_allocation = \"Dynamic\" public_ip_address_id = azurerm_public_ip.pip.id } } # Connect the security group to the network interface resource \"azurerm_network_interface_security_group_association\" \"nsg_association\" { network_interface_id = azurerm_network_interface.nic.id network_security_group_id = azurerm_network_security_group.nsg.id } # Create virtual machine resource \"azurerm_linux_virtual_machine\" \"vm\" { name = \"5xhbgmvm\" resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location size = \"Standard_B2s\" # B1s free eligible, B2s has more ram admin_username = \"azureuser\" network_interface_ids = [ azurerm_network_interface.nic.id ] os_disk { caching = \"ReadWrite\" storage_account_type = \"Premium_LRS\" disk_size_gb = 64 } source_image_reference { publisher = \"Canonical\" offer = \"0001-com-ubuntu-server-focal\" sku = \"20_04-lts-gen2\" version = \"latest\" } admin_ssh_key { username = \"azureuser\" public_key = tls_private_key.ssh_key.public_key_openssh } patch_mode = \"AutomaticByPlatform\" } # Get the public IP after it's allocated (since it's dynamic) data \"azurerm_public_ip\" \"pip_data\" { name = azurerm_public_ip.pip.name resource_group_name = azurerm_resource_group.rg.name depends_on = [azurerm_linux_virtual_machine.vm] } # Output variables output \"admin_username\" { value = azurerm_linux_virtual_machine.vm.admin_username } output \"public_ip_address\" { value = data.azurerm_public_ip.pip_data.ip_address } output \"ssh_private_key\" { value = tls_private_key.ssh_key.private_key_pem sensitive = true } output \"ssh_public_key\" { value = tls_private_key.ssh_key.public_key_openssh } Logge dich via SSH in die aktuell provisionierte VM ein und erstelle ein Skript, in dem: 1. Docker installiert wird 2. Eine Docker Compose-Datei erstellt und gestartet wird 3. Lege eine Resource an (z.B. azurerm_virtual_machine_extension (siehe terraform-docs) ) (siehe azure-docs) , die dieses Skript beim erstmaligen Start der VM in Zukunft ausf\u00fchrt und daf\u00fcr sorgt, dass sofort alles verf\u00fcgbar ist, was man braucht, um die Webanwendung ausf\u00fchren zu k\u00f6nnen. Teste das Skript zun\u00e4chst \u00fcber SSH. Wenn die output -Direktiven richtig gesetzt sind, kannst du gleich folgendes Skript zum einloggen nehmen: #!/bin/bash private_key=$(terraform output -raw ssh_private_key) username=$(terraform output -raw admin_username) ip=$(terraform output -raw public_ip_address) # Create a temporary key file, since ssh is whiny about using /dev/stdin temp_key=$(mktemp) echo \"$private_key\" > \"$temp_key\" chmod 600 \"$temp_key\" # Connect using the temporary file ssh -i \"$temp_key\" \"$username@$ip\" # Clean up rm \"$temp_key\" echo \"$temp_key deleted\" Beispiel f\u00fcr ein Startup-Skript: #!/bin/bash # Update package lists sudo apt-get update # Install prerequisites sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common # Add Docker's official GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - # Add Docker repository sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Install Docker sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Start Docker service sudo systemctl start docker sudo systemctl enable docker # Add the VM's system user to docker group sudo usermod -aG docker azureuser exit 0 Weitere Unterlagen Why You NEED To Learn Terraform | Practical Tutorial - Travis Media Terraform explained in 15 mins | Terraform Tutorial for Beginners - TechWorldwithNana Statt VMs: azurerm_linux_web_app Weiterf\u00fchrende \u00dcbungen Teil 2 - Shellskript nach VM provisionierung","title":"Dateibasierte Provisionierung mit Terraform"},{"location":"TeachingMaterials/DevOps/Terraform/#dateibasierte-provisionierung-mit-terraform","text":"","title":"Dateibasierte Provisionierung mit Terraform"},{"location":"TeachingMaterials/DevOps/Terraform/#vorbereitung","text":"How to install the Azure CLI - learn.microsoft.com Terrform installation","title":"Vorbereitung"},{"location":"TeachingMaterials/DevOps/Terraform/#vorwort","text":"In Docker Compose kann man eine Umgebung f\u00fcr seine Software definieren, d.h. welche Ports und anderen Services verf\u00fcgbar sind. Terraform macht dasselbe auf der Ebene der Infrastruktur. Man legt hier seine Umgebung fest - welche VM erzeugt werden soll, wie viel Arbeits- und Festplattenspeicher diese hat. Auch \u00f6ffentliche IP-Adressen und Hostnamen kann man festlegen. All das sind sogenannte Resources in einer .tf Datei. Diese Resources kann man sp\u00e4ter verwenden, um seine Services darauf auszuf\u00fchren.","title":"Vorwort"},{"location":"TeachingMaterials/DevOps/Terraform/#aufbau-einer-tf-datei","text":"Ganz oben hat man meistens einen Provider-Block. In diesem wird festgelegt, wo die erstellten Ressourcen angelegt werden, z.B. auf Azure, AWS, Oracle Cloud oder Google Cloud. Dazu muss man sich zun\u00e4chst die CLIs dieser Provider installieren. Terraform wird dann die eingegebenen Ressourcen in Kommandos f\u00fcr die CLIs \u00fcbersetzen, damit diese auf den Providern angelegt werden. Beispiel f\u00fcr einen Azure Provider Block: terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"~> 3.0\" } } } provider \"azurerm\" { features {} } Hat man den Provider festgelegt, kann man verschiedene Ressourcen anlegen, zum Beispiel \u00f6ffentliche IP-Adressen, virtuelle Maschinen, App Services oder Kubernetes Cluster. Je nach Provider muss man die Ressourcen allerdings unterschiedlich anlegen. Man kann nicht eine VM erstellen und diese dann 1:1 auf Oracle Cloud und Google Cloud ausf\u00fchren lassen. Es gibt f\u00fcr jeden Cloud-Provider genaue Vorgaben, wie die Ressourcen zu definieren sind. Jedoch ist es damit immer noch einfacher, schnell von einem Provider zum anderen zu wechseln, wenn man f\u00fcr jeden Provider eine eigene .tf-Datei anlegt. Im Vergleich zu einer manuellen Migration \u00fcber die Web-Interfaces oder CLIs der Anbieter. Beispiel f\u00fcr eine Azure VM Resource: resource \"azurerm_virtual_machine\" \"example\" { name = \"example-vm\" location = \"westeurope\" resource_group_name = azurerm_resource_group.example.name network_interface_ids = [azurerm_network_interface.example.id] vm_size = \"Standard_B1s\" storage_image_reference { publisher = \"Canonical\" offer = \"UbuntuServer\" sku = \"18.04-LTS\" version = \"latest\" } os_disk { name = \"example-osdisk\" caching = \"ReadWrite\" storage_account_type = \"Standard_LRS\" } } Hat man alle Ressourcen angelegt und diese wurden erstellt, wird auf der lokalen Festplatte eine terraform.tfstate Datei erstellt, in der der aktuelle Zustand der Cloud-Ressourcen festgehalten wird. Sollte w\u00e4hrend dem Erstellen der Ressourcen ein Fehler passieren, kann man immer von diesem aktuellen Zustand ausgehen und muss bestimmte Ressourcen nicht doppelt erstellen bzw. verhindert so, dass sie doppelt erstellt werden. Wenn alle Ressourcen erstellt wurden, kann man zur leichteren Weiterverarbeitung Outputs definieren. Darin werden bestimmte w\u00e4hrend der Erstellung der Ressourcen angelegte Variablen und Parameter auf die Konsole geschrieben. Man kann auch sp\u00e4ter mit terraform output <output-name> einzelne Outputs abfragen, z.B. die IP-Adresse der aktuell angelegten virtuellen Maschine. Beispiel f\u00fcr Outputs: output \"public_ip\" { value = azurerm_public_ip.example.ip_address } output \"vm_username\" { value = \"azureuser\" }","title":"Aufbau einer .tf Datei"},{"location":"TeachingMaterials/DevOps/Terraform/#aufgabe","text":"Nimm folgende .tf-Datei als Grunlage um dir eine VM zu erstellen: # Configure the Azure provider terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"~> 3.0\" } tls = { source = \"hashicorp/tls\" version = \"~> 4.0\" } } } provider \"azurerm\" { features {} } # Generate SSH key pair resource \"tls_private_key\" \"ssh_key\" { algorithm = \"RSA\" rsa_bits = 4096 } # Create resource group resource \"azurerm_resource_group\" \"rg\" { name = \"tf_rg\" location = \"westeurope\" } # Create virtual network resource \"azurerm_virtual_network\" \"vnet\" { name = \"5xhbgm-vnet\" address_space = [\"10.1.0.0/16\"] location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name } # Create subnet resource \"azurerm_subnet\" \"subnet\" { name = \"default\" resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = [\"10.1.1.0/24\"] } # Create network security group and rules resource \"azurerm_network_security_group\" \"nsg\" { name = \"5xhbgm-nsg\" location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name security_rule { name = \"SSH\" priority = 300 direction = \"Inbound\" access = \"Allow\" protocol = \"Tcp\" source_port_range = \"*\" destination_port_range = \"22\" source_address_prefix = \"*\" destination_address_prefix = \"*\" } security_rule { name = \"HTTP\" priority = 320 direction = \"Inbound\" access = \"Allow\" protocol = \"Tcp\" source_port_range = \"*\" destination_port_range = \"80\" source_address_prefix = \"*\" destination_address_prefix = \"*\" } security_rule { name = \"HTTPS\" priority = 340 direction = \"Inbound\" access = \"Allow\" protocol = \"Tcp\" source_port_range = \"*\" destination_port_range = \"443\" source_address_prefix = \"*\" destination_address_prefix = \"*\" } } # Create public IP resource \"azurerm_public_ip\" \"pip\" { name = \"5xhbgm-ip\" location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name allocation_method = \"Dynamic\" } # Create network interface resource \"azurerm_network_interface\" \"nic\" { name = \"5xhbgm-nic\" location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name ip_configuration { name = \"ipconfig1\" subnet_id = azurerm_subnet.subnet.id private_ip_address_allocation = \"Dynamic\" public_ip_address_id = azurerm_public_ip.pip.id } } # Connect the security group to the network interface resource \"azurerm_network_interface_security_group_association\" \"nsg_association\" { network_interface_id = azurerm_network_interface.nic.id network_security_group_id = azurerm_network_security_group.nsg.id } # Create virtual machine resource \"azurerm_linux_virtual_machine\" \"vm\" { name = \"5xhbgmvm\" resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location size = \"Standard_B2s\" # B1s free eligible, B2s has more ram admin_username = \"azureuser\" network_interface_ids = [ azurerm_network_interface.nic.id ] os_disk { caching = \"ReadWrite\" storage_account_type = \"Premium_LRS\" disk_size_gb = 64 } source_image_reference { publisher = \"Canonical\" offer = \"0001-com-ubuntu-server-focal\" sku = \"20_04-lts-gen2\" version = \"latest\" } admin_ssh_key { username = \"azureuser\" public_key = tls_private_key.ssh_key.public_key_openssh } patch_mode = \"AutomaticByPlatform\" } # Get the public IP after it's allocated (since it's dynamic) data \"azurerm_public_ip\" \"pip_data\" { name = azurerm_public_ip.pip.name resource_group_name = azurerm_resource_group.rg.name depends_on = [azurerm_linux_virtual_machine.vm] } # Output variables output \"admin_username\" { value = azurerm_linux_virtual_machine.vm.admin_username } output \"public_ip_address\" { value = data.azurerm_public_ip.pip_data.ip_address } output \"ssh_private_key\" { value = tls_private_key.ssh_key.private_key_pem sensitive = true } output \"ssh_public_key\" { value = tls_private_key.ssh_key.public_key_openssh } Logge dich via SSH in die aktuell provisionierte VM ein und erstelle ein Skript, in dem: 1. Docker installiert wird 2. Eine Docker Compose-Datei erstellt und gestartet wird 3. Lege eine Resource an (z.B. azurerm_virtual_machine_extension (siehe terraform-docs) ) (siehe azure-docs) , die dieses Skript beim erstmaligen Start der VM in Zukunft ausf\u00fchrt und daf\u00fcr sorgt, dass sofort alles verf\u00fcgbar ist, was man braucht, um die Webanwendung ausf\u00fchren zu k\u00f6nnen. Teste das Skript zun\u00e4chst \u00fcber SSH. Wenn die output -Direktiven richtig gesetzt sind, kannst du gleich folgendes Skript zum einloggen nehmen: #!/bin/bash private_key=$(terraform output -raw ssh_private_key) username=$(terraform output -raw admin_username) ip=$(terraform output -raw public_ip_address) # Create a temporary key file, since ssh is whiny about using /dev/stdin temp_key=$(mktemp) echo \"$private_key\" > \"$temp_key\" chmod 600 \"$temp_key\" # Connect using the temporary file ssh -i \"$temp_key\" \"$username@$ip\" # Clean up rm \"$temp_key\" echo \"$temp_key deleted\" Beispiel f\u00fcr ein Startup-Skript: #!/bin/bash # Update package lists sudo apt-get update # Install prerequisites sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common # Add Docker's official GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - # Add Docker repository sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Install Docker sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Start Docker service sudo systemctl start docker sudo systemctl enable docker # Add the VM's system user to docker group sudo usermod -aG docker azureuser exit 0","title":"Aufgabe"},{"location":"TeachingMaterials/DevOps/Terraform/#weitere-unterlagen","text":"Why You NEED To Learn Terraform | Practical Tutorial - Travis Media Terraform explained in 15 mins | Terraform Tutorial for Beginners - TechWorldwithNana Statt VMs: azurerm_linux_web_app","title":"Weitere Unterlagen"},{"location":"TeachingMaterials/DevOps/Terraform/#weiterfuhrende-ubungen","text":"Teil 2 - Shellskript nach VM provisionierung","title":"Weiterf\u00fchrende \u00dcbungen"},{"location":"TeachingMaterials/DevOps/Terraform/extension/","text":"Aufbauend auf Teil 1 der Terraform-\u00dcbung Automatisches Deployment mittels Shellskript F\u00fcge zu deiner .tf -Datei nach der VM-Ressource folgende Ressource f\u00fcr eine VM-Extension an: resource \"azurerm_virtual_machine_extension\" \"init-script\" { name = \"docker-install-and-init\" virtual_machine_id = azurerm_linux_virtual_machine.vm.id publisher = \"Microsoft.Azure.Extensions\" type = \"CustomScript\" type_handler_version = \"2.1\" protected_settings = jsonencode({ \"commandToExecute\": file(\"./cloud-init.sh\"), }) depends_on = [ azurerm_linux_virtual_machine.vm ] } Weitere Informationen & Dokumentation zu VM Extensions Terraform azurerm_virtual_machine_extension specification Azure virtual machine extensions and features Shellskript zum einrichten der VM Die obige Virtual Machine Extension f\u00fchrt die Datei cloud-init.sh aus, nachdem die VM gestartet wurde. Bei mir ist in dieser Datei folgendes enthalten: #!/bin/bash # Update package lists sudo apt-get update # Install prerequisites sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common # Add Docker's official GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - # Add Docker repository sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Install Docker sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Start Docker service sudo systemctl start docker sudo systemctl enable docker # Add the VM's system user to docker group sudo usermod -aG docker azureuser TOKEN=<secret> echo \"$TOKEN\" | sudo docker login registry.gitlab.com -u maximiliankraft --password-stdin cat > docker-compose.yml << EOF services: springimage: ports: - 80:8080 image: registry.gitlab.com/maximiliankraft/5xhbgm_server_2425:v0.4 depends_on: - db environment: - MYSQL_ROOT_PASSWORD=root - MYSQL_DATABASE=fhir volumes: - mysqlvolume:/var/lib/mysql db: image: mysql environment: - MYSQL_ROOT_PASSWORD=root - MYSQL_DATABASE=fhir volumes: - mysqlvolume:/var/lib/mysql volumes: mysqlvolume: EOF sudo docker compose up db -d sleep 10 sudo docker compose up springimage -d exit 0 \u00dcbung Passe das Shellskript f\u00fcr deine Spring-Anwendung/Diplomprojekt an. Durch den Befehl terraform apply soll nicht nur eine VM erstellt werden, sondern diese auch automatisch konfiguriert werden. Beschreibe beides Detailiert in einem Protokoll und gib dieses auf Moodle ab. Weitere Informationen git clone https://<token>@git[hub|lab].com/owner/repo.git Git Clone mit Token (Github) Git Clone mit Token (Gitlab)","title":"Automatisches Deployment mittels Shellskript"},{"location":"TeachingMaterials/DevOps/Terraform/extension/#automatisches-deployment-mittels-shellskript","text":"F\u00fcge zu deiner .tf -Datei nach der VM-Ressource folgende Ressource f\u00fcr eine VM-Extension an: resource \"azurerm_virtual_machine_extension\" \"init-script\" { name = \"docker-install-and-init\" virtual_machine_id = azurerm_linux_virtual_machine.vm.id publisher = \"Microsoft.Azure.Extensions\" type = \"CustomScript\" type_handler_version = \"2.1\" protected_settings = jsonencode({ \"commandToExecute\": file(\"./cloud-init.sh\"), }) depends_on = [ azurerm_linux_virtual_machine.vm ] }","title":"Automatisches Deployment mittels Shellskript"},{"location":"TeachingMaterials/DevOps/Terraform/extension/#weitere-informationen-dokumentation-zu-vm-extensions","text":"Terraform azurerm_virtual_machine_extension specification Azure virtual machine extensions and features","title":"Weitere Informationen &amp; Dokumentation zu VM Extensions"},{"location":"TeachingMaterials/DevOps/Terraform/extension/#shellskript-zum-einrichten-der-vm","text":"Die obige Virtual Machine Extension f\u00fchrt die Datei cloud-init.sh aus, nachdem die VM gestartet wurde. Bei mir ist in dieser Datei folgendes enthalten: #!/bin/bash # Update package lists sudo apt-get update # Install prerequisites sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common # Add Docker's official GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - # Add Docker repository sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" # Install Docker sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io # Start Docker service sudo systemctl start docker sudo systemctl enable docker # Add the VM's system user to docker group sudo usermod -aG docker azureuser TOKEN=<secret> echo \"$TOKEN\" | sudo docker login registry.gitlab.com -u maximiliankraft --password-stdin cat > docker-compose.yml << EOF services: springimage: ports: - 80:8080 image: registry.gitlab.com/maximiliankraft/5xhbgm_server_2425:v0.4 depends_on: - db environment: - MYSQL_ROOT_PASSWORD=root - MYSQL_DATABASE=fhir volumes: - mysqlvolume:/var/lib/mysql db: image: mysql environment: - MYSQL_ROOT_PASSWORD=root - MYSQL_DATABASE=fhir volumes: - mysqlvolume:/var/lib/mysql volumes: mysqlvolume: EOF sudo docker compose up db -d sleep 10 sudo docker compose up springimage -d exit 0","title":"Shellskript zum einrichten der VM"},{"location":"TeachingMaterials/DevOps/Terraform/extension/#ubung","text":"Passe das Shellskript f\u00fcr deine Spring-Anwendung/Diplomprojekt an. Durch den Befehl terraform apply soll nicht nur eine VM erstellt werden, sondern diese auch automatisch konfiguriert werden. Beschreibe beides Detailiert in einem Protokoll und gib dieses auf Moodle ab.","title":"\u00dcbung"},{"location":"TeachingMaterials/DevOps/Terraform/extension/#weitere-informationen","text":"git clone https://<token>@git[hub|lab].com/owner/repo.git Git Clone mit Token (Github) Git Clone mit Token (Gitlab)","title":"Weitere Informationen"},{"location":"TeachingMaterials/HTTPGrundlagen/httpGrundlagen/","text":"Manuelles durchf\u00fchren von HTTP-Requests Ein HTTP-Request besteht aus Header sowie Body . Im Header werden Daten \u00fcbertragen die das Protokoll zum arbeiten braucht. Der Aufbau eines HTTP-Requests besteht aus 3 Teilen im groben. Hier eine Beispielhafte Anfrage: GET / HTTP/1.0 Host: www.spengergasse.at Wichtig hierbei ist das eine neue Zeile nicht nur aus einem Newline \\n bzw. Linefeed besteht, sondern zus\u00e4tzlich auch ein Carriage-Return \\r dabei haben. Streng genommen sieht das obere Paket also so aus: GET / HTTP/1.0\\r\\n Host: www.spengergasse.at\\r\\n \\r\\n Exkurs Steuerzeichen \\n bzw. \\r ist eine Darstellung von Steuerzeichen in plain text. In ASCII sind die ersten 32 Zeichen steuerzeichen. \\n ist das 13. und \\r das 10. Zeichen. Sie sind daf\u00fcr Zust\u00e4ndig dass beim dr\u00fccken der Enter-Taste auch eine neue Zeile erzeugt wird. Es gibt noch viele andere Zeichen \\0 ist das Ende eines Strings, das letzte Zeichen ist zum l\u00f6schen vorheriger Zeichen usw. \\r\\n bedeutet also das eine spezielle Art von Enter notwendig ist in HTTP. \\n alleine w\u00e4re auch m\u00f6glich, ist aber nicht HTTP-Konform. Texteditoren unter Windows benutzen f\u00fcr Enter automatisch \\r\\n . Unter Linux nur \\n . ASCII-Zeichentabelle, hexadezimale Nummerierung Code \u20260 \u20261 \u20262 \u20263 \u20264 \u20265 \u20266 \u20267 \u20268 \u20269 \u2026A \u2026B \u2026C \u2026D \u2026E \u2026F 0\u2026 NUL SOH STX ETX EOT ENQ ACK BEL BS HT LF VT FF CR SO SI 1\u2026 DLE DC1 DC2 DC3 DC4 NAK SYN ETB CAN EM SUB ESC FS GS RS US 2\u2026 SP ! \" # $ % & ' ( ) * + , - . / 3\u2026 0 1 2 3 4 5 6 7 8 9 : ; < = > ? 4\u2026 @ A B C D E F G H I J K L M N O 5\u2026 P Q R S T U V W X Y Z [ \\ ] ^ _ 6\u2026 ` a b c d e f g h i j k l m n o 7\u2026 p q r s t u v w x y z { | } ~ DEL HTTP Request selbst erstellen Auf Linux gibt es das Programm netcat bereits vorinstalliert. Ausf\u00fchrbar mit dem Befehl nc . Um es auszuf\u00fchren kannst du dir WSL2 herunterladen oder dir die f\u00fcr Windows kompilierte Version hier herunterladen. Das Programm l\u00e4uft in der Konsole und nimmt als Argument einen Host sowie einen Port. Darauf \u00f6ffnet es dann einen Socket. Das Programm nimmt nun Eingaben entgegen die es wenn eine neue Zeile kommt an den Server schickt. Probier mal das Programm mit folgenden Argumenten zu starten: nc www.google.at 80 . Dann baut netcat eine Socket-Verbindung zu den Google-Servern auf Port 80 (dem Standard HTTP-Port) auf. In der Eingabemaske kannst du jetzt einen GET-Request absetzen indem du GET / HTTP/1.0 schreibst. Die Google-Server sind hier sehr tolerant in dem was f\u00fcr Anfragen sie annhemen. GET / reicht auch schon. Request auf spengergasse.at Der Server unserer Schule (und so gut wie alle anderen) ist hier weit weniger tolerant. Das selbst zusammengebaute Paket muss exakt den HTTP-Spezifikationen entsprechen. Setze daf\u00fcr zun\u00e4chst einen HTTP-Request mit deinem Browser ab und speichere die Anfrage als Textdatei. Was steht sonst noch in dem Header aus GET ...? Liste die Einstellungen auf und erkl\u00e4re wof\u00fcr sie gut sind. Den Dateiinhalt kannst du nun \u00fcber netcat an den Server schicken. Wenn alles passt kommt der Response-Code 200 zur\u00fcck. Ist beim Request was falsch kommt 400 zur\u00fcck. Die Request-Kommandos sehen ca. so aus: Linux: cat request.txt | nc www.spengergasse.at 80 Windows: type request.txt | .\\nc.exe www.spengergasse.at 80 Die Befehle cat bzw. type geben den Dateiinhalt auf stdout aus. Statt das wir ihn auf der Konsole anzeigen \u00fcbergeben wir ihn jedoch als stdin an ein weiteres Programm -natcat. Dies Funktioniert mit dem Pipe-Operator | . Dort rufen wir dann netcat mit Host, Port, und bereits bestehenden Eingaben auf. Besonderheiten bei HTTPS HTTPS (offizieller Name: HTTP Over TLS) hat zwischen TCP und HTTP noch TLS dazwischen. Das erm\u00f6glicht es zum einen dass die Daten verschl\u00fcsselt \u00fcbertragen werden, jedoch kann man so nicht mehr ohne weiteres rein textbasierte Daten an einen HTTPS Server schicken. Man muss sich mit dem darunterliegenden TLS-Protokoll einen Schl\u00fcssel ausmachen mit dem alle weiteren Daten verschl\u00fcsselt werden. Eine TLS-Verbindung mit einem beliebigen Port kann man mit folgendem Befehl aufmachen: openssl s_client -connect <someserver>:443 Nach dem Austausch der Zertifikaten und dem Schl\u00fcsselaufbau wartet das Programm auf rein textbasierte Eingaben. Gibt man nun GET / ein w\u00e4hrend man mit den Google-Servern verbunden ist bekommt man (\u00fcber TLS) HTTP-Daten zur\u00fcck. Auf Ubuntu (bzw. Linux generell) kann man auch mit sudo apt install ncat -y einen Nachfolger von nc installieren welcher TLS unterst\u00fctzt. Einen einfachen HTTPS/1.1 Request kann man dann ausf\u00fchren mit: printf 'GET / HTTP/1.1\\r\\nHost: google.com\\r\\n\\r\\n' | ncat --ssl google.com 443 Fragen Finde herraus wo du in deinem Browser dir anschauen kannst welche Requests an die HTTP-Server geschickt werden. Tipp: Es ist auch dort wo sich die Entwicklerkonsole befindet. Wof\u00fcr stehen die Response Codes 200, 201, 400, 404, 500? Welche Request-Methoden neben GET gibt es noch? Wie m\u00fcsste man die Requestdatei umbauen um einen Post o.\u00e4 mit Daten abzusetzen? Wie sieht deine Textdatei aus die zu einem erfolgreichen Request gef\u00fchrt hat? Worauf musstest du achten? Gibt es Einstellungen die du noch weglassen kannst? F\u00fchre einen Request auf http://httpbin.org aus. Wie m\u00fcssen header und body aussehen damit du im body ein JSON mit einem POST-Request \u00fcbertragen kannst? Wie sieht die Response aus? Kann man HTTP \u00fcber UDP ausf\u00fchren? Jeder der mir diese Fragen am Ende der Stunden beantworten kann bekommt ein Plus. Allerdings sind die Fragen auch unabh\u00e4ngig davon wichtig. Bei der n\u00e4chsten PLF werden die Fragen sehr \u00e4hnlich zu den hier gestellten sein. Quellen: Carriage Return HTTP RFC2616 HTTPS RFC2818","title":"HTTP Grundlagen"},{"location":"TeachingMaterials/HTTPGrundlagen/httpGrundlagen/#manuelles-durchfuhren-von-http-requests","text":"Ein HTTP-Request besteht aus Header sowie Body . Im Header werden Daten \u00fcbertragen die das Protokoll zum arbeiten braucht. Der Aufbau eines HTTP-Requests besteht aus 3 Teilen im groben. Hier eine Beispielhafte Anfrage: GET / HTTP/1.0 Host: www.spengergasse.at Wichtig hierbei ist das eine neue Zeile nicht nur aus einem Newline \\n bzw. Linefeed besteht, sondern zus\u00e4tzlich auch ein Carriage-Return \\r dabei haben. Streng genommen sieht das obere Paket also so aus: GET / HTTP/1.0\\r\\n Host: www.spengergasse.at\\r\\n \\r\\n","title":"Manuelles durchf\u00fchren von HTTP-Requests"},{"location":"TeachingMaterials/HTTPGrundlagen/httpGrundlagen/#exkurs-steuerzeichen","text":"\\n bzw. \\r ist eine Darstellung von Steuerzeichen in plain text. In ASCII sind die ersten 32 Zeichen steuerzeichen. \\n ist das 13. und \\r das 10. Zeichen. Sie sind daf\u00fcr Zust\u00e4ndig dass beim dr\u00fccken der Enter-Taste auch eine neue Zeile erzeugt wird. Es gibt noch viele andere Zeichen \\0 ist das Ende eines Strings, das letzte Zeichen ist zum l\u00f6schen vorheriger Zeichen usw. \\r\\n bedeutet also das eine spezielle Art von Enter notwendig ist in HTTP. \\n alleine w\u00e4re auch m\u00f6glich, ist aber nicht HTTP-Konform. Texteditoren unter Windows benutzen f\u00fcr Enter automatisch \\r\\n . Unter Linux nur \\n . ASCII-Zeichentabelle, hexadezimale Nummerierung Code \u20260 \u20261 \u20262 \u20263 \u20264 \u20265 \u20266 \u20267 \u20268 \u20269 \u2026A \u2026B \u2026C \u2026D \u2026E \u2026F 0\u2026 NUL SOH STX ETX EOT ENQ ACK BEL BS HT LF VT FF CR SO SI 1\u2026 DLE DC1 DC2 DC3 DC4 NAK SYN ETB CAN EM SUB ESC FS GS RS US 2\u2026 SP ! \" # $ % & ' ( ) * + , - . / 3\u2026 0 1 2 3 4 5 6 7 8 9 : ; < = > ? 4\u2026 @ A B C D E F G H I J K L M N O 5\u2026 P Q R S T U V W X Y Z [ \\ ] ^ _ 6\u2026 ` a b c d e f g h i j k l m n o 7\u2026 p q r s t u v w x y z { | } ~ DEL","title":"Exkurs Steuerzeichen"},{"location":"TeachingMaterials/HTTPGrundlagen/httpGrundlagen/#http-request-selbst-erstellen","text":"Auf Linux gibt es das Programm netcat bereits vorinstalliert. Ausf\u00fchrbar mit dem Befehl nc . Um es auszuf\u00fchren kannst du dir WSL2 herunterladen oder dir die f\u00fcr Windows kompilierte Version hier herunterladen. Das Programm l\u00e4uft in der Konsole und nimmt als Argument einen Host sowie einen Port. Darauf \u00f6ffnet es dann einen Socket. Das Programm nimmt nun Eingaben entgegen die es wenn eine neue Zeile kommt an den Server schickt. Probier mal das Programm mit folgenden Argumenten zu starten: nc www.google.at 80 . Dann baut netcat eine Socket-Verbindung zu den Google-Servern auf Port 80 (dem Standard HTTP-Port) auf. In der Eingabemaske kannst du jetzt einen GET-Request absetzen indem du GET / HTTP/1.0 schreibst. Die Google-Server sind hier sehr tolerant in dem was f\u00fcr Anfragen sie annhemen. GET / reicht auch schon.","title":"HTTP Request selbst erstellen"},{"location":"TeachingMaterials/HTTPGrundlagen/httpGrundlagen/#request-auf-spengergasseat","text":"Der Server unserer Schule (und so gut wie alle anderen) ist hier weit weniger tolerant. Das selbst zusammengebaute Paket muss exakt den HTTP-Spezifikationen entsprechen. Setze daf\u00fcr zun\u00e4chst einen HTTP-Request mit deinem Browser ab und speichere die Anfrage als Textdatei. Was steht sonst noch in dem Header aus GET ...? Liste die Einstellungen auf und erkl\u00e4re wof\u00fcr sie gut sind. Den Dateiinhalt kannst du nun \u00fcber netcat an den Server schicken. Wenn alles passt kommt der Response-Code 200 zur\u00fcck. Ist beim Request was falsch kommt 400 zur\u00fcck. Die Request-Kommandos sehen ca. so aus: Linux: cat request.txt | nc www.spengergasse.at 80 Windows: type request.txt | .\\nc.exe www.spengergasse.at 80 Die Befehle cat bzw. type geben den Dateiinhalt auf stdout aus. Statt das wir ihn auf der Konsole anzeigen \u00fcbergeben wir ihn jedoch als stdin an ein weiteres Programm -natcat. Dies Funktioniert mit dem Pipe-Operator | . Dort rufen wir dann netcat mit Host, Port, und bereits bestehenden Eingaben auf.","title":"Request auf spengergasse.at"},{"location":"TeachingMaterials/HTTPGrundlagen/httpGrundlagen/#besonderheiten-bei-https","text":"HTTPS (offizieller Name: HTTP Over TLS) hat zwischen TCP und HTTP noch TLS dazwischen. Das erm\u00f6glicht es zum einen dass die Daten verschl\u00fcsselt \u00fcbertragen werden, jedoch kann man so nicht mehr ohne weiteres rein textbasierte Daten an einen HTTPS Server schicken. Man muss sich mit dem darunterliegenden TLS-Protokoll einen Schl\u00fcssel ausmachen mit dem alle weiteren Daten verschl\u00fcsselt werden. Eine TLS-Verbindung mit einem beliebigen Port kann man mit folgendem Befehl aufmachen: openssl s_client -connect <someserver>:443 Nach dem Austausch der Zertifikaten und dem Schl\u00fcsselaufbau wartet das Programm auf rein textbasierte Eingaben. Gibt man nun GET / ein w\u00e4hrend man mit den Google-Servern verbunden ist bekommt man (\u00fcber TLS) HTTP-Daten zur\u00fcck. Auf Ubuntu (bzw. Linux generell) kann man auch mit sudo apt install ncat -y einen Nachfolger von nc installieren welcher TLS unterst\u00fctzt. Einen einfachen HTTPS/1.1 Request kann man dann ausf\u00fchren mit: printf 'GET / HTTP/1.1\\r\\nHost: google.com\\r\\n\\r\\n' | ncat --ssl google.com 443","title":"Besonderheiten bei HTTPS"},{"location":"TeachingMaterials/HTTPGrundlagen/httpGrundlagen/#fragen","text":"Finde herraus wo du in deinem Browser dir anschauen kannst welche Requests an die HTTP-Server geschickt werden. Tipp: Es ist auch dort wo sich die Entwicklerkonsole befindet. Wof\u00fcr stehen die Response Codes 200, 201, 400, 404, 500? Welche Request-Methoden neben GET gibt es noch? Wie m\u00fcsste man die Requestdatei umbauen um einen Post o.\u00e4 mit Daten abzusetzen? Wie sieht deine Textdatei aus die zu einem erfolgreichen Request gef\u00fchrt hat? Worauf musstest du achten? Gibt es Einstellungen die du noch weglassen kannst? F\u00fchre einen Request auf http://httpbin.org aus. Wie m\u00fcssen header und body aussehen damit du im body ein JSON mit einem POST-Request \u00fcbertragen kannst? Wie sieht die Response aus? Kann man HTTP \u00fcber UDP ausf\u00fchren? Jeder der mir diese Fragen am Ende der Stunden beantworten kann bekommt ein Plus. Allerdings sind die Fragen auch unabh\u00e4ngig davon wichtig. Bei der n\u00e4chsten PLF werden die Fragen sehr \u00e4hnlich zu den hier gestellten sein. Quellen: Carriage Return HTTP RFC2616 HTTPS RFC2818","title":"Fragen"},{"location":"TeachingMaterials/HTTPGrundlagen/JSPromises/promises/","text":"Als Abgabe kopiere deinen Code mit Syntax-Highlighting in ein PDF. Wenn der Code bereits kommentiert ist reicht es aus nur den Code reinzugeben. Ansonsten jede Funktion kurz beschreiben. // Aufgabe 1: Einfaches Promise erstellen // Erstelle ein Promise, dass nach 2 Sekunden mit dem Wert \"Hallo Welt\" erf\u00fcllt wird. function simplePromise() { // Deine L\u00f6sung hier } // Aufgabe 2: Promise-Kette // Erstelle eine Funktion, die eine Zahl verdoppelt und das Ergebnis als Promise zur\u00fcckgibt. // Verwende dann .then(), um das Ergebnis zu verdreifachen. function doubleNumber(num) { // Deine L\u00f6sung hier } // Aufgabe 3: Promise.all() // Erstelle drei Promises, die nach unterschiedlichen Zeiten erf\u00fcllt werden. // Verwende Promise.all(), um auf die Erf\u00fcllung aller Promises zu warten. // Gib auch mit Promise.any() aus wann das erste erf\u00fcllt ist function promiseAll() { // Deine L\u00f6sung hier } // Aufgabe 4: Error Handling // Erstelle ein Promise, das zuf\u00e4llig entweder erf\u00fcllt oder abgelehnt wird. // Eien Erf\u00fcllung geht durch den Aufruf von resolve(), eine Ablehnung via reject() // Behandle beide F\u00e4lle mit .then() und .catch(). function randomPromise() { // Deine L\u00f6sung hier } // Aufgabe 5: async/await // Schreibe eine asynchrone Funktion, die die obige randomPromise() Funktion verwendet // und das Ergebnis mit await abwartet. Behandle Fehler mit try/catch. async function asyncFunction() { // Deine L\u00f6sung hier } // Aufgabe 6: Fetch API // Verwende die Fetch API, um Daten von 'https://jsonplaceholder.typicode.com/todos/1' abzurufen. // Konvertiere die Antwort in JSON und gib den Titel des Todos aus. function fetchData() { // Deine L\u00f6sung hier } Weitere Unterlagen MDN Promise MDN - Using promises Common promise mistakes","title":"\u00dcbungsaufgaben zu Promises"},{"location":"TeachingMaterials/HTTPGrundlagen/JSPromises/promises/#weitere-unterlagen","text":"MDN Promise MDN - Using promises Common promise mistakes","title":"Weitere Unterlagen"},{"location":"TeachingMaterials/Ionic/ionic/","text":"App mit Ionic erstellen Gehe folgendes Tutorial durch: https://ionicframework.com/docs/react/your-first-app . Abgabe Gehe das Tutorial durch bis zum Punkt Deploying Mobile . Es soll also: Ein Ionic Projekt angelegt werden Fotos erstellen Fotos speichern Fotos abrufen App mit Android Studio/XCode builden Lege in einem Protokoll Schritt f\u00fcr Schritt dar wie du die apk/ipa-Datei erstellt hast und dass sie auf einem Mobiltelefon ausf\u00fchrbar ist.","title":"App mit Ionic erstellen"},{"location":"TeachingMaterials/Ionic/ionic/#app-mit-ionic-erstellen","text":"Gehe folgendes Tutorial durch: https://ionicframework.com/docs/react/your-first-app .","title":"App mit Ionic erstellen"},{"location":"TeachingMaterials/Ionic/ionic/#abgabe","text":"Gehe das Tutorial durch bis zum Punkt Deploying Mobile . Es soll also: Ein Ionic Projekt angelegt werden Fotos erstellen Fotos speichern Fotos abrufen App mit Android Studio/XCode builden Lege in einem Protokoll Schritt f\u00fcr Schritt dar wie du die apk/ipa-Datei erstellt hast und dass sie auf einem Mobiltelefon ausf\u00fchrbar ist.","title":"Abgabe"},{"location":"TeachingMaterials/Ionic/projekt/","text":"Projekt mit Ionic und Remix Wir sind nun in der Lage mit Remix eine Webanwendung zu bauen die sowohl - ein Frontend ( .tsx -Dateien) - als auch ein Backend ( .server.ts -Dateien) beinhaltet Zudem haben wir mit Ionic eine einfache Foto-App - f\u00fcrs Web gehostet - f\u00fcr Android/iOS gebaut Mit diesem Vorwissen sollt ihr nun ein eigenes Projekt erstellen. Themenstellung und Funktionsumfang ist euch \u00fcberlassen solange folgende Grundvoraussetzungen erf\u00fcllt sind: Ein in Remix geschriebenes Backend Datenbank (MongoDB, SQlite, MySQL, etc.) ist egal Ein in Ionic geschriebenes Frontend/App M\u00f6glichkeit zum einloggen am Backend/API mithilfe von remix-auth authenticator kann in der Action-Function eingesetzt werden. Siehe Usage","title":"Projekt"},{"location":"TeachingMaterials/Ionic/projekt/#projekt-mit-ionic-und-remix","text":"Wir sind nun in der Lage mit Remix eine Webanwendung zu bauen die sowohl - ein Frontend ( .tsx -Dateien) - als auch ein Backend ( .server.ts -Dateien) beinhaltet Zudem haben wir mit Ionic eine einfache Foto-App - f\u00fcrs Web gehostet - f\u00fcr Android/iOS gebaut Mit diesem Vorwissen sollt ihr nun ein eigenes Projekt erstellen. Themenstellung und Funktionsumfang ist euch \u00fcberlassen solange folgende Grundvoraussetzungen erf\u00fcllt sind: Ein in Remix geschriebenes Backend Datenbank (MongoDB, SQlite, MySQL, etc.) ist egal Ein in Ionic geschriebenes Frontend/App M\u00f6glichkeit zum einloggen am Backend/API mithilfe von remix-auth authenticator kann in der Action-Function eingesetzt werden. Siehe Usage","title":"Projekt mit Ionic und Remix"},{"location":"TeachingMaterials/LegoMindstorms/","text":"Lego Mindstorms Vorbereitung Open Roberta Treiber installieren f\u00fcr NXT Open Roberta Lab","title":"Lego Mindstorms"},{"location":"TeachingMaterials/LegoMindstorms/#lego-mindstorms","text":"","title":"Lego Mindstorms"},{"location":"TeachingMaterials/LegoMindstorms/#vorbereitung","text":"Open Roberta Treiber installieren f\u00fcr NXT Open Roberta Lab","title":"Vorbereitung"},{"location":"TeachingMaterials/prismaremix/","text":"Remix (siehe remix.run ) ist ein komponentenbasiertes Webentwicklungs-Framework welches auf React (siehe react.dev ) basiert. React ist das am meisten heruntergeladene Komponentenbasierte Framework auf npm (siehe npm-stats.com ). Damit lassen sich bereits viele Webseiten gestalten. Jedoch ben\u00f6tigt man f\u00fcr viele Webanwendungen auch einen Weg die Daten zu speichern. Dazu gibt es das ORM (Object Relational Mapper) Prisma (siehe prisma.io ).ORMs erm\u00f6glichen einem dass man Datenbanken ansteuert ohne SQL-Statements oder andere Datenbank-Spezifische Kommandos verwendet. Jeder Eintrag in der Datenbank wird zu einem Objekt im Programm. Sie mappen Objekte zu Relationalen Umgebungen, deswegen ORM. Damit kann man mit der Datenbank arbeiten wie wenn es eine gew\u00f6hnliche Sammlung von Objekten w\u00e4re. Viele ORMs (Prisma eingeschlossen) erm\u00f6glichen es auch mehrere Datenbanken zu verwenden. Man muss also wenn man die Datenbank wechselt nicht im Code an unz\u00e4hligen Stellen SQL-Statements \u00e4ndern. Das \u00fcbernimmt das ORM im Hintergrund mit Datenbank-Treibern die die Prisma-Aufrufe in den jewiligen Datenbank-Aufruf transponieren. Von Prisma gibt es auch einen sch\u00f6nen mehrteiligen Blogartikel wie man Remix, React und Prisma gemeinsam verwendet: https://www.prisma.io/blog/fullstack-remix-prisma-mongodb-1-7D0BfTXBmB6r Eine Einf\u00fchrung was Prisma ist gibt es hier Diesen f\u00fcnfteiligen Blogartikel bitte ich euch durchzugehen. Jedoch ist der Artikel aus 2022, seit dem haben sich in Prisam und den anderen Technologien einige Kleinigkeiten ge\u00e4ndert. Ich versuche hier alle aufzulisten die mir aufgefallen sind. Auch werden in dem Tutorial viele Proerit\u00e4re Cloud-Angebote verwendet. Ich habe weiter unten beschrieben welche offeneren Ans\u00e4tze man stattdessen verwenden kann. Wenn der Code von deren Webseite mit den aktuellen Versionen nicht funktioniert, einfach in den Prisma Docs nachsehen wie es heute umgesetzt wird. Oder Lehrer oder eine KI fragen. \u00c4nderungen zu Punkt 1 Als Datenbank wird in dieser Tutorialreihe MongoDB verwendet. Da MongoDB einen eigenen Service zum laufen braucht und es auch kein relationales Datenbanksystem ist, bitte ich euch stattdessen SqLite (siehe prisma.io/docs ) zu verwenden. Auch andere Datenbanken die ohne Service auskommen w\u00e4ren in Ordnung. Du kannst auch eine Datenbank mit Serivce, wie Postgres (oder auch MongoDB) verwenden. Jedoch ist es wichtig dass die Datenbank bei dir lokal funktioniert und auch bei einem Deployment auf einem Server zuverl\u00e4ssig funktioniert. Wir werden uns alles selbst hosten. Um zu gew\u00e4hrleisten dass alles funktioniert im Zweifel bitte nicht aus dem Tutorial alles 1:1 in eure Anwendung kopieren. Ein Beispiel ist z.B Tailwind CSS. In dem Tutorial wird folgende Dateistruktur abgebildet: \u2502\u2500\u2500 app \u2502 \u251c\u2500\u2500 entry.client.tsx \u2502 \u251c\u2500\u2500 entry.server.tsx \u2502 \u251c\u2500\u2500 root.tsx \u2502 \u2514\u2500\u2500 routes \u2502 \u251c\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 index.tsx \u251c\u2500\u2500 node_modules \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 public \u251c\u2500\u2500 remix.config.js \u251c\u2500\u2500 remix.env.d.ts \u251c\u2500\u2500 server.js \u251c\u2500\u2500 tsconfig.json \u251c\u2500\u2500 README.md \u2514\u2500\u2500 vercel.json Im aktuellen Stand (2024) sieht diese aber so aus: \u251c\u2500\u2500 app \u251c\u2500\u2500 node_modules \u251c\u2500\u2500 package.json \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 postcss.config.js \u251c\u2500\u2500 public \u251c\u2500\u2500 README.md \u251c\u2500\u2500 tailwind.config.ts // tailwind config bereits vorhanden \u251c\u2500\u2500 tsconfig.json \u2514\u2500\u2500 vite.config.ts Achtet genau darauf welche Dinge es schon gibt und was sich ge\u00e4ndert hat. So kann man z.B in der aktuellen Version (2024) die komplette Tailwind-Konfiguration weglassen. Diese ist mittlerweile im Start-Projekt bereits enthalten. Auch das package.json hat sich ver\u00e4ndert. Fr\u00fcher wurde noch das Kommando remix dev genutzt um die Webanwendung zu starten. MIttlerweile wird in der package.json Konfiguration vite verwendet. Das kommando lautet jetzt remix vite:dev . F\u00fcr den Punkt Set up prisma gibt es f\u00fcr SQLite ein eignees Tutorial hier . Im Punkt Model the Data muss man sich an den ge\u00e4nderten Datenbank-Treiber anpassen. Die Konfiguration id String @id @default(auto()) @map(\"_id\") @db.ObjectId macht Sinn f\u00fcr MongoDb. Jedoch kann man diese nicht 1:1 mit SQLite verwenden. @db.ObjectId unterst\u00fctzt dieser Treiber z.B nicht. Hier ein Verweis auf das vorher erw\u00e4hnte Tutorial, wie es in SQLite gehen kann: id Int @id @default(autoincrement()) Quelle: Prisma Quickstart SQLite \u00c4nderungen zu Punkt 2 Das Benennungsschema f\u00fcr automatisches Routing wurde ge\u00e4ndert. Um den Pfad /new/post zu routen musste man fr\u00fcher einen neuen Ordner /new/ anlegen. Jetzt muss man die einzelnen Ordner durch einen Punkt trennen. Die Datei sieht dann so aus: /new.post.tsx . Auch werden f\u00fcr MongoDB embedded types verwendet. Da diese in SQLite nicht unterst\u00fctzt werden, entweder alles in eine gro\u00dfe Datenstruktur geben, oder durch mehrere Tabellen die embedded Types nachbilden. Durch foreign keys kann man die einzelnen Eintr\u00e4ge der Tabellen verbinden. Wie man im Prisma Schema die notwendige @relation -Annotation erstellt wird ( hier prisma.io/docs/ ) erkl\u00e4rt. \u00c4nderungen zu Punkt 4 Beim Hochladen von Profilbildern wird in der Tutorialreihe ein Amazon AWS S3 Bucket verwendet. Kann man verwenden, aber viel einfacher ist es, die Daten lokal zu speichern und bereitzustellen. Auch wird das als deprecated markierte Paket cuid verwendet. Es wird stattdessen empfohlen dass sicherere @paralleldrive/cuid2 zu verwenden. Damit kann man ebenfalls zuf\u00e4llige Dateinamen erstellen. Nachdem S3 nicht verwendet werden soll muss man sich einen anderen Weg \u00fcberlegen wie man die Bilder speichern kann. Hierbei kann es helfen einen Schritt zur\u00fcck zu gehen und sich zu \u00fcberlegen welche Technologien einem zur Verf\u00fcgung stehen. Da w\u00e4re zum einen Remix, welches auf React basiert. Remix hat zwar Werkzeuge zum Dateiupload (siehe remix.run/docs ). Jedoch sind die noch nicht sehr ausgereift. Da React nur ein Komponentenbasiertes Framework ist und nur Clientseitig l\u00e4uft, hat es keine M\u00f6glichkeit zum Upload von Bildern auf einen Server. Remix l\u00e4uft mittels nodejs. Einer Laufzeitumgebung welche serverseitig Javascript Code ausf\u00fchrt. Hier kann man ansetzen um den Upload und das Speichern der Bilder vorzunehmen. und mittles des fs Pakets gibt es auch eine M\u00f6glichkeit dies direkt durchzuf\u00fchren (siehe nodejs.org/learn ). In dem Tutorial wurde eine eigene Komponente erstellt zum Anzeigen und Hochladen des Profilbildes. Als vereinfachte Variante habe ich es mit einem File-Input umgesetzt. es w\u00fcrde sich aber zum \u00dcben anbieten diesen File-Input in eine Komponente zu verfrachten. <form method=\"POST\" encType=\"multipart/form-data\"> <!-- firstName, lastName.... --> <input type=\"file\" name=\"profilePicture\" /> <!-- submit --> </form> Das File-Objekt bekommt man aus dem Formular dann folgenderma\u00dfen: export const action: ActionFunction = async (args: ActionFunctionArgs) => { const formData = await args.request.formData(); const profilePicture = formData.get(\"profilePicture\"); await uploadProfilePicture(profilePicture); Ebenfalls serverseitig kann man das Profilbild dann auf dem Server mit folgender Funktion speichern: import fs from \"fs\" import path from 'path'; // needs `npm i @paralleldrive/cuid2` import { createId } from \"@paralleldrive/cuid2\"; /*** * @returns path of the uploaded file, undefined if an invalid file got provided */ export async function uploadProfilePicture(file: File): Promise<string | undefined> { // check if a new file was uploaded if(file.size === 0 || file.name === \"\"){ return undefined; } // go up two folders from the current script location const __dirname = path.dirname(\"~\") // Define the directory where the file will be saved const uploadDir = path.join(__dirname, 'public', 'pps'); // Ensure the directory exists if (!fs.existsSync(uploadDir)) { fs.mkdirSync(uploadDir, { recursive: true }); } // Define the full path for the new file // todo choose file extension based on provided mime type const fileName = createId()+'.jpg' const filePath = path.join(uploadDir, fileName); try { // Convert the file to a Buffer const buffer = Buffer.from(await file.arrayBuffer()); // Write the file to the specified path fs.writeFileSync(filePath, buffer); // Return the file path or URL as needed return fileName; // Or return a URL if serving via a web server } catch (error) { console.error('Error saving file:', error); throw new Error('Failed to upload profile picture'); } } \u00c4nderungen zu Punkt 5 Den 5. Punkt bez\u00fcglich Deployment lasst ihr bitte ganz weg. Vercel ist keine eigenst\u00e4ndige Technologie, sondern ein Produkt. Es nimmt einem zwar viel ab, jedoch hat man auch weniger Kontrolle wie das eigene Deployment arbeitet. Wir werden stattdessen mit Azure arbeiten. Dort bekommt man als Sch\u00fcler ein Kontingent von 100$ um auf deren Platform alle m\u00f6glichen Ressourcen zu verwenden. Eine eigene Anleitung dazu findest du hier . Abgaberichtlinien Verwendung der Protokollvorlage Beim reinkopieren von Code Syntax-Highlighting (der bunte Text in der IDE) verwenden Regelm\u00e4\u00dfig Commits in Git erstellen und diese mit git log abrufen und ins Protokoll anf\u00fcgen Weitere Infos Grundlagen Typescript Grundlagen React Grundlagen Remix","title":"Webentwicklung mit Remix und Prisma"},{"location":"TeachingMaterials/prismaremix/#anderungen-zu-punkt-1","text":"Als Datenbank wird in dieser Tutorialreihe MongoDB verwendet. Da MongoDB einen eigenen Service zum laufen braucht und es auch kein relationales Datenbanksystem ist, bitte ich euch stattdessen SqLite (siehe prisma.io/docs ) zu verwenden. Auch andere Datenbanken die ohne Service auskommen w\u00e4ren in Ordnung. Du kannst auch eine Datenbank mit Serivce, wie Postgres (oder auch MongoDB) verwenden. Jedoch ist es wichtig dass die Datenbank bei dir lokal funktioniert und auch bei einem Deployment auf einem Server zuverl\u00e4ssig funktioniert. Wir werden uns alles selbst hosten. Um zu gew\u00e4hrleisten dass alles funktioniert im Zweifel bitte nicht aus dem Tutorial alles 1:1 in eure Anwendung kopieren. Ein Beispiel ist z.B Tailwind CSS. In dem Tutorial wird folgende Dateistruktur abgebildet: \u2502\u2500\u2500 app \u2502 \u251c\u2500\u2500 entry.client.tsx \u2502 \u251c\u2500\u2500 entry.server.tsx \u2502 \u251c\u2500\u2500 root.tsx \u2502 \u2514\u2500\u2500 routes \u2502 \u251c\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 index.tsx \u251c\u2500\u2500 node_modules \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 package.json \u251c\u2500\u2500 public \u251c\u2500\u2500 remix.config.js \u251c\u2500\u2500 remix.env.d.ts \u251c\u2500\u2500 server.js \u251c\u2500\u2500 tsconfig.json \u251c\u2500\u2500 README.md \u2514\u2500\u2500 vercel.json Im aktuellen Stand (2024) sieht diese aber so aus: \u251c\u2500\u2500 app \u251c\u2500\u2500 node_modules \u251c\u2500\u2500 package.json \u251c\u2500\u2500 package-lock.json \u251c\u2500\u2500 postcss.config.js \u251c\u2500\u2500 public \u251c\u2500\u2500 README.md \u251c\u2500\u2500 tailwind.config.ts // tailwind config bereits vorhanden \u251c\u2500\u2500 tsconfig.json \u2514\u2500\u2500 vite.config.ts Achtet genau darauf welche Dinge es schon gibt und was sich ge\u00e4ndert hat. So kann man z.B in der aktuellen Version (2024) die komplette Tailwind-Konfiguration weglassen. Diese ist mittlerweile im Start-Projekt bereits enthalten. Auch das package.json hat sich ver\u00e4ndert. Fr\u00fcher wurde noch das Kommando remix dev genutzt um die Webanwendung zu starten. MIttlerweile wird in der package.json Konfiguration vite verwendet. Das kommando lautet jetzt remix vite:dev . F\u00fcr den Punkt Set up prisma gibt es f\u00fcr SQLite ein eignees Tutorial hier . Im Punkt Model the Data muss man sich an den ge\u00e4nderten Datenbank-Treiber anpassen. Die Konfiguration id String @id @default(auto()) @map(\"_id\") @db.ObjectId macht Sinn f\u00fcr MongoDb. Jedoch kann man diese nicht 1:1 mit SQLite verwenden. @db.ObjectId unterst\u00fctzt dieser Treiber z.B nicht. Hier ein Verweis auf das vorher erw\u00e4hnte Tutorial, wie es in SQLite gehen kann: id Int @id @default(autoincrement()) Quelle: Prisma Quickstart SQLite","title":"\u00c4nderungen zu Punkt 1"},{"location":"TeachingMaterials/prismaremix/#anderungen-zu-punkt-2","text":"Das Benennungsschema f\u00fcr automatisches Routing wurde ge\u00e4ndert. Um den Pfad /new/post zu routen musste man fr\u00fcher einen neuen Ordner /new/ anlegen. Jetzt muss man die einzelnen Ordner durch einen Punkt trennen. Die Datei sieht dann so aus: /new.post.tsx . Auch werden f\u00fcr MongoDB embedded types verwendet. Da diese in SQLite nicht unterst\u00fctzt werden, entweder alles in eine gro\u00dfe Datenstruktur geben, oder durch mehrere Tabellen die embedded Types nachbilden. Durch foreign keys kann man die einzelnen Eintr\u00e4ge der Tabellen verbinden. Wie man im Prisma Schema die notwendige @relation -Annotation erstellt wird ( hier prisma.io/docs/ ) erkl\u00e4rt.","title":"\u00c4nderungen zu Punkt 2"},{"location":"TeachingMaterials/prismaremix/#anderungen-zu-punkt-4","text":"Beim Hochladen von Profilbildern wird in der Tutorialreihe ein Amazon AWS S3 Bucket verwendet. Kann man verwenden, aber viel einfacher ist es, die Daten lokal zu speichern und bereitzustellen. Auch wird das als deprecated markierte Paket cuid verwendet. Es wird stattdessen empfohlen dass sicherere @paralleldrive/cuid2 zu verwenden. Damit kann man ebenfalls zuf\u00e4llige Dateinamen erstellen. Nachdem S3 nicht verwendet werden soll muss man sich einen anderen Weg \u00fcberlegen wie man die Bilder speichern kann. Hierbei kann es helfen einen Schritt zur\u00fcck zu gehen und sich zu \u00fcberlegen welche Technologien einem zur Verf\u00fcgung stehen. Da w\u00e4re zum einen Remix, welches auf React basiert. Remix hat zwar Werkzeuge zum Dateiupload (siehe remix.run/docs ). Jedoch sind die noch nicht sehr ausgereift. Da React nur ein Komponentenbasiertes Framework ist und nur Clientseitig l\u00e4uft, hat es keine M\u00f6glichkeit zum Upload von Bildern auf einen Server. Remix l\u00e4uft mittels nodejs. Einer Laufzeitumgebung welche serverseitig Javascript Code ausf\u00fchrt. Hier kann man ansetzen um den Upload und das Speichern der Bilder vorzunehmen. und mittles des fs Pakets gibt es auch eine M\u00f6glichkeit dies direkt durchzuf\u00fchren (siehe nodejs.org/learn ). In dem Tutorial wurde eine eigene Komponente erstellt zum Anzeigen und Hochladen des Profilbildes. Als vereinfachte Variante habe ich es mit einem File-Input umgesetzt. es w\u00fcrde sich aber zum \u00dcben anbieten diesen File-Input in eine Komponente zu verfrachten. <form method=\"POST\" encType=\"multipart/form-data\"> <!-- firstName, lastName.... --> <input type=\"file\" name=\"profilePicture\" /> <!-- submit --> </form> Das File-Objekt bekommt man aus dem Formular dann folgenderma\u00dfen: export const action: ActionFunction = async (args: ActionFunctionArgs) => { const formData = await args.request.formData(); const profilePicture = formData.get(\"profilePicture\"); await uploadProfilePicture(profilePicture); Ebenfalls serverseitig kann man das Profilbild dann auf dem Server mit folgender Funktion speichern: import fs from \"fs\" import path from 'path'; // needs `npm i @paralleldrive/cuid2` import { createId } from \"@paralleldrive/cuid2\"; /*** * @returns path of the uploaded file, undefined if an invalid file got provided */ export async function uploadProfilePicture(file: File): Promise<string | undefined> { // check if a new file was uploaded if(file.size === 0 || file.name === \"\"){ return undefined; } // go up two folders from the current script location const __dirname = path.dirname(\"~\") // Define the directory where the file will be saved const uploadDir = path.join(__dirname, 'public', 'pps'); // Ensure the directory exists if (!fs.existsSync(uploadDir)) { fs.mkdirSync(uploadDir, { recursive: true }); } // Define the full path for the new file // todo choose file extension based on provided mime type const fileName = createId()+'.jpg' const filePath = path.join(uploadDir, fileName); try { // Convert the file to a Buffer const buffer = Buffer.from(await file.arrayBuffer()); // Write the file to the specified path fs.writeFileSync(filePath, buffer); // Return the file path or URL as needed return fileName; // Or return a URL if serving via a web server } catch (error) { console.error('Error saving file:', error); throw new Error('Failed to upload profile picture'); } }","title":"\u00c4nderungen zu Punkt 4"},{"location":"TeachingMaterials/prismaremix/#anderungen-zu-punkt-5","text":"Den 5. Punkt bez\u00fcglich Deployment lasst ihr bitte ganz weg. Vercel ist keine eigenst\u00e4ndige Technologie, sondern ein Produkt. Es nimmt einem zwar viel ab, jedoch hat man auch weniger Kontrolle wie das eigene Deployment arbeitet. Wir werden stattdessen mit Azure arbeiten. Dort bekommt man als Sch\u00fcler ein Kontingent von 100$ um auf deren Platform alle m\u00f6glichen Ressourcen zu verwenden. Eine eigene Anleitung dazu findest du hier .","title":"\u00c4nderungen zu Punkt 5"},{"location":"TeachingMaterials/prismaremix/#abgaberichtlinien","text":"Verwendung der Protokollvorlage Beim reinkopieren von Code Syntax-Highlighting (der bunte Text in der IDE) verwenden Regelm\u00e4\u00dfig Commits in Git erstellen und diese mit git log abrufen und ins Protokoll anf\u00fcgen","title":"Abgaberichtlinien"},{"location":"TeachingMaterials/prismaremix/#weitere-infos","text":"Grundlagen Typescript Grundlagen React Grundlagen Remix","title":"Weitere Infos"},{"location":"TeachingMaterials/prismaremix/githuboauth/","text":"OAuth 2.0 Authentifizierung mit GitHub in einer Remix-Anwendung Einf\u00fchrung in OAuth 2.0 OAuth 2.0 ist ein offener Standard f\u00fcr Authentifizierungs- und Autorisierungsprozesse, der es Diensten von Drittanbietern erm\u00f6glicht, auf Ressourcen eines Benutzers zuzugreifen, ohne dessen Zugangsdaten direkt zu verwenden. Dieser Prozess wird durch ein Token-System realisiert, das tempor\u00e4ren Zugriff auf bestimmte Ressourcen gew\u00e4hrt. Der OAuth 2.0 Prozess ist in RFC 6749 standardisiert und umfasst vier Rollen: Resource Owner : Der Benutzer, der Zugriff auf sein Konto gew\u00e4hren kann Client : Die Anwendung, die Zugriff auf das Konto des Benutzers anfordert Authorization Server : Der Server, der Zugriffstoken ausstellt Resource Server : Der Server, der die gesch\u00fctzten Ressourcen hostet Der OAuth 2.0 Autorisierungsablauf mit GitHub Der OAuth 2.0 Ablauf, speziell f\u00fcr die GitHub-Authentifizierung, l\u00e4uft folgenderma\u00dfen ab: Autorisierungsanfrage : Die Anwendung leitet den Benutzer zur GitHub-Autorisierungsseite weiter Benutzerautorisierung : Der Benutzer genehmigt die Zugriffsanfrage Autorisierungscode : GitHub sendet einen tempor\u00e4ren Code an die Anwendung zur\u00fcck Tokenanfrage : Die Anwendung tauscht diesen Code gegen ein Zugriffstoken aus Ressourcenzugriff : Mit diesem Token kann die Anwendung auf die API und Ressourcen zugreifen Quelle: Authorizing OAuth apps | Web Application Flow Einrichten einer GitHub OAuth-App Um OAuth mit GitHub zu verwenden, muss eine OAuth-App in den GitHub-Entwicklereinstellungen registriert werden. Schritt 1: Erstellen einer neuen OAuth-App in GitHub Melde dich bei GitHub an Navigiere zu den GitHub Developer Settings W\u00e4hle \"OAuth Apps\" und klicke auf \"New OAuth App\" Schritt 2: Konfiguration der OAuth-App F\u00fclle folgende Felder aus: Application name : Ein aussagekr\u00e4ftiger Name f\u00fcr deine App (z.B. \"Remix GitHub Login\") Homepage URL : Die Basis-URL deiner Anwendung (z.B. \"http://localhost:5173\") Application description : (Optional) Eine kurze Beschreibung deiner Anwendung Authorization callback URL : Die URL, an die GitHub nach der Authentifizierung weiterleitet (z.B. \"http://localhost:5173/auth/github/callback\") Schritt 3: Client ID und Client Secret Nach dem Erstellen der App erh\u00e4ltst du zwei wichtige Credentials: Client ID : \u00d6ffentlicher Identifikator f\u00fcr deine App Client Secret : Geheimer Schl\u00fcssel, der sicher aufbewahrt werden muss Diese Credentials werden in deiner Anwendung ben\u00f6tigt, um den OAuth-Prozess zu durchlaufen. Umsetzung in einer Remix-Anwendung In diesem Projekt wurde der OAuth 2.0 Authentifizierungsfluss mit GitHub direkt implementiert, ohne die Passport.js-Middleware zu verwenden. Dies bietet mehr Kontrolle \u00fcber den Authentifizierungsprozess und erleichtert das Verst\u00e4ndnis der einzelnen Schritte. Code-Beispiel: Einleitung des OAuth-Flows // app/auth/github.ts (Auszug) export const githubStrategy = { options: { clientID: \"your-client-id\", clientSecret: \"your-client-secret\", callbackURL: \"http://localhost:5173/auth/github/callback\", }, getAuthorizationURL(scopes: string[] = [\"user:email\"]) { const url = new URL(\"https://github.com/login/oauth/authorize\"); url.searchParams.set(\"client_id\", this.options.clientID); url.searchParams.set(\"redirect_uri\", this.options.callbackURL); url.searchParams.set(\"scope\", scopes.join(\" \")); return url.toString(); }, // Weitere Methoden... }; Code-Beispiel: Verarbeitung des Callback // app/routes/auth.github.callback.tsx (Auszug) export let loader: LoaderFunction = async ({ request }) => { try { const url = new URL(request.url); const code = url.searchParams.get(\"code\"); if (!code) { return redirect(\"/login?error=github-auth-failed-no-code\"); } // Code gegen Access Token austauschen const accessToken = await githubStrategy.getAccessToken(code); // Benutzerprofil abrufen // Anleitung dazu findest du hier: https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/authorizing-oauth-apps#3-use-the-access-token-to-access-the-api const profile = await getUserProfile(accessToken); // In Session speichern const session = await getSession(request.headers.get(\"Cookie\")); session.set(\"user\", { id: profile.id, displayName: profile.displayName || profile.username, // Weitere Benutzerdaten... }); return redirect(\"/\", { headers: { \"Set-Cookie\": await commitSession(session) } }); } catch (error) { return redirect(\"/login?error=github-auth-failed\"); } }; Aufgabe: GitHub OAuth in einer Remix-Anwendung In dieser Aufgabe erstellst du eine Remix-Anwendung mit GitHub-Authentifizierung. Befolge die folgenden Schritte, um diese Funktionalit\u00e4t zu implementieren: 1. Neue Remix-Anwendung erstellen Verwende die Vorlage auf Moodle, ODER Erstelle eine neue Remix-Anwendung mit folgendem Befehl: npx create-remix@latest Folge den Anweisungen im Terminal, um die Anwendung zu konfigurieren. 2. GitHub OAuth-App einrichten Erstelle eine OAuth-App in deinen GitHub-Entwicklereinstellungen, wie oben beschrieben: - Application name : Remix GitHub Login - Homepage URL : http://localhost:5173 - Authorization callback URL : http://localhost:5173/auth/github/callback Notiere dir die erhaltene Client ID und das Client Secret. 3. \u00dcbersicht der Userdaten Nachdem der OAuth-Flow fertig ist, erhalten wir die Userdaten in einem JSON-Dokument. Erstelle eine sch\u00f6ne \u00dcbersicht in der der Benutzername, E-Mail und optional noch weitere Informationen deines Accounts ersichtlich sind. Beim Design kannst du dich an dem Code aus login.tsx orientieren. Die \u00dcbersicht soll in _index.tsx implementiert werden. /auth/github/callback verweist dort hin wenn der OAuth-Flow fertig ist. 4. Anwendung testen Starte die Anwendung mit: npm run dev \u00d6ffne http://localhost:5173 und teste den \"Login with GitHub\"-Button. Zus\u00e4tzliche Ressourcen OAuth 2.0 Spezifikation (RFC 6749) GitHub OAuth Dokumentation Remix Dokumentation GitHub REST API Authorizing OAuth apps | Web Application Flow","title":"React Remix und GitHub OAuth"},{"location":"TeachingMaterials/prismaremix/githuboauth/#oauth-20-authentifizierung-mit-github-in-einer-remix-anwendung","text":"","title":"OAuth 2.0 Authentifizierung mit GitHub in einer Remix-Anwendung"},{"location":"TeachingMaterials/prismaremix/githuboauth/#einfuhrung-in-oauth-20","text":"OAuth 2.0 ist ein offener Standard f\u00fcr Authentifizierungs- und Autorisierungsprozesse, der es Diensten von Drittanbietern erm\u00f6glicht, auf Ressourcen eines Benutzers zuzugreifen, ohne dessen Zugangsdaten direkt zu verwenden. Dieser Prozess wird durch ein Token-System realisiert, das tempor\u00e4ren Zugriff auf bestimmte Ressourcen gew\u00e4hrt. Der OAuth 2.0 Prozess ist in RFC 6749 standardisiert und umfasst vier Rollen: Resource Owner : Der Benutzer, der Zugriff auf sein Konto gew\u00e4hren kann Client : Die Anwendung, die Zugriff auf das Konto des Benutzers anfordert Authorization Server : Der Server, der Zugriffstoken ausstellt Resource Server : Der Server, der die gesch\u00fctzten Ressourcen hostet","title":"Einf\u00fchrung in OAuth 2.0"},{"location":"TeachingMaterials/prismaremix/githuboauth/#der-oauth-20-autorisierungsablauf-mit-github","text":"Der OAuth 2.0 Ablauf, speziell f\u00fcr die GitHub-Authentifizierung, l\u00e4uft folgenderma\u00dfen ab: Autorisierungsanfrage : Die Anwendung leitet den Benutzer zur GitHub-Autorisierungsseite weiter Benutzerautorisierung : Der Benutzer genehmigt die Zugriffsanfrage Autorisierungscode : GitHub sendet einen tempor\u00e4ren Code an die Anwendung zur\u00fcck Tokenanfrage : Die Anwendung tauscht diesen Code gegen ein Zugriffstoken aus Ressourcenzugriff : Mit diesem Token kann die Anwendung auf die API und Ressourcen zugreifen Quelle: Authorizing OAuth apps | Web Application Flow","title":"Der OAuth 2.0 Autorisierungsablauf mit GitHub"},{"location":"TeachingMaterials/prismaremix/githuboauth/#einrichten-einer-github-oauth-app","text":"Um OAuth mit GitHub zu verwenden, muss eine OAuth-App in den GitHub-Entwicklereinstellungen registriert werden.","title":"Einrichten einer GitHub OAuth-App"},{"location":"TeachingMaterials/prismaremix/githuboauth/#schritt-1-erstellen-einer-neuen-oauth-app-in-github","text":"Melde dich bei GitHub an Navigiere zu den GitHub Developer Settings W\u00e4hle \"OAuth Apps\" und klicke auf \"New OAuth App\"","title":"Schritt 1: Erstellen einer neuen OAuth-App in GitHub"},{"location":"TeachingMaterials/prismaremix/githuboauth/#schritt-2-konfiguration-der-oauth-app","text":"F\u00fclle folgende Felder aus: Application name : Ein aussagekr\u00e4ftiger Name f\u00fcr deine App (z.B. \"Remix GitHub Login\") Homepage URL : Die Basis-URL deiner Anwendung (z.B. \"http://localhost:5173\") Application description : (Optional) Eine kurze Beschreibung deiner Anwendung Authorization callback URL : Die URL, an die GitHub nach der Authentifizierung weiterleitet (z.B. \"http://localhost:5173/auth/github/callback\")","title":"Schritt 2: Konfiguration der OAuth-App"},{"location":"TeachingMaterials/prismaremix/githuboauth/#schritt-3-client-id-und-client-secret","text":"Nach dem Erstellen der App erh\u00e4ltst du zwei wichtige Credentials: Client ID : \u00d6ffentlicher Identifikator f\u00fcr deine App Client Secret : Geheimer Schl\u00fcssel, der sicher aufbewahrt werden muss Diese Credentials werden in deiner Anwendung ben\u00f6tigt, um den OAuth-Prozess zu durchlaufen.","title":"Schritt 3: Client ID und Client Secret"},{"location":"TeachingMaterials/prismaremix/githuboauth/#umsetzung-in-einer-remix-anwendung","text":"In diesem Projekt wurde der OAuth 2.0 Authentifizierungsfluss mit GitHub direkt implementiert, ohne die Passport.js-Middleware zu verwenden. Dies bietet mehr Kontrolle \u00fcber den Authentifizierungsprozess und erleichtert das Verst\u00e4ndnis der einzelnen Schritte.","title":"Umsetzung in einer Remix-Anwendung"},{"location":"TeachingMaterials/prismaremix/githuboauth/#code-beispiel-einleitung-des-oauth-flows","text":"// app/auth/github.ts (Auszug) export const githubStrategy = { options: { clientID: \"your-client-id\", clientSecret: \"your-client-secret\", callbackURL: \"http://localhost:5173/auth/github/callback\", }, getAuthorizationURL(scopes: string[] = [\"user:email\"]) { const url = new URL(\"https://github.com/login/oauth/authorize\"); url.searchParams.set(\"client_id\", this.options.clientID); url.searchParams.set(\"redirect_uri\", this.options.callbackURL); url.searchParams.set(\"scope\", scopes.join(\" \")); return url.toString(); }, // Weitere Methoden... };","title":"Code-Beispiel: Einleitung des OAuth-Flows"},{"location":"TeachingMaterials/prismaremix/githuboauth/#code-beispiel-verarbeitung-des-callback","text":"// app/routes/auth.github.callback.tsx (Auszug) export let loader: LoaderFunction = async ({ request }) => { try { const url = new URL(request.url); const code = url.searchParams.get(\"code\"); if (!code) { return redirect(\"/login?error=github-auth-failed-no-code\"); } // Code gegen Access Token austauschen const accessToken = await githubStrategy.getAccessToken(code); // Benutzerprofil abrufen // Anleitung dazu findest du hier: https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/authorizing-oauth-apps#3-use-the-access-token-to-access-the-api const profile = await getUserProfile(accessToken); // In Session speichern const session = await getSession(request.headers.get(\"Cookie\")); session.set(\"user\", { id: profile.id, displayName: profile.displayName || profile.username, // Weitere Benutzerdaten... }); return redirect(\"/\", { headers: { \"Set-Cookie\": await commitSession(session) } }); } catch (error) { return redirect(\"/login?error=github-auth-failed\"); } };","title":"Code-Beispiel: Verarbeitung des Callback"},{"location":"TeachingMaterials/prismaremix/githuboauth/#aufgabe-github-oauth-in-einer-remix-anwendung","text":"In dieser Aufgabe erstellst du eine Remix-Anwendung mit GitHub-Authentifizierung. Befolge die folgenden Schritte, um diese Funktionalit\u00e4t zu implementieren:","title":"Aufgabe: GitHub OAuth in einer Remix-Anwendung"},{"location":"TeachingMaterials/prismaremix/githuboauth/#1-neue-remix-anwendung-erstellen","text":"Verwende die Vorlage auf Moodle, ODER Erstelle eine neue Remix-Anwendung mit folgendem Befehl: npx create-remix@latest Folge den Anweisungen im Terminal, um die Anwendung zu konfigurieren.","title":"1. Neue Remix-Anwendung erstellen"},{"location":"TeachingMaterials/prismaremix/githuboauth/#2-github-oauth-app-einrichten","text":"Erstelle eine OAuth-App in deinen GitHub-Entwicklereinstellungen, wie oben beschrieben: - Application name : Remix GitHub Login - Homepage URL : http://localhost:5173 - Authorization callback URL : http://localhost:5173/auth/github/callback Notiere dir die erhaltene Client ID und das Client Secret.","title":"2. GitHub OAuth-App einrichten"},{"location":"TeachingMaterials/prismaremix/githuboauth/#3-ubersicht-der-userdaten","text":"Nachdem der OAuth-Flow fertig ist, erhalten wir die Userdaten in einem JSON-Dokument. Erstelle eine sch\u00f6ne \u00dcbersicht in der der Benutzername, E-Mail und optional noch weitere Informationen deines Accounts ersichtlich sind. Beim Design kannst du dich an dem Code aus login.tsx orientieren. Die \u00dcbersicht soll in _index.tsx implementiert werden. /auth/github/callback verweist dort hin wenn der OAuth-Flow fertig ist.","title":"3. \u00dcbersicht der Userdaten"},{"location":"TeachingMaterials/prismaremix/githuboauth/#4-anwendung-testen","text":"Starte die Anwendung mit: npm run dev \u00d6ffne http://localhost:5173 und teste den \"Login with GitHub\"-Button.","title":"4. Anwendung testen"},{"location":"TeachingMaterials/prismaremix/githuboauth/#zusatzliche-ressourcen","text":"OAuth 2.0 Spezifikation (RFC 6749) GitHub OAuth Dokumentation Remix Dokumentation GitHub REST API Authorizing OAuth apps | Web Application Flow","title":"Zus\u00e4tzliche Ressourcen"},{"location":"TeachingMaterials/prismaremix/offline/","text":"remix-pwa Progressive Web Apps (PWAs) sind Webanwendungen, die wie native Apps funktionieren. Sie bieten Offline-Funktionalit\u00e4t, schnelle Ladezeiten und k\u00f6nnen auf dem Startbildschirm eines Ger\u00e4ts installiert werden. In Remix kannst du PWAs erstellen, indem du Service Worker, Cache-API und andere Technologien nutzt. clientLoader / clientAction In Remix werden clientLoader und clientAction verwendet, um Daten auf dem Client zu laden und Aktionen auszuf\u00fchren. Diese Funktionen erm\u00f6glichen es, Daten vom Server zu holen und auf dem Client zu verarbeiten, was besonders wichtig f\u00fcr Offline-Funktionalit\u00e4t ist. service worker Ein Service Worker ist ein Skript, das vom Browser im Hintergrund ausgef\u00fchrt wird und Netzwerkanfragen abfangen kann. Er erm\u00f6glicht das Caching von Ressourcen und die Bereitstellung von Offline-Funktionalit\u00e4t. In Remix kannst du einen Service Worker registrieren, um deine Anwendung offline verf\u00fcgbar zu machen. manifest Das Web App Manifest ist eine JSON-Datei, die Metadaten \u00fcber deine PWA enth\u00e4lt, wie Name, Icons und Start-URL. Es erm\u00f6glicht das Hinzuf\u00fcgen deiner Webanwendung zum Startbildschirm eines Ger\u00e4ts und sorgt f\u00fcr ein natives App-\u00e4hnliches Erlebnis. cache api Die Cache API erm\u00f6glicht das Speichern von Netzwerkantworten im Cache, sodass sie sp\u00e4ter offline verwendet werden k\u00f6nnen. Du kannst die Cache API in deinem Service Worker verwenden, um Ressourcen wie HTML, CSS, JavaScript und Bilder zu cachen. fetch interceptor Ein Fetch Interceptor ist ein Mechanismus, der Netzwerkanfragen abf\u00e4ngt und modifiziert. In einem Service Worker kannst du Fetch-Events abfangen, um Ressourcen aus dem Cache bereitzustellen oder alternative Inhalte anzuzeigen, wenn der Benutzer offline ist. Beispiel: https://github.com/maximiliankraft/angular-pouchdb/","title":"Offline"},{"location":"TeachingMaterials/prismaremix/offline/#remix-pwa","text":"Progressive Web Apps (PWAs) sind Webanwendungen, die wie native Apps funktionieren. Sie bieten Offline-Funktionalit\u00e4t, schnelle Ladezeiten und k\u00f6nnen auf dem Startbildschirm eines Ger\u00e4ts installiert werden. In Remix kannst du PWAs erstellen, indem du Service Worker, Cache-API und andere Technologien nutzt.","title":"remix-pwa"},{"location":"TeachingMaterials/prismaremix/offline/#clientloader-clientaction","text":"In Remix werden clientLoader und clientAction verwendet, um Daten auf dem Client zu laden und Aktionen auszuf\u00fchren. Diese Funktionen erm\u00f6glichen es, Daten vom Server zu holen und auf dem Client zu verarbeiten, was besonders wichtig f\u00fcr Offline-Funktionalit\u00e4t ist.","title":"clientLoader / clientAction"},{"location":"TeachingMaterials/prismaremix/offline/#service-worker","text":"Ein Service Worker ist ein Skript, das vom Browser im Hintergrund ausgef\u00fchrt wird und Netzwerkanfragen abfangen kann. Er erm\u00f6glicht das Caching von Ressourcen und die Bereitstellung von Offline-Funktionalit\u00e4t. In Remix kannst du einen Service Worker registrieren, um deine Anwendung offline verf\u00fcgbar zu machen.","title":"service worker"},{"location":"TeachingMaterials/prismaremix/offline/#manifest","text":"Das Web App Manifest ist eine JSON-Datei, die Metadaten \u00fcber deine PWA enth\u00e4lt, wie Name, Icons und Start-URL. Es erm\u00f6glicht das Hinzuf\u00fcgen deiner Webanwendung zum Startbildschirm eines Ger\u00e4ts und sorgt f\u00fcr ein natives App-\u00e4hnliches Erlebnis.","title":"manifest"},{"location":"TeachingMaterials/prismaremix/offline/#cache-api","text":"Die Cache API erm\u00f6glicht das Speichern von Netzwerkantworten im Cache, sodass sie sp\u00e4ter offline verwendet werden k\u00f6nnen. Du kannst die Cache API in deinem Service Worker verwenden, um Ressourcen wie HTML, CSS, JavaScript und Bilder zu cachen.","title":"cache api"},{"location":"TeachingMaterials/prismaremix/offline/#fetch-interceptor","text":"Ein Fetch Interceptor ist ein Mechanismus, der Netzwerkanfragen abf\u00e4ngt und modifiziert. In einem Service Worker kannst du Fetch-Events abfangen, um Ressourcen aus dem Cache bereitzustellen oder alternative Inhalte anzuzeigen, wenn der Benutzer offline ist.","title":"fetch interceptor"},{"location":"TeachingMaterials/prismaremix/offline/#beispiel","text":"https://github.com/maximiliankraft/angular-pouchdb/","title":"Beispiel:"},{"location":"TeachingMaterials/prismaremix/react/","text":"Zun\u00e4chst muss man React installieren. Dazu gibt es hier eine Anleitung. Dort wird auch erkl\u00e4rt warum man React am besten mit einem anderen Framework wie Remix oder Next.js einsetzt. Mit dem Befehl npx create-remix <projektname> kann man sich ein Projekt generieren lassen. Falls noch nicht installiert, ben\u00f6tigt man nodejs daf\u00fcr. F\u00fcr die ersten Schritte mit React gibt es hier ein Tutorial in dem eine kleine Tic-Tac-Toe App erstellt wird.","title":"Grundlagen React"},{"location":"TeachingMaterials/prismaremix/remix/","text":"Remix baut auf React auf. React kann aber nur lokal auf dem Browser der Benutzer ausgef\u00fchrt werden. Remix k\u00fcmmert sich zus\u00e4tzlich um alles was serverseitig passiert. Basierend auf dem Dateinamen z.B das Routing. Hei\u00dft eine Datei home.profile.tsx ist deren Komponente unter /home/profile aufrufbar. Ausnahme: Mit _index.tsx wird der oberste Pfad / beschrieben. In einer solchen TSX-Datei kann es 3 Funktionen geben. Eine Funktion die eine Komponente beschreibt: export function MyComponent(){ return <div> <p>Welcome to my Component<p> </div> } Die Funktion loader wird auf dem Server ausgef\u00fchrt bevor die Komponente ausgeliefert wird. Wichtig ist hier zu unterscheiden dass die Funktion MyComponent() auf dem Client ausgef\u00fchrt wird. Man kann also nicht ohne weiteres eine Variable definieren die dann in beiden Funktionen verf\u00fcgbar ist. Dazu m\u00fcssen zus\u00e4tzliche Daten vom Server zum Client \u00fcbertragen werden. Das kann z.B so funktionieren: export const loader: LoaderFunction = async (args: LoaderFunctionArgs) => { return json({\"data\". \"server-generated data\"}) } export function MyComponent(){ const {data} = useLoaderData(); return <div> <p>Welcome to my Component<p> {data} </div> } Die dritte Funktion action ist daf\u00fcr zust\u00e4ndig dass in die andere Richtung Daten vom Benutzer zum Server gesendet werden k\u00f6nnen. Nat\u00fcrlich kann man auch immer - wie in klassischen React Anwednungen - die fetch-API (siehe MDN fetch ) verwenden. Der Name der Funktion leitet sich aus dem Attributsnamen action des <form> -Tags ab. export const action: ActionFunction = async (args: ActionFunctionArgs) => { const storyData = await args.request.formData(); console.log(storyData) // returns (on the server) /* { \"title\": \"<entered title>\", \"content\": \"<entered content>\" } */ } export function MyComponent(){ return <form method=\"POST\"> <input type=\"text\" name=\"title\" /> <textarea name=\"content\" ></textarea> <input type=\"Submit\" /> </form> }","title":"Grundlagen Remix"},{"location":"TeachingMaterials/prismaremix/typescript/","text":"In Remix-Code (bzw. in Typescript allgemein) findet man oft folgenden Ausdruck: interface FnProps{ attr1: string; attr2: string; } const fn: (FnProps) => void = ({attr1, attr2}): void => { console.log(attr1, attr2) } Das ist eine kurze Schreibweise von: const fn: (FnProps) => void = (props): void => { console.log(props.attr1, props.attr2) } Das nennt sich Destructuring assignment , mehr dazu findest du in der Dokumentation von Typescript hier . Funktioniert mit Arrays auch z.B. Destructuring ist keine Typescript-Eigenschaft sondern von JavaScript. Man kann auch mehrere ineinander verschaltelte Objekte durch destructuring ansteuren, mehr dazu auf MDN hier . Andere wichtige Bereiche aus der Dokumentation sind: - Type Annotations on Variables - More on Functions - Object Types - Modules Diese bitte alle durchlesen.","title":"Grundlagen Typescript"}]}